{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45695246",
   "metadata": {},
   "source": [
    "# Gradient boosting model\n",
    "\n",
    "One, two, you know what to do...\n",
    "\n",
    "## Notebook set up\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from itertools import combinations, permutations\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler, KBinsDiscretizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e1f00",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17412612",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample sizes, use 1 for full dataset\n",
    "OPTIMIZATION_SAMPLE = 0.1\n",
    "EVALUATION_SAMPLE = 0.2\n",
    "CV_FOLDS = 3\n",
    "N_JOBS = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf85c9",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed701ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = 'https://gperdrizet.github.io/FSA_devops/assets/data/unit3/diabetes_prediction_train.csv'\n",
    "\n",
    "# Load the training dataset\n",
    "df = pd.read_csv(train_df_path)\n",
    "\n",
    "# Split test set for internal evaluation\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=315)\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Display first few rows of training data\n",
    "train_df.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information (columns, dtypes, non-null counts)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa085fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3963201",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop ID column\n",
    "train_df.drop(columns=['id'], inplace=True)\n",
    "test_df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "print(f'ID column removed')\n",
    "print(f'Remaining columns: {list(train_df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e719d1",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "### 2.1. Column definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label\n",
    "label = 'diagnosed_diabetes'\n",
    "\n",
    "# Define numerical features to apply IQR clipping\n",
    "numerical_features = [\n",
    "    'age', 'physical_activity_minutes_per_week', 'diet_score',\n",
    "    'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi',\n",
    "    'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate',\n",
    "    'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides',\n",
    "]\n",
    "\n",
    "# Define ordinal features to encode\n",
    "ordinal_features = ['education_level', 'income_level', 'alcohol_consumption_per_week']\n",
    "\n",
    "# Define ordinal categories in order\n",
    "education_categories = [['No formal', 'Highschool', 'Graduate', 'Postgraduate']]\n",
    "income_categories = [['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']]\n",
    "alcohol_categories = [[1, 2, 3, 4, 5, 6, 7, 8, 9]]\n",
    "\n",
    "# Define features for one-hot encoding\n",
    "nominal_features = [\n",
    "    'gender', 'ethnicity', 'smoking_status', 'employment_status',\n",
    "    'family_history_diabetes', 'hypertension_history', 'cardiovascular_history'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56138fb0",
   "metadata": {},
   "source": [
    "### 2.2. Ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df527d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ordinal encoder with categories\n",
    "ordinal_encoder = OrdinalEncoder(\n",
    "    categories=education_categories + income_categories + alcohol_categories,\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "# Fit and transform ordinal features\n",
    "ordinal_encoded = ordinal_encoder.fit_transform(train_df[ordinal_features])\n",
    "train_df.drop(columns=ordinal_features, inplace=True)\n",
    "train_df[ordinal_features] = ordinal_encoded\n",
    "\n",
    "# And the test data\n",
    "ordinal_encoded = ordinal_encoder.transform(test_df[ordinal_features])\n",
    "test_df.drop(columns=ordinal_features, inplace=True)\n",
    "test_df[ordinal_features] = ordinal_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c16c9",
   "metadata": {},
   "source": [
    "### 2.3. Nominal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae985fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoder\n",
    "onehot_encoder = OneHotEncoder(\n",
    "    drop='first',\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore'\n",
    ")\n",
    "\n",
    "# Convert encoded features to DataFrame\n",
    "encoded_features_df = pd.DataFrame(\n",
    "    onehot_encoder.fit_transform(train_df[nominal_features]),\n",
    "    columns=onehot_encoder.get_feature_names_out(nominal_features)\n",
    ")\n",
    "\n",
    "# Remove original nominal features and add encoded versions\n",
    "train_df = pd.concat([train_df.drop(columns=nominal_features), encoded_features_df], axis=1)\n",
    "\n",
    "# And the test data\n",
    "encoded_features_df = pd.DataFrame(\n",
    "    onehot_encoder.transform(test_df[nominal_features]),\n",
    "    columns=onehot_encoder.get_feature_names_out(nominal_features)\n",
    ")\n",
    "\n",
    "test_df = pd.concat([test_df.drop(columns=nominal_features), encoded_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572141e",
   "metadata": {},
   "source": [
    "## 3. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da12a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_df.drop(columns=[label]).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993c51b",
   "metadata": {},
   "source": [
    "### 3.1. Feature discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a KbinsDiscretizer\n",
    "binning_transformer = KBinsDiscretizer(n_bins=5, encode='ordinal')\n",
    "\n",
    "# Bin the numerical features\n",
    "binned_features = binning_transformer.fit_transform(train_df[numerical_features])\n",
    "\n",
    "# Add new binned features to the training DataFrame\n",
    "binned_features_df = pd.DataFrame(binned_features, columns=[f'binned_{feature}' for feature in numerical_features])\n",
    "train_df = pd.concat([train_df, binned_features_df], axis=1)\n",
    "\n",
    "# And the test data\n",
    "binned_features = binning_transformer.transform(test_df[numerical_features])\n",
    "binned_features_df = pd.DataFrame(binned_features, columns=[f'binned_{feature}' for feature in numerical_features])\n",
    "test_df = pd.concat([test_df, binned_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6743d1f9",
   "metadata": {},
   "source": [
    "### 3.2. Clustering\n",
    "\n",
    "#### 3.2.1. Heart health clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd916f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "heart_features = ['systolic_bp', 'diastolic_bp', 'heart_rate', 'hypertension_history', 'cardiovascular_history']\n",
    "\n",
    "# Fit KMean clustering model on the training data\n",
    "kmeans_model = KMeans(n_clusters=4, random_state=315)\n",
    "kmeans_model.fit(train_df[heart_features], train_df[label])\n",
    "\n",
    "# Add cluster membership as a new feature\n",
    "train_df['heart_cluster'] = kmeans_model.predict(train_df[heart_features])\n",
    "test_df['heart_cluster'] = kmeans_model.predict(test_df[heart_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b440ec7",
   "metadata": {},
   "source": [
    "#### 3.2.2. Cholesterol clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff0975",
   "metadata": {},
   "outputs": [],
   "source": [
    "cholesterol_features = ['cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides']\n",
    "\n",
    "# Fit KMean clustering model on the training data\n",
    "kmeans_model = KMeans(n_clusters=4, random_state=315)\n",
    "kmeans_model.fit(train_df[cholesterol_features], train_df[label])\n",
    "\n",
    "# Add cluster membership as a new feature\n",
    "train_df['cholesterol_cluster'] = kmeans_model.predict(train_df[cholesterol_features])\n",
    "test_df['cholesterol_cluster'] = kmeans_model.predict(test_df[cholesterol_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f44f40",
   "metadata": {},
   "source": [
    "#### 3.2.3. Lifestyle clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "lifestyle_features = [\n",
    "    'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day',\n",
    "    'screen_time_hours_per_day', 'alcohol_consumption_per_week'\n",
    "]\n",
    "\n",
    "# Fit KMean clustering model on the training data\n",
    "kmeans_model = KMeans(n_clusters=4, random_state=315)\n",
    "kmeans_model.fit(train_df[lifestyle_features], train_df[label])\n",
    "\n",
    "# Add cluster membership as a new feature\n",
    "train_df['lifestyle_cluster'] = kmeans_model.predict(train_df[lifestyle_features])\n",
    "test_df['lifestyle_cluster'] = kmeans_model.predict(test_df[lifestyle_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa842a",
   "metadata": {},
   "source": [
    "### 3.3. Other synthetic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a477351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dictionary to collect new features and add at the end\n",
    "new_train_features = {}\n",
    "new_test_features = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335b8b3",
   "metadata": {},
   "source": [
    "#### 3.3.1. Difference features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4653da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_a, feature_b in combinations(features, 2):\n",
    "    feature_name = f'{feature_a}-{feature_b}'\n",
    "    new_train_features[feature_name] = train_df[feature_a] - train_df[feature_b]\n",
    "    new_test_features[feature_name] = test_df[feature_a] - test_df[feature_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11977a5d",
   "metadata": {},
   "source": [
    "#### 3.3.2. Sum features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_a, feature_b in combinations(features, 2):\n",
    "    feature_name = f'{feature_a}+{feature_b}'\n",
    "    new_train_features[feature_name] = train_df[feature_a] + train_df[feature_b]\n",
    "    new_test_features[feature_name] = test_df[feature_a] + test_df[feature_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a7b51",
   "metadata": {},
   "source": [
    "#### 3.3.3. Ratio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b558a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_a, feature_b in permutations(features, 2):\n",
    "    feature_name = f'{feature_a}/{feature_b}'\n",
    "    new_train_features[feature_name] = train_df[feature_a] / (train_df[feature_b] + train_df[feature_b].min() + 1)\n",
    "    new_test_features[feature_name] = test_df[feature_a] / (test_df[feature_b] + test_df[feature_b].min() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7a56c",
   "metadata": {},
   "source": [
    "#### 3.3.4. Reciprocal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef30504",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    feature_name = f'1/{feature}'\n",
    "    new_train_features[feature_name] = 1 / (train_df[feature] + train_df[feature].min() + 1)\n",
    "    new_test_features[feature_name] = 1 / (test_df[feature] + test_df[feature].min() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0269b7",
   "metadata": {},
   "source": [
    "#### 3.3.5. Log features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    feature_name = f'log{feature}'\n",
    "    new_train_features[feature_name] = np.log(train_df[feature] + train_df[feature].min() + 1)\n",
    "    new_test_features[feature_name] = np.log(test_df[feature] + test_df[feature].min() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57630d0d",
   "metadata": {},
   "source": [
    "#### 3.3.6. Square root features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    feature_name = f'root{feature}'\n",
    "    new_train_features[feature_name] = (train_df[feature] + train_df[feature].min() + 1) ** (1/2)\n",
    "    new_test_features[feature_name] = (test_df[feature] + test_df[feature].min() + 1) ** (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b2ff5d",
   "metadata": {},
   "source": [
    "#### 3.3.7. Square features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a52f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    feature_name = f'root{feature}'\n",
    "    new_train_features[feature_name] = (train_df[feature] + train_df[feature].min() + 1) ** 2\n",
    "    new_test_features[feature_name] = (test_df[feature] + test_df[feature].min() + 1) ** 2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e630e7",
   "metadata": {},
   "source": [
    "#### 3.3.8. Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.concat([train_df, pd.DataFrame(new_train_features)], axis=1)\n",
    "test_df = pd.concat([test_df, pd.DataFrame(new_test_features)], axis=1)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a989583",
   "metadata": {},
   "source": [
    "## 4. Model training and optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a19b10e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_eval_df = train_df.sample(frac=EVALUATION_SAMPLE, random_state=315).reset_index(drop=True)\n",
    "train_optimization_df = train_df.sample(frac=OPTIMIZATION_SAMPLE, random_state=315).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f8015",
   "metadata": {},
   "source": [
    "### 4.1. Baseline model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model with default parameters\n",
    "baseline_model = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('feature_selector', SelectPercentile()),\n",
    "    ('classifier', HistGradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Estimate AUC with cross-validation\n",
    "baseline_scores = cross_val_score(\n",
    "    baseline_model,\n",
    "    train_eval_df.drop(columns=['diagnosed_diabetes']).sample(frac=OPTIMIZATION_SAMPLE, random_state=315),\n",
    "    train_eval_df['diagnosed_diabetes'],\n",
    "    cv=CV_FOLDS,\n",
    "    scoring=make_scorer(roc_auc_score),\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "print(f'Baseline model mean cross-validation score (ROC-AUC): {np.mean(baseline_scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a38db3",
   "metadata": {},
   "source": [
    "### 4.1. Hyperparameter optimization with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d048c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "from scipy.stats import uniform, randint, loguniform\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('feature_selector', SelectPercentile()),\n",
    "    ('classifier', HistGradientBoostingClassifier(random_state=315))\n",
    "])\n",
    "\n",
    "# Best cross-validation score (ROC-AUC): 0.7083\n",
    "\n",
    "# Best parameters:\n",
    "#   classifier__early_stopping: True\n",
    "#   classifier__l2_regularization: 62.63475886951615\n",
    "#   classifier__learning_rate: 0.03649801250450118\n",
    "#   classifier__max_bins: 64\n",
    "#   classifier__max_depth: None\n",
    "#   classifier__max_iter: 413\n",
    "#   classifier__min_samples_leaf: 43\n",
    "#   classifier__n_iter_no_change: 30\n",
    "#   classifier__validation_fraction: 0.1\n",
    "#   feature_selector__percentile: 25\n",
    "\n",
    "# Took 171 minutes on 100,000 samples with n_jobs=1\n",
    "\n",
    "# Define parameter distributions for randomized search\n",
    "param_distributions = {\n",
    "    'feature_selector__percentile': uniform(0.25, 0.95),\n",
    "    'classifier__learning_rate': loguniform(0.001, 0.3),\n",
    "    'classifier__max_iter': randint(100, 501),\n",
    "    'classifier__max_depth': [10, 15, 20, 25, None],\n",
    "    'classifier__min_samples_leaf': randint(5, 51),\n",
    "    'classifier__l2_regularization': loguniform(1e-4, 100.0),\n",
    "    'classifier__max_bins': [64, 128, 255],\n",
    "    'classifier__early_stopping': [True],\n",
    "    'classifier__validation_fraction': [0.1],\n",
    "    'classifier__n_iter_no_change': [30]\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=200,\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=N_JOBS,\n",
    "    random_state=315,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Fit the randomized search\n",
    "random_search.fit(\n",
    "    train_optimization_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_optimization_df['diagnosed_diabetes']\n",
    ")\n",
    "\n",
    "# Get the best model from random search\n",
    "optimized_model = random_search.best_estimator_\n",
    "\n",
    "print(f'\\nBest mean cross-validation score (ROC-AUC): {random_search.best_score_:.4f}')\n",
    "print(f'\\nBest parameters:')\n",
    "\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f'  {param}: {value}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e97e4",
   "metadata": {},
   "source": [
    "### 4.2. Evaluate optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with cross-validation to verify performance\n",
    "scores = cross_val_score(\n",
    "    optimized_model,\n",
    "    train_eval_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_eval_df['diagnosed_diabetes'],\n",
    "    cv=CV_FOLDS,\n",
    "    scoring=make_scorer(roc_auc_score),\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "print(f'Optimized model mean cross-validation score (ROC-AUC): {np.mean(scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2d6f2",
   "metadata": {},
   "source": [
    "### 4.4. Compare baseline vs optimized performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for boxplot\n",
    "comparison_data = [baseline_scores, scores]\n",
    "labels = ['Baseline\\n(Unoptimized)', 'Optimized']\n",
    "\n",
    "# Create boxplot\n",
    "plt.title('Cross-Validation Performance')\n",
    "plt.boxplot(comparison_data, tick_labels=labels, patch_artist=True, widths=0.6)\n",
    "plt.ylabel('ROC-AUC Score')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f'Baseline  - mean score (ROC-AUC): {np.mean(baseline_scores):.4f}, Std: {np.std(baseline_scores):.4f}')\n",
    "print(f'Optimized - mean score (ROC-AUC): {np.mean(scores):.4f}, Std: {np.std(scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5235f51",
   "metadata": {},
   "source": [
    "### 4.5. Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for both models using cross-validation\n",
    "baseline_predictions = cross_val_predict(\n",
    "    baseline_model,\n",
    "    train_eval_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_eval_df['diagnosed_diabetes'],\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "optimized_predictions = cross_val_predict(\n",
    "    optimized_model,\n",
    "    train_eval_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_eval_df['diagnosed_diabetes'],\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "# Plot confusion matrices side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Baseline confusion matrix\n",
    "axes[0].set_title('Unoptimized model')\n",
    "\n",
    "disp1 = ConfusionMatrixDisplay.from_predictions(\n",
    "    train_df['diagnosed_diabetes'],\n",
    "    baseline_predictions,\n",
    "    normalize='true',\n",
    "    ax=axes[0],\n",
    "    colorbar=False\n",
    ")\n",
    "\n",
    "# Optimized confusion matrix\n",
    "axes[1].set_title('Optimized model')\n",
    "\n",
    "disp3 = ConfusionMatrixDisplay.from_predictions(\n",
    "    train_df['diagnosed_diabetes'],\n",
    "    optimized_predictions,\n",
    "    normalize='true',\n",
    "    ax=axes[1],\n",
    "    colorbar=False\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b9d34",
   "metadata": {},
   "source": [
    "## 5. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cc18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model directory and ensure it exists\n",
    "model_dir = Path('../models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create model filename\n",
    "model_name = 'gradient_boosting'\n",
    "model_path = model_dir / f'{model_name}.joblib'\n",
    "\n",
    "# Save the final model\n",
    "joblib.dump(optimized_model, model_path)\n",
    "print('Model saved to:', model_path)\n",
    "print(f'File size: {model_path.stat().st_size / (1024**2):.2f} MB')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
