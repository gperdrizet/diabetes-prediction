{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45695246",
   "metadata": {},
   "source": [
    "# Gradient boosting model\n",
    "\n",
    "One, two, you know what to do...\n",
    "\n",
    "## Notebook set up\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5c3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "from itertools import combinations, permutations\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.model_selection import cross_val_score, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf85c9",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed701ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>31</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol_consumption_per_week</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physical_activity_minutes_per_week</th>\n",
       "      <td>45</td>\n",
       "      <td>73</td>\n",
       "      <td>158</td>\n",
       "      <td>77</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diet_score</th>\n",
       "      <td>7.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleep_hours_per_day</th>\n",
       "      <td>6.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen_time_hours_per_day</th>\n",
       "      <td>6.1</td>\n",
       "      <td>5.8</td>\n",
       "      <td>9.1</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>33.4</td>\n",
       "      <td>23.8</td>\n",
       "      <td>24.1</td>\n",
       "      <td>26.6</td>\n",
       "      <td>28.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waist_to_hip_ratio</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>systolic_bp</th>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>95</td>\n",
       "      <td>121</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diastolic_bp</th>\n",
       "      <td>70</td>\n",
       "      <td>77</td>\n",
       "      <td>89</td>\n",
       "      <td>69</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_rate</th>\n",
       "      <td>62</td>\n",
       "      <td>71</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cholesterol_total</th>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>188</td>\n",
       "      <td>182</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hdl_cholesterol</th>\n",
       "      <td>58</td>\n",
       "      <td>50</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ldl_cholesterol</th>\n",
       "      <td>114</td>\n",
       "      <td>121</td>\n",
       "      <td>114</td>\n",
       "      <td>85</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triglycerides</th>\n",
       "      <td>102</td>\n",
       "      <td>124</td>\n",
       "      <td>108</td>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <td>Hispanic</td>\n",
       "      <td>White</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_level</th>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income_level</th>\n",
       "      <td>Lower-Middle</td>\n",
       "      <td>Upper-Middle</td>\n",
       "      <td>Lower-Middle</td>\n",
       "      <td>Lower-Middle</td>\n",
       "      <td>Upper-Middle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoking_status</th>\n",
       "      <td>Current</td>\n",
       "      <td>Never</td>\n",
       "      <td>Never</td>\n",
       "      <td>Current</td>\n",
       "      <td>Never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employment_status</th>\n",
       "      <td>Employed</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Retired</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_history_diabetes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypertension_history</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cardiovascular_history</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diagnosed_diabetes</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               0             1             2  \\\n",
       "id                                             0             1             2   \n",
       "age                                           31            50            32   \n",
       "alcohol_consumption_per_week                   1             2             3   \n",
       "physical_activity_minutes_per_week            45            73           158   \n",
       "diet_score                                   7.7           5.7           8.5   \n",
       "sleep_hours_per_day                          6.8           6.5           7.4   \n",
       "screen_time_hours_per_day                    6.1           5.8           9.1   \n",
       "bmi                                         33.4          23.8          24.1   \n",
       "waist_to_hip_ratio                          0.93          0.83          0.83   \n",
       "systolic_bp                                  112           120            95   \n",
       "diastolic_bp                                  70            77            89   \n",
       "heart_rate                                    62            71            73   \n",
       "cholesterol_total                            199           199           188   \n",
       "hdl_cholesterol                               58            50            59   \n",
       "ldl_cholesterol                              114           121           114   \n",
       "triglycerides                                102           124           108   \n",
       "gender                                    Female        Female          Male   \n",
       "ethnicity                               Hispanic         White      Hispanic   \n",
       "education_level                       Highschool    Highschool    Highschool   \n",
       "income_level                        Lower-Middle  Upper-Middle  Lower-Middle   \n",
       "smoking_status                           Current         Never         Never   \n",
       "employment_status                       Employed      Employed       Retired   \n",
       "family_history_diabetes                        0             0             0   \n",
       "hypertension_history                           0             0             0   \n",
       "cardiovascular_history                         0             0             0   \n",
       "diagnosed_diabetes                           1.0           1.0           0.0   \n",
       "\n",
       "                                               3             4  \n",
       "id                                             3             4  \n",
       "age                                           54            54  \n",
       "alcohol_consumption_per_week                   3             1  \n",
       "physical_activity_minutes_per_week            77            55  \n",
       "diet_score                                   4.6           5.7  \n",
       "sleep_hours_per_day                          7.0           6.2  \n",
       "screen_time_hours_per_day                    9.2           5.1  \n",
       "bmi                                         26.6          28.8  \n",
       "waist_to_hip_ratio                          0.83           0.9  \n",
       "systolic_bp                                  121           108  \n",
       "diastolic_bp                                  69            60  \n",
       "heart_rate                                    74            85  \n",
       "cholesterol_total                            182           206  \n",
       "hdl_cholesterol                               54            49  \n",
       "ldl_cholesterol                               85           131  \n",
       "triglycerides                                123           124  \n",
       "gender                                    Female          Male  \n",
       "ethnicity                                  White         White  \n",
       "education_level                       Highschool    Highschool  \n",
       "income_level                        Lower-Middle  Upper-Middle  \n",
       "smoking_status                           Current         Never  \n",
       "employment_status                       Employed       Retired  \n",
       "family_history_diabetes                        0             0  \n",
       "hypertension_history                           1             1  \n",
       "cardiovascular_history                         0             0  \n",
       "diagnosed_diabetes                           1.0           1.0  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_path = 'https://gperdrizet.github.io/FSA_devops/assets/data/unit3/diabetes_prediction_train.csv'\n",
    "\n",
    "# Load the training dataset\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "\n",
    "# Display first few rows of training data\n",
    "train_df.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ff30c8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 700000 entries, 0 to 699999\n",
      "Data columns (total 26 columns):\n",
      " #   Column                              Non-Null Count   Dtype  \n",
      "---  ------                              --------------   -----  \n",
      " 0   id                                  700000 non-null  int64  \n",
      " 1   age                                 700000 non-null  int64  \n",
      " 2   alcohol_consumption_per_week        700000 non-null  int64  \n",
      " 3   physical_activity_minutes_per_week  700000 non-null  int64  \n",
      " 4   diet_score                          700000 non-null  float64\n",
      " 5   sleep_hours_per_day                 700000 non-null  float64\n",
      " 6   screen_time_hours_per_day           700000 non-null  float64\n",
      " 7   bmi                                 700000 non-null  float64\n",
      " 8   waist_to_hip_ratio                  700000 non-null  float64\n",
      " 9   systolic_bp                         700000 non-null  int64  \n",
      " 10  diastolic_bp                        700000 non-null  int64  \n",
      " 11  heart_rate                          700000 non-null  int64  \n",
      " 12  cholesterol_total                   700000 non-null  int64  \n",
      " 13  hdl_cholesterol                     700000 non-null  int64  \n",
      " 14  ldl_cholesterol                     700000 non-null  int64  \n",
      " 15  triglycerides                       700000 non-null  int64  \n",
      " 16  gender                              700000 non-null  object \n",
      " 17  ethnicity                           700000 non-null  object \n",
      " 18  education_level                     700000 non-null  object \n",
      " 19  income_level                        700000 non-null  object \n",
      " 20  smoking_status                      700000 non-null  object \n",
      " 21  employment_status                   700000 non-null  object \n",
      " 22  family_history_diabetes             700000 non-null  int64  \n",
      " 23  hypertension_history                700000 non-null  int64  \n",
      " 24  cardiovascular_history              700000 non-null  int64  \n",
      " 25  diagnosed_diabetes                  700000 non-null  float64\n",
      "dtypes: float64(6), int64(14), object(6)\n",
      "memory usage: 138.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display dataset information (columns, dtypes, non-null counts)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3963201",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b00d402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID column removed\n",
      "Remaining columns: ['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate', 'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides', 'gender', 'ethnicity', 'education_level', 'income_level', 'smoking_status', 'employment_status', 'family_history_diabetes', 'hypertension_history', 'cardiovascular_history', 'diagnosed_diabetes']\n"
     ]
    }
   ],
   "source": [
    "# Drop ID column\n",
    "train_df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "print(f'ID column removed')\n",
    "print(f'Remaining columns: {list(train_df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48e719d1",
   "metadata": {},
   "source": [
    "## 2. Preprocessing\n",
    "\n",
    "### 2.1. Column definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed8d20fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label\n",
    "label = 'diagnosed_diabetes'\n",
    "\n",
    "# Define numerical features to apply IQR clipping\n",
    "numerical_features = [\n",
    "    'age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week',\n",
    "    'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi',\n",
    "    'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate',\n",
    "    'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides',\n",
    "]\n",
    "\n",
    "# Define ordinal features to encode\n",
    "ordinal_features = ['education_level', 'income_level']\n",
    "\n",
    "# Define ordinal categories in order\n",
    "education_categories = [['No formal', 'Highschool', 'Graduate', 'Postgraduate']]\n",
    "income_categories = [['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']]\n",
    "\n",
    "# Define features for one-hot encoding\n",
    "nominal_features = [\n",
    "    'gender', 'ethnicity', 'smoking_status', 'employment_status',\n",
    "    'family_history_diabetes', 'hypertension_history', 'cardiovascular_history'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56138fb0",
   "metadata": {},
   "source": [
    "### 2.2. Ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5df527d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ordinal encoder with categories\n",
    "ordinal_encoder = OrdinalEncoder(\n",
    "    categories=education_categories + income_categories,\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "# Fit and transform ordinal features\n",
    "ordinal_encoded = ordinal_encoder.fit_transform(train_df[ordinal_features])\n",
    "train_df[ordinal_features] = ordinal_encoded\n",
    "\n",
    "# Remove original ordinal features and add encoded versions\n",
    "train_df.drop(columns=ordinal_features, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c16c9",
   "metadata": {},
   "source": [
    "### 2.3. Nominal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ae985fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one-hot encoder\n",
    "onehot_encoder = OneHotEncoder(\n",
    "    drop='first',\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore'\n",
    ")\n",
    "\n",
    "# Convert encoded features to DataFrame\n",
    "encoded_features_df = pd.DataFrame(\n",
    "    onehot_encoder.fit_transform(train_df[nominal_features]),\n",
    "    columns=onehot_encoder.get_feature_names_out(nominal_features)\n",
    ")\n",
    "\n",
    "# Remove original nominal features and add encoded versions\n",
    "train_df = pd.concat([train_df.drop(columns=nominal_features), encoded_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572141e",
   "metadata": {},
   "source": [
    "## 3. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4da12a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = train_df.drop(columns=[label]).columns.tolist()\n",
    "new_features = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335b8b3",
   "metadata": {},
   "source": [
    "### 3.1. Difference features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e4653da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_a, feature_b in combinations(features, 2):\n",
    "    feature_name = f'{feature_a}-{feature_b}'\n",
    "    new_features[feature_name] = train_df[feature_a] - train_df[feature_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11977a5d",
   "metadata": {},
   "source": [
    "### 3.2. Sum features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1739c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_a, feature_b in combinations(features, 2):\n",
    "    feature_name = f'{feature_a}+{feature_b}'\n",
    "    new_features[feature_name] = train_df[feature_a] + train_df[feature_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a7b51",
   "metadata": {},
   "source": [
    "### 3.3. Ratio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "77b558a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature_a, feature_b in permutations(features, 2):\n",
    "    feature_name = f'{feature_a}/{feature_b}'\n",
    "    new_features[feature_name] = train_df[feature_a] / (train_df[feature_b] + train_df[feature_b].min() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7a56c",
   "metadata": {},
   "source": [
    "### 3.4. Reciprocal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2ef30504",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    feature_name = f'1/{feature}'\n",
    "    new_features[feature_name] = 1 / (train_df[feature] + train_df[feature].min() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0269b7",
   "metadata": {},
   "source": [
    "### 3.5. Log features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "df47201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    feature_name = f'log{feature}'\n",
    "    new_features[feature_name] = np.log(train_df[feature] + train_df[feature].min() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57630d0d",
   "metadata": {},
   "source": [
    "### 3.6. Square root features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b2de4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    feature_name = f'root{feature}'\n",
    "    new_features[feature_name] = (train_df[feature] + train_df[feature].min() + 1) ** (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73b2ff5d",
   "metadata": {},
   "source": [
    "### 3.7. Square features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "32a52f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    feature_name = f'root{feature}'\n",
    "    new_features[feature_name] = (train_df[feature] + train_df[feature].min() + 1) ** 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e630e7",
   "metadata": {},
   "source": [
    "### 3.6. Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5aa2bf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 700000 entries, 0 to 699999\n",
      "Columns: 1741 entries, age to rootcardiovascular_history_1\n",
      "dtypes: float64(1631), int64(110)\n",
      "memory usage: 9.1 GB\n"
     ]
    }
   ],
   "source": [
    "train_df = pd.concat([train_df, pd.DataFrame(new_features)], axis=1)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a989583",
   "metadata": {},
   "source": [
    "## 4. Model training and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f8015",
   "metadata": {},
   "source": [
    "### 4.0. Baseline model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e856c13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline model cross-validation mean AUC: 0.6233\n"
     ]
    }
   ],
   "source": [
    "# Create baseline model with default parameters\n",
    "baseline_model = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('feature_selector', SelectPercentile(percentile=10)),\n",
    "    ('classifier', HistGradientBoostingClassifier())\n",
    "])\n",
    "\n",
    "# Estimate AUC with cross-validation\n",
    "baseline_scores = cross_val_score(\n",
    "    baseline_model,\n",
    "    train_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_df['diagnosed_diabetes'],\n",
    "    cv=3,\n",
    "    scoring=make_scorer(roc_auc_score)\n",
    ")\n",
    "\n",
    "print(f'Baseline model cross-validation mean AUC: {np.mean(baseline_scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a38db3",
   "metadata": {},
   "source": [
    "### 4.1. Hyperparameter optimization with RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d048c0e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax. Perhaps you forgot a comma? (<unknown>, line 27)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[36m(most recent call last)\u001b[39m:\n",
      "  File \u001b[92m/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:3701\u001b[39m in \u001b[95mrun_code\u001b[39m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  Cell \u001b[92mIn[18]\u001b[39m\u001b[92m, line 1\u001b[39m\n    get_ipython().run_cell_magic('time', '', \"\\nfrom scipy.stats import uniform, randint, loguniform\\n\\n# Define the pipeline\\npipeline = Pipeline([\\n    ('scaler', MinMaxScaler()),\\n    ('feature_selector', SelectPercentile(percentile=10)),\\n    ('classifier', HistGradientBoostingClassifier(random_state=315))\\n])\\n\\n# Best run: 0.6305\\n#   feature_selector__percentile: 25\\n#   classifier__validation_fraction: 0.1\\n#   classifier__n_iter_no_change: 20\\n#   classifier__min_samples_leaf: 10\\n#   classifier__max_iter: 250\\n#   classifier__max_depth: 15\\n#   classifier__max_bins: 128\\n#   classifier__learning_rate: 0.15\\n#   classifier__l2_regularization: 10.0\\n#   classifier__early_stopping: False\\n\\n# Define parameter distributions for randomized search\\nparam_distributions = {\\n    'feature_selector__percentile': [25],                      # 5 to 30 inclusive\\n    'classifier__learning_rate': loguniform(0.001, 0.3),       # Log-uniform between 0.001 and 0.3\\n    'classifier__max_iter': randint(100, 501),                 # 50 to 300 inclusive\\n    'classifier__max_depth': [10, 15, 20, 25 None],            # Keep discrete choices including None\\n    'classifier__min_samples_leaf': randint(5, 51),            # 5 to 50 inclusive\\n    'classifier__l2_regularization': loguniform(1e-4, 100.0),  # Log-uniform from 0.0001 to 10\\n    'classifier__max_bins': [64, 128, 255],                    # Keep binary choice\\n    'classifier__early_stopping': [True],                      # Always True\\n    'classifier__validation_fraction': [0.1],                  # Uniform between 0.05 and 0.30\\n    'classifier__n_iter_no_change': [30]                       # 5 to 20 inclusive\\n}\\n\\n# Create RandomizedSearchCV\\nrandom_search = RandomizedSearchCV(\\n    pipeline,\\n    param_distributions=param_distributions,\\n    n_iter=100,  # Number of random combinations to try\\n    cv=3,\\n    scoring='roc_auc',\\n    n_jobs=1,\\n    random_state=315,\\n    verbose=0\\n)\\n\\n# Fit the randomized search\\nsample_df = train_df.sample(n=100000, random_state=315)\\n\\nrandom_search.fit(\\n    sample_df.drop(columns=['diagnosed_diabetes']),\\n    sample_df['diagnosed_diabetes']\\n)\\n\\nprint(f'\\\\nBest cross-validation score (ROC-AUC): {random_search.best_score_:.4f}')\\nprint(f'\\\\nBest parameters:')\\n\\nfor param, value in random_search.best_params_.items():\\n    print(f'  {param}: {value}')\\n\\nprint()\\n\")\n",
      "  File \u001b[92m/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2572\u001b[39m in \u001b[95mrun_cell_magic\u001b[39m\n    result = fn(*args, **kwargs)\n",
      "  File \u001b[92m/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/IPython/core/magics/execution.py:1356\u001b[39m in \u001b[95mtime\u001b[39m\n    expr_ast = self.shell.compile.ast_parse(expr)\n",
      "\u001b[36m  \u001b[39m\u001b[36mFile \u001b[39m\u001b[32m/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/IPython/core/compilerop.py:86\u001b[39m\u001b[36m in \u001b[39m\u001b[35mast_parse\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mreturn compile(source, filename, symbol, self.flags | PyCF_ONLY_AST, 1)\u001b[39m\n",
      "  \u001b[36mFile \u001b[39m\u001b[32m<unknown>:27\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31m'classifier__max_depth': [10, 15, 20, 25 None],            # Keep discrete choices including None\u001b[39m\n                                          ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m invalid syntax. Perhaps you forgot a comma?\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from scipy.stats import uniform, randint, loguniform\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('scaler', MinMaxScaler()),\n",
    "    ('feature_selector', SelectPercentile(percentile=10)),\n",
    "    ('classifier', HistGradientBoostingClassifier(random_state=315))\n",
    "])\n",
    "\n",
    "# Best run: 0.6305\n",
    "#   feature_selector__percentile: 25\n",
    "#   classifier__validation_fraction: 0.1\n",
    "#   classifier__n_iter_no_change: 20\n",
    "#   classifier__min_samples_leaf: 10\n",
    "#   classifier__max_iter: 250\n",
    "#   classifier__max_depth: 15\n",
    "#   classifier__max_bins: 128\n",
    "#   classifier__learning_rate: 0.15\n",
    "#   classifier__l2_regularization: 10.0\n",
    "#   classifier__early_stopping: False\n",
    "\n",
    "# Define parameter distributions for randomized search\n",
    "param_distributions = {\n",
    "    'feature_selector__percentile': [25],                      # 5 to 30 inclusive\n",
    "    'classifier__learning_rate': loguniform(0.001, 0.3),       # Log-uniform between 0.001 and 0.3\n",
    "    'classifier__max_iter': randint(100, 501),                 # 50 to 300 inclusive\n",
    "    'classifier__max_depth': [10, 15, 20, 25, None],           # Keep discrete choices including None\n",
    "    'classifier__min_samples_leaf': randint(5, 51),            # 5 to 50 inclusive\n",
    "    'classifier__l2_regularization': loguniform(1e-4, 100.0),  # Log-uniform from 0.0001 to 10\n",
    "    'classifier__max_bins': [64, 128, 255],                    # Keep binary choice\n",
    "    'classifier__early_stopping': [True],                      # Always True\n",
    "    'classifier__validation_fraction': [0.1],                  # Uniform between 0.05 and 0.30\n",
    "    'classifier__n_iter_no_change': [30]                       # 5 to 20 inclusive\n",
    "}\n",
    "\n",
    "# Create RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(\n",
    "    pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=200,\n",
    "    cv=3,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=1,\n",
    "    random_state=315,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Fit the randomized search\n",
    "sample_df = train_df.sample(n=100000, random_state=315)\n",
    "\n",
    "random_search.fit(\n",
    "    sample_df.drop(columns=['diagnosed_diabetes']),\n",
    "    sample_df['diagnosed_diabetes']\n",
    ")\n",
    "\n",
    "print(f'\\nBest cross-validation score (ROC-AUC): {random_search.best_score_:.4f}')\n",
    "print(f'\\nBest parameters:')\n",
    "\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f'  {param}: {value}')\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e97e4",
   "metadata": {},
   "source": [
    "### 4.2. Evaluate optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best model from random search\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Evaluate with cross-validation to verify performance\n",
    "scores = cross_val_score(\n",
    "    best_model,\n",
    "    train_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_df['diagnosed_diabetes'],\n",
    "    cv=5,\n",
    "    scoring=make_scorer(roc_auc_score),\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "print(f'Optimized model cross-validation mean AUC: {np.mean(scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303945ae",
   "metadata": {},
   "source": [
    "### 4.3. Train final model on full dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e89d56be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the final model on the full dataset\n",
    "final_model = random_search.best_estimator_\n",
    "\n",
    "final_model.fit(\n",
    "    train_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_df['diagnosed_diabetes']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2d6f2",
   "metadata": {},
   "source": [
    "### 4.4. Compare baseline vs optimized performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for boxplot\n",
    "comparison_data = [baseline_scores, scores]\n",
    "labels = ['Baseline\\n(Unoptimized)', 'Optimized']\n",
    "\n",
    "# Create boxplot\n",
    "plt.title('Cross-Validation Performance')\n",
    "plt.boxplot(comparison_data, tick_labels=labels, patch_artist=True, widths=0.6)\n",
    "plt.ylabel('ROC-AUC Score')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f'Baseline  - Mean: {np.mean(baseline_scores):.4f}, Std: {np.std(baseline_scores):.4f}')\n",
    "print(f'Optimized - Mean: {np.mean(scores):.4f}, Std: {np.std(scores):.4f}')\n",
    "print(f'Improvement: {improvement:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5235f51",
   "metadata": {},
   "source": [
    "### 4.5. Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "# Get predictions for both models using cross-validation\n",
    "baseline_predictions = cross_val_predict(\n",
    "    baseline_model,\n",
    "    train_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_df['diagnosed_diabetes'],\n",
    "    cv=3\n",
    ")\n",
    "\n",
    "optimized_predictions = cross_val_predict(\n",
    "    final_model,\n",
    "    train_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_df['diagnosed_diabetes'],\n",
    "    cv=5\n",
    ")\n",
    "\n",
    "# Create confusion matrices\n",
    "baseline_cm = confusion_matrix(train_df['diagnosed_diabetes'], baseline_predictions)\n",
    "optimized_cm = confusion_matrix(train_df['diagnosed_diabetes'], optimized_predictions)\n",
    "\n",
    "# Plot confusion matrices side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Baseline confusion matrix\n",
    "disp1 = ConfusionMatrixDisplay(confusion_matrix=baseline_cm, display_labels=['No Diabetes', 'Diabetes'])\n",
    "disp1.plot(ax=axes[0], cmap='Blues', values_format='d')\n",
    "axes[0].set_title('Baseline Model\\n(Unoptimized)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Optimized confusion matrix\n",
    "disp2 = ConfusionMatrixDisplay(confusion_matrix=optimized_cm, display_labels=['No Diabetes', 'Diabetes'])\n",
    "disp2.plot(ax=axes[1], cmap='Greens', values_format='d')\n",
    "axes[1].set_title('Optimized Model', fontsize=12, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display metrics for both models\n",
    "def calculate_metrics(cm):\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "    return accuracy, precision, recall, f1, specificity\n",
    "\n",
    "baseline_metrics = calculate_metrics(baseline_cm)\n",
    "optimized_metrics = calculate_metrics(optimized_cm)\n",
    "\n",
    "print('\\nConfusion Matrix Metrics:')\n",
    "print('=' * 70)\n",
    "print(f'{\"Metric\":<15} {\"Baseline\":<15} {\"Optimized\":<15} {\"Difference\":<15}')\n",
    "print('-' * 70)\n",
    "\n",
    "metric_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'Specificity']\n",
    "for name, base, opt in zip(metric_names, baseline_metrics, optimized_metrics):\n",
    "    diff = opt - base\n",
    "    sign = '+' if diff >= 0 else ''\n",
    "    print(f'{name:<15} {base:<15.4f} {opt:<15.4f} {sign}{diff:<15.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b9d34",
   "metadata": {},
   "source": [
    "## 5. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cc18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model directory and ensure it exists\n",
    "model_dir = Path('../models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create model filename\n",
    "model_name = 'gradient_boosting'\n",
    "model_path = model_dir / f'{model_name}.joblib'\n",
    "\n",
    "# Save the final model\n",
    "joblib.dump(final_model, model_path)\n",
    "print('Model saved to:', model_path)\n",
    "print(f'File size: {model_path.stat().st_size / (1024**2):.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af43164",
   "metadata": {},
   "source": [
    "## 6. Save model description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model description filename\n",
    "markdown_path = model_dir / f'{model_name}.md'\n",
    "\n",
    "# Create markdown description\n",
    "markdown_content = f\"\"\"# Logistic Regression Model for Diabetes Prediction\n",
    "\n",
    "## Model overview\n",
    "\n",
    "This dataset contains a scikit-learn Pipeline object that chains together multiple preprocessing, feature engineering, and modeling steps into a single estimator. Pipelines ensure that all transformations are applied consistently during both training and inference, preventing data leakage and simplifying deployment. The pipeline was optimized using RandomizedSearchCV. Optimized hyperparameters are marked with *[optimized]* in the pipeline component descriptions below.\n",
    "\n",
    "For details on model optimization and training, see the [logistic regression optimization and training notebook](https://github.com/gperdrizet/diabetes-prediction/blob/main/notebooks/01.1-logistic_regression_model.ipynb) on GitHub.\n",
    "\n",
    "## Files\n",
    "\n",
    "- **Model file**: `{model_path.name}` (scikit-learn Pipeline object serialized with joblib)\n",
    "- **Custom transformers**: `logistic_regression_transformers.py` (required for model deserialization)\n",
    "- **Documentation**: `{markdown_path.name}`\n",
    "\n",
    "Key features:\n",
    "- **End-to-end processing**: Automatically handles all preprocessing from raw data to predictions\n",
    "- **Reproducible transformations**: All fitted parameters (scalers, encoders, PCA components) are preserved\n",
    "- **Hyperparameter optimization**: Parameters across all pipeline steps were jointly optimized\n",
    "\n",
    "## Training information\n",
    "\n",
    "- **Training date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Training samples**: {len(train_df):,}\n",
    "- **Cross-validation score (ROC-AUC)**: {random_search.best_score_:.4f}\n",
    "\n",
    "## Hyperparameter optimization\n",
    "\n",
    "- **Method**: Randomized Search CV\n",
    "- **Cross-validation folds**: {CV_FOLDS}\n",
    "- **Iterations**: {optimal_n_iter}\n",
    "- **Scoring metric**: ROC-AUC\n",
    "- **Optimization runtime**: {optimization_time:.1f} seconds ({optimization_time/60:.1f} minutes)\n",
    "\n",
    "## Inference performance\n",
    "\n",
    "Measured on test dataset with {len(test_df):,} samples using `tracemalloc` to track peak memory allocation:\n",
    "\n",
    "- **Inference time**: {inference_time:.4f} seconds\n",
    "- **Throughput**: {samples_per_second:,.0f} samples/second\n",
    "- **Peak memory**: {memory_footprint_gb:.4f} GB\n",
    "\n",
    "## Pipeline components\n",
    "\n",
    "### 1. Preprocessing\n",
    "\n",
    "#### ID column removal\n",
    "- **ID column dropper**: Automatically removes the 'id' column from input data (custom transformer)\n",
    "\n",
    "#### Numerical features\n",
    "- **Standardization**: Standard scaling (mean=0, std=1)\n",
    "- **Features**: age, alcohol_consumption_per_week, diet_score, physical_activity_minutes_per_week, sleep_hours_per_day, screen_time_hours_per_day, bmi, waist_to_hip_ratio, systolic_bp, diastolic_bp, heart_rate, cholesterol_total, hdl_cholesterol, ldl_cholesterol, triglycerides\n",
    "\n",
    "#### Ordinal features\n",
    "- **Ordinal encoding**: education_level, income_level\n",
    "\n",
    "#### Nominal features\n",
    "- **One-hot encoding**: gender, ethnicity, smoking_status, employment_status, family_history_diabetes, hypertension_history, cardiovascular_history (drop first category)\n",
    "\n",
    "### 2. Feature engineering\n",
    "\n",
    "- **Polynomial features**:\n",
    "  - Degree: {random_search.best_params_.get('poly__degree', 'N/A')} *[optimized]*\n",
    "  - Include bias: {random_search.best_params_.get('poly__include_bias', 'N/A')} *[optimized]*\n",
    "  - Interaction only: {random_search.best_params_.get('poly__interaction_only', 'N/A')} *[optimized]*\n",
    "\n",
    "- **Constant feature removal**: Removes features with zero variance (custom transformer)\n",
    "\n",
    "- **Post-polynomial standardization**: Standard scaling after polynomial transformation\n",
    "\n",
    "- **PCA dimensionality reduction**:\n",
    "  - Components: {random_search.best_params_.get('pca__n_components', 'N/A')} *[optimized]*\n",
    "  - SVD solver: {random_search.best_params_.get('pca__svd_solver', 'N/A')} *[optimized]*\n",
    "  - Whiten: {random_search.best_params_.get('pca__whiten', 'N/A')} *[optimized]*\n",
    "\n",
    "### 3. Classifier\n",
    "\n",
    "- **Algorithm**: Logistic regression\n",
    "- **Penalty**: {random_search.best_params_.get('logit__penalty', 'N/A')} *[optimized]*\n",
    "- **Regularization (C)**: {f\"{random_search.best_params_.get('logit__C'):.4f}\" if random_search.best_params_.get('logit__C') is not None else 'N/A'} *[optimized]*\n",
    "- **Max iterations**: {random_search.best_params_.get('logit__max_iter', 'N/A')}\n",
    "- **Class weight**: {random_search.best_params_.get('logit__class_weight', 'N/A')}\n",
    "\n",
    "## Custom transformers\n",
    "\n",
    "The model uses two custom scikit-learn transformers defined in `logistic_regression_transformers.py`:\n",
    "\n",
    "### IDColumnDropper\n",
    "Automatically removes the 'id' column from input DataFrames before processing. This allows the model to accept raw test data without manual preprocessing.\n",
    "\n",
    "### ConstantFeatureRemover\n",
    "Removes features with zero variance after polynomial transformation. This eliminates redundant features that don't contribute to model predictions, reducing dimensionality and improving computational efficiency.\n",
    "\n",
    "**Important**: The `logistic_regression_transformers.py` file must be available in the Python path when loading the model, as joblib stores references to these classes and needs to import them during deserialization.\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the models directory to the path (adjust as needed)\n",
    "sys.path.insert(0, str(Path('models').resolve()))\n",
    "\n",
    "# Load the model (this will import the custom transformers)\n",
    "model = joblib.load('models/{model_path.name}')\n",
    "\n",
    "# Prepare test data (pipeline will automatically handle 'id' column)\n",
    "X_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "```\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Input data can include the 'id' column - it will be automatically removed by the pipeline\n",
    "- The pipeline handles all preprocessing and feature engineering automatically\n",
    "- The `logistic_regression_transformers.py` file must be in the Python path when loading the model\n",
    "\"\"\"\n",
    "\n",
    "with open(markdown_path, 'w') as f:\n",
    "    f.write(markdown_content)\n",
    "\n",
    "print('Model description saved to:', markdown_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
