{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b636d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-06 15:55:42.878025: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765036542.899447   61345 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765036542.906146   61345 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Running on: CPU (24 cores)\n",
      "GPU disabled: CUDA drivers not available in dev container\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Disable GPU (CUDA drivers not available in dev container)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n",
    "\n",
    "# Set environment variable to limit thread usage\n",
    "os.environ['OMP_NUM_THREADS'] = '8'\n",
    "\n",
    "# Third party imports\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Add models directory to path for custom transformers\n",
    "models_path = Path('../models').resolve()\n",
    "sys.path.insert(0, str(models_path))\n",
    "\n",
    "# Import custom transformers from models directory\n",
    "from logistic_regression_transformers import IDColumnDropper, IQRClipper\n",
    "\n",
    "# Import ensemble modules from functions package\n",
    "from functions import ensemble_database\n",
    "from functions.ensemble_hill_climbing import (\n",
    "    generate_random_pipeline,\n",
    "    calculate_ensemble_diversity,\n",
    "    quick_optimize_pipeline,\n",
    "    adaptive_simulated_annealing_acceptance,\n",
    "    update_temperature,\n",
    "    compute_pipeline_hash,\n",
    "    log_iteration\n",
    ")\n",
    "from functions.ensemble_stage2_model import (\n",
    "    build_stage2_dnn,\n",
    "    train_stage2_dnn,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    evaluate_ensemble\n",
    ")\n",
    "\n",
    "# Configure TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Running on: CPU (24 cores)\")\n",
    "print(f\"GPU disabled: CUDA drivers not available in dev container\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c63dc5d",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d004dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Database initialized at: /workspaces/diabetes-prediction/data/ensemble_training.db\n",
      "Configuration:\n",
      "  Random state: 315\n",
      "  K-folds: 5\n",
      "  Max iterations: 500\n",
      "  Plateau threshold: 100\n",
      "  Resume from checkpoint: False\n",
      "  Ensemble directory: ../models/ensemble_stage1_models\n",
      "  Database: /workspaces/diabetes-prediction/data/ensemble_training.db\n"
     ]
    }
   ],
   "source": [
    "# Random state for reproducibility\n",
    "RANDOM_STATE = 315\n",
    "\n",
    "# K-fold configuration\n",
    "N_FOLDS = 5\n",
    "\n",
    "# Hill climbing configuration\n",
    "MAX_ITERATIONS = 500\n",
    "PLATEAU_ITERATIONS = 100  # Stop if no improvement for this many iterations\n",
    "BASE_TEMPERATURE = 0.01\n",
    "TEMPERATURE_DECAY = 0.995\n",
    "\n",
    "# Stage 1 optimization\n",
    "QUICK_OPTIMIZE_ITERATIONS = 10\n",
    "QUICK_OPTIMIZE_CV = 3\n",
    "QUICK_OPTIMIZE_JOBS = 8\n",
    "\n",
    "# Founder ensemble optimization (more thorough)\n",
    "FOUNDER_OPTIMIZE_ITERATIONS = 20\n",
    "FOUNDER_OPTIMIZE_CV = 3\n",
    "\n",
    "# Stage 2 DNN configuration\n",
    "STAGE2_EPOCHS = 100\n",
    "STAGE2_BATCH_SIZE = 128\n",
    "STAGE2_PATIENCE = 10\n",
    "\n",
    "# Checkpoint configuration\n",
    "RESUME_FROM_CHECKPOINT = False  # Set to True to resume from saved checkpoint\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "MODELS_DIR = Path('../models')\n",
    "ENSEMBLE_DIR = MODELS_DIR / 'ensemble_stage1_models'\n",
    "CHECKPOINT_PATH = MODELS_DIR / 'ensemble_checkpoint.pkl'\n",
    "\n",
    "# Create directories\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ENSEMBLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize SQLite database for logging\n",
    "ensemble_database.init_database()\n",
    "\n",
    "print(f\"Configuration:\")\n",
    "print(f\"  Random state: {RANDOM_STATE}\")\n",
    "print(f\"  K-folds: {N_FOLDS}\")\n",
    "print(f\"  Max iterations: {MAX_ITERATIONS}\")\n",
    "print(f\"  Plateau threshold: {PLATEAU_ITERATIONS}\")\n",
    "print(f\"  Resume from checkpoint: {RESUME_FROM_CHECKPOINT}\")\n",
    "print(f\"  Ensemble directory: {ENSEMBLE_DIR}\")\n",
    "print(f\"  Database: {ensemble_database.DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d227c51",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ddfa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (700000, 26)\n",
      "Class distribution:\n",
      "diagnosed_diabetes\n",
      "1.0    0.623296\n",
      "0.0    0.376704\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_df_path = 'https://gperdrizet.github.io/FSA_devops/assets/data/unit3/diabetes_prediction_train.csv'\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(train_df['diagnosed_diabetes'].value_counts(normalize=True))\n",
    "\n",
    "# Define label and features\n",
    "label = 'diagnosed_diabetes'\n",
    "\n",
    "# Feature definitions (from logistic regression notebook)\n",
    "numerical_features = [\n",
    "    'age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week',\n",
    "    'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi',\n",
    "    'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate',\n",
    "    'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides',\n",
    "    'family_history_diabetes', 'hypertension_history', 'cardiovascular_history'\n",
    "]\n",
    "\n",
    "ordinal_features = ['education_level', 'income_level']\n",
    "education_categories = [['No formal', 'Highschool', 'Graduate', 'Postgraduate']]\n",
    "income_categories = [['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']]\n",
    "\n",
    "nominal_features = ['gender', 'ethnicity', 'smoking_status', 'employment_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eace57b",
   "metadata": {},
   "source": [
    "### Create K-Fold Splits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da7141fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created 5 stratified folds:\n",
      "  Fold 0: 560000 train, 140000 val\n",
      "  Fold 1: 560000 train, 140000 val\n",
      "  Fold 2: 560000 train, 140000 val\n",
      "  Fold 3: 560000 train, 140000 val\n",
      "  Fold 4: 560000 train, 140000 val\n"
     ]
    }
   ],
   "source": [
    "# Create stratified k-fold splits\n",
    "skf = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=RANDOM_STATE)\n",
    "\n",
    "# Store fold indices\n",
    "fold_indices = []\n",
    "for train_idx, val_idx in skf.split(train_df, train_df[label]):\n",
    "    fold_indices.append((train_idx, val_idx))\n",
    "\n",
    "print(f\"Created {N_FOLDS} stratified folds:\")\n",
    "for i, (train_idx, val_idx) in enumerate(fold_indices):\n",
    "    print(f\"  Fold {i}: {len(train_idx)} train, {len(val_idx)} val\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a2b790",
   "metadata": {},
   "source": [
    "### Create Base Preprocessor\n",
    "\n",
    "This preprocessor will be shared across all stage 1 models for consistent encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2ae61db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base preprocessor created\n",
      "  Numerical features: 18\n",
      "  Ordinal features: 2\n",
      "  Nominal features: 4\n"
     ]
    }
   ],
   "source": [
    "# Create numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('clipper', IQRClipper(iqr_multiplier=2.0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create encoders\n",
    "ordinal_encoder = OrdinalEncoder(\n",
    "    categories=education_categories + income_categories,\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "onehot_encoder = OneHotEncoder(\n",
    "    drop='first',\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore'\n",
    ")\n",
    "\n",
    "# Create base preprocessor\n",
    "base_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('ord', ordinal_encoder, ordinal_features),\n",
    "        ('nom', onehot_encoder, nominal_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Base preprocessor created\")\n",
    "print(f\"  Numerical features: {len(numerical_features)}\")\n",
    "print(f\"  Ordinal features: {len(ordinal_features)}\")\n",
    "print(f\"  Nominal features: {len(nominal_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972055fd",
   "metadata": {},
   "source": [
    "## Initialize or Resume Ensemble\n",
    "\n",
    "### Option 1: Initialize Founder Ensemble (if starting fresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9609e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING FOUNDER ENSEMBLE (5 models, one per fold)\n",
      "================================================================================\n",
      "\n",
      "Training founder model 1/5 (Fold 0)\n",
      "--------------------------------------------------------------------------------\n",
      "  Pipeline config:\n",
      "    Classifier: extra_trees\n",
      "    Transformers: ['reciprocal', 'difference', 'log']\n",
      "    Use PCA: True\n",
      "  Optimizing on full training data...\n",
      "  Pipeline config:\n",
      "    Classifier: extra_trees\n",
      "    Transformers: ['reciprocal', 'difference', 'log']\n",
      "    Use PCA: True\n",
      "  Optimizing on full training data...\n",
      "Optimization failed: n_components=165 must be between 0 and min(n_samples, n_features)=6 with svd_solver='full'\n",
      "  Optimization complete (4.7s)\n",
      "  CV ROC-AUC: 0.000000\n",
      "Optimization failed: n_components=165 must be between 0 and min(n_samples, n_features)=6 with svd_solver='full'\n",
      "  Optimization complete (4.7s)\n",
      "  CV ROC-AUC: 0.000000\n"
     ]
    },
    {
     "ename": "NotFittedError",
     "evalue": "This ColumnTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotFittedError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Generate predictions on validation fold\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(optimized_pipeline, \u001b[33m'\u001b[39m\u001b[33mpredict_proba\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m     val_pred = \u001b[43moptimized_pipeline\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict_proba\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m)\u001b[49m[:, \u001b[32m1\u001b[39m]\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     57\u001b[39m     val_pred = optimized_pipeline.decision_function(X_val)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/pipeline.py:546\u001b[39m, in \u001b[36mPipeline.predict_proba\u001b[39m\u001b[34m(self, X, **predict_proba_params)\u001b[39m\n\u001b[32m    544\u001b[39m Xt = X\n\u001b[32m    545\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, name, transform \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._iter(with_final=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m546\u001b[39m     Xt = \u001b[43mtransform\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mXt\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    547\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.steps[-\u001b[32m1\u001b[39m][\u001b[32m1\u001b[39m].predict_proba(Xt, **predict_proba_params)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/utils/_set_output.py:140\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    138\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    141\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    142\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    143\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[32m    144\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    145\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    146\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/compose/_column_transformer.py:770\u001b[39m, in \u001b[36mColumnTransformer.transform\u001b[39m\u001b[34m(self, X)\u001b[39m\n\u001b[32m    753\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mtransform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X):\n\u001b[32m    754\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Transform X separately by each transformer, concatenate results.\u001b[39;00m\n\u001b[32m    755\u001b[39m \n\u001b[32m    756\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    768\u001b[39m \u001b[33;03m        sparse matrices.\u001b[39;00m\n\u001b[32m    769\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m770\u001b[39m     \u001b[43mcheck_is_fitted\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    771\u001b[39m     X = _check_X(X)\n\u001b[32m    773\u001b[39m     fit_dataframe_and_transform_dataframe = \u001b[38;5;28mhasattr\u001b[39m(\n\u001b[32m    774\u001b[39m         \u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mfeature_names_in_\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    775\u001b[39m     ) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(X, \u001b[33m\"\u001b[39m\u001b[33mcolumns\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/sklearn/utils/validation.py:1390\u001b[39m, in \u001b[36mcheck_is_fitted\u001b[39m\u001b[34m(estimator, attributes, msg, all_or_any)\u001b[39m\n\u001b[32m   1385\u001b[39m     fitted = [\n\u001b[32m   1386\u001b[39m         v \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mvars\u001b[39m(estimator) \u001b[38;5;28;01mif\u001b[39;00m v.endswith(\u001b[33m\"\u001b[39m\u001b[33m_\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m v.startswith(\u001b[33m\"\u001b[39m\u001b[33m__\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1387\u001b[39m     ]\n\u001b[32m   1389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m fitted:\n\u001b[32m-> \u001b[39m\u001b[32m1390\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m NotFittedError(msg % {\u001b[33m\"\u001b[39m\u001b[33mname\u001b[39m\u001b[33m\"\u001b[39m: \u001b[38;5;28mtype\u001b[39m(estimator).\u001b[34m__name__\u001b[39m})\n",
      "\u001b[31mNotFittedError\u001b[39m: This ColumnTransformer instance is not fitted yet. Call 'fit' with appropriate arguments before using this estimator."
     ]
    }
   ],
   "source": [
    "if not RESUME_FROM_CHECKPOINT:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"INITIALIZING FOUNDER ENSEMBLE (5 models, one per fold)\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ensemble_models = []\n",
    "    fold_predictions = [None] * N_FOLDS\n",
    "    fold_labels = [None] * N_FOLDS\n",
    "    founder_scores = []\n",
    "    \n",
    "    # Train one founder model per fold\n",
    "    for fold in range(N_FOLDS):\n",
    "        print(f\"\\nTraining founder model {fold + 1}/{N_FOLDS} (Fold {fold})\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        # Get fold data\n",
    "        train_idx, val_idx = fold_indices[fold]\n",
    "        X_train = train_df.iloc[train_idx].drop(columns=[label])\n",
    "        y_train = train_df.iloc[train_idx][label]\n",
    "        X_val = train_df.iloc[val_idx].drop(columns=[label])\n",
    "        y_val = train_df.iloc[val_idx][label]\n",
    "        \n",
    "        # Generate random pipeline for founder\n",
    "        pipeline, metadata = generate_random_pipeline(\n",
    "            iteration=fold,\n",
    "            random_state=RANDOM_STATE + fold,\n",
    "            base_preprocessor=base_preprocessor\n",
    "        )\n",
    "        \n",
    "        print(f\"  Pipeline config:\")\n",
    "        print(f\"    Classifier: {metadata['classifier_type']}\")\n",
    "        print(f\"    Transformers: {metadata['transformers_used']}\")\n",
    "        print(f\"    Use PCA: {metadata['use_pca']}\")\n",
    "        \n",
    "        # Optimize on this fold's training data\n",
    "        print(f\"  Optimizing on fold training data...\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        optimized_pipeline, cv_score = quick_optimize_pipeline(\n",
    "            pipeline=pipeline,\n",
    "            X=X_train,\n",
    "            y=y_train,\n",
    "            n_iter=FOUNDER_OPTIMIZE_ITERATIONS,\n",
    "            cv=FOUNDER_OPTIMIZE_CV,\n",
    "            n_jobs=QUICK_OPTIMIZE_JOBS,\n",
    "            random_state=RANDOM_STATE + fold\n",
    "        )\n",
    "        \n",
    "        optimization_time = time.time() - start_time\n",
    "        print(f\"  Optimization complete ({optimization_time:.1f}s)\")\n",
    "        print(f\"  CV ROC-AUC: {cv_score:.6f}\")\n",
    "        \n",
    "        # Generate predictions on validation fold\n",
    "        if hasattr(optimized_pipeline, 'predict_proba'):\n",
    "            val_pred = optimized_pipeline.predict_proba(X_val)[:, 1]\n",
    "        else:\n",
    "            val_pred = optimized_pipeline.decision_function(X_val)\n",
    "        \n",
    "        fold_predictions[fold] = val_pred\n",
    "        fold_labels[fold] = y_val.values\n",
    "        \n",
    "        # Calculate validation AUC\n",
    "        val_auc = roc_auc_score(y_val, val_pred)\n",
    "        founder_scores.append(val_auc)\n",
    "        \n",
    "        print(f\"  Validation ROC-AUC: {val_auc:.6f}\")\n",
    "        \n",
    "        # Save founder model\n",
    "        ensemble_models.append(optimized_pipeline)\n",
    "        model_path = ENSEMBLE_DIR / f'founder_model_{fold}.joblib'\n",
    "        joblib.dump(optimized_pipeline, model_path)\n",
    "        \n",
    "        # Log founder\n",
    "        pipeline_hash = compute_pipeline_hash(optimized_pipeline, metadata)\n",
    "        ensemble_id = f\"founder_{fold}\"\n",
    "        log_iteration(\n",
    "            iteration=fold,\n",
    "            fold=fold,\n",
    "            accepted=True,\n",
    "            rejection_reason='founder',\n",
    "            pipeline_hash=pipeline_hash,\n",
    "            stage1_cv_score=cv_score,\n",
    "            stage1_val_auc=val_auc,\n",
    "            stage2_val_auc=0.0,  # Will update after stage 2 training\n",
    "            ensemble_size=fold + 1,\n",
    "            diversity_score=0.0,\n",
    "            temperature=BASE_TEMPERATURE,\n",
    "            metadata=metadata,\n",
    "            ensemble_id=ensemble_id\n",
    "        )\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"FOUNDER ENSEMBLE COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Mean validation ROC-AUC: {np.mean(founder_scores):.6f} Â± {np.std(founder_scores):.6f}\")\n",
    "    print(f\"Per-fold scores: {[f'{s:.6f}' for s in founder_scores]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d339f",
   "metadata": {},
   "source": [
    "### Train Initial Stage 2 DNN (if starting fresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcfe2f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not RESUME_FROM_CHECKPOINT:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"TRAINING INITIAL STAGE 2 DNN\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    # For initial stage 2, use simple architecture\n",
    "    stage2_model = build_stage2_dnn(\n",
    "        n_models=len(ensemble_models),\n",
    "        n_layers=1,\n",
    "        units_per_layer=64,\n",
    "        dropout=0.3,\n",
    "        batch_norm=True,\n",
    "        activation='relu',\n",
    "        learning_rate=0.001\n",
    "    )\n",
    "    \n",
    "    print(f\"Initial stage 2 DNN architecture:\")\n",
    "    stage2_model.summary()\n",
    "    \n",
    "    # Train on each fold and evaluate\n",
    "    fold_stage2_scores = []\n",
    "    \n",
    "    for fold in range(N_FOLDS):\n",
    "        print(f\"\\nTraining stage 2 on fold {fold}...\")\n",
    "        \n",
    "        # Get predictions for this fold\n",
    "        X_stage2 = fold_predictions[fold].reshape(-1, 1)\n",
    "        y_stage2 = fold_labels[fold]\n",
    "        \n",
    "        # Use a simple train/val split for initial training\n",
    "        split_idx = int(len(X_stage2) * 0.8)\n",
    "        X_train_s2 = X_stage2[:split_idx]\n",
    "        y_train_s2 = y_stage2[:split_idx]\n",
    "        X_val_s2 = X_stage2[split_idx:]\n",
    "        y_val_s2 = y_stage2[split_idx:]\n",
    "        \n",
    "        # Train (pass ensemble_id as log_path parameter for logging)\n",
    "        ensemble_id = f\"founder_stage2_fold{fold}\"\n",
    "        stage2_model, history = train_stage2_dnn(\n",
    "            model=stage2_model,\n",
    "            X_train=X_train_s2,\n",
    "            y_train=y_train_s2,\n",
    "            X_val=X_val_s2,\n",
    "            y_val=y_val_s2,\n",
    "            epochs=50,\n",
    "            batch_size=32,\n",
    "            patience=10,\n",
    "            log_path=ensemble_id,  # Pass ensemble_id as string\n",
    "            iteration=fold,\n",
    "            fold=fold\n",
    "        )\n",
    "        \n",
    "        # Evaluate on full fold\n",
    "        fold_pred_s2 = stage2_model.predict(X_stage2, verbose=0).flatten()\n",
    "        fold_auc_s2 = roc_auc_score(y_stage2, fold_pred_s2)\n",
    "        fold_stage2_scores.append(fold_auc_s2)\n",
    "        \n",
    "        print(f\"  Fold {fold} stage 2 ROC-AUC: {fold_auc_s2:.6f}\")\n",
    "    \n",
    "    # Set initial best score\n",
    "    best_ensemble_score = np.mean(fold_stage2_scores)\n",
    "    \n",
    "    print(f\"\\nInitial ensemble performance:\")\n",
    "    print(f\"  Mean stage 2 ROC-AUC: {best_ensemble_score:.6f} Â± {np.std(fold_stage2_scores):.6f}\")\n",
    "    \n",
    "    # Initialize hill climbing variables\n",
    "    start_iteration = N_FOLDS\n",
    "    current_fold = 0\n",
    "    temperature = BASE_TEMPERATURE\n",
    "    acceptance_history = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4dbf52",
   "metadata": {},
   "source": [
    "## Hill Climbing Loop\n",
    "\n",
    "Iteratively add diverse models with simulated annealing acceptance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eb40c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"STARTING HILL CLIMBING LOOP\")\n",
    "print(f\"{'=' * 80}\")\n",
    "\n",
    "iterations_since_improvement = 0\n",
    "consecutive_rejections = 0\n",
    "\n",
    "for iteration in range(start_iteration, MAX_ITERATIONS):\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"Iteration {iteration + 1}/{MAX_ITERATIONS}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Current ensemble size: {len(ensemble_models)}\")\n",
    "    print(f\"Best score: {best_ensemble_score:.6f}\")\n",
    "    print(f\"Temperature: {temperature:.6f}\")\n",
    "    print(f\"Iterations since improvement: {iterations_since_improvement}/{PLATEAU_ITERATIONS}\")\n",
    "    \n",
    "    # Get current fold\n",
    "    current_fold = iteration % N_FOLDS\n",
    "    print(f\"Using fold {current_fold} for validation\")\n",
    "    \n",
    "    # Generate random pipeline\n",
    "    pipeline, metadata = generate_random_pipeline(\n",
    "        iteration=iteration,\n",
    "        random_state=RANDOM_STATE + iteration,\n",
    "        base_preprocessor=base_preprocessor\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nPipeline configuration:\")\n",
    "    print(f\"  Row sample: {metadata['row_sample_pct']:.1%}\")\n",
    "    print(f\"  Col sample: {metadata['col_sample_pct']:.1%}\")\n",
    "    print(f\"  Classifier: {metadata['classifier_type']}\")\n",
    "    print(f\"  Transformers: {', '.join(metadata['transformers_used']) if metadata['transformers_used'] else 'None'}\")\n",
    "    \n",
    "    # Quick optimize\n",
    "    print(f\"\\nOptimizing pipeline...\")\n",
    "    optimized_pipeline, cv_score = quick_optimize_pipeline(\n",
    "        pipeline=pipeline,\n",
    "        X_train=X_train,\n",
    "        y_train=y_train,\n",
    "        random_state=RANDOM_STATE + iteration,\n",
    "        n_iter=QUICK_OPTIMIZE_ITERATIONS,\n",
    "        cv_folds=QUICK_OPTIMIZE_CV,\n",
    "        n_jobs=QUICK_OPTIMIZE_JOBS\n",
    "    )\n",
    "    \n",
    "    print(f\"  Stage 1 CV ROC-AUC: {cv_score:.6f}\")\n",
    "    \n",
    "    # Evaluate on validation fold\n",
    "    X_val_fold = fold_data[current_fold]['X_val']\n",
    "    y_val_fold = fold_data[current_fold]['y_val']\n",
    "    \n",
    "    if hasattr(optimized_pipeline, 'predict_proba'):\n",
    "        val_pred = optimized_pipeline.predict_proba(X_val_fold)[:, 1]\n",
    "    else:\n",
    "        val_pred = optimized_pipeline.decision_function(X_val_fold)\n",
    "    \n",
    "    val_auc = roc_auc_score(y_val_fold, val_pred)\n",
    "    print(f\"  Stage 1 validation ROC-AUC: {val_auc:.6f}\")\n",
    "    \n",
    "    # Add to candidate pool and retrain stage 2\n",
    "    print(f\"\\nEvaluating ensemble with candidate...\")\n",
    "    candidate_ensemble = ensemble_models + [optimized_pipeline]\n",
    "    \n",
    "    # Evaluate ensemble on current fold\n",
    "    candidate_score = evaluate_ensemble(\n",
    "        stage1_models=candidate_ensemble,\n",
    "        stage2_model=stage2_model,\n",
    "        X=X_val_fold,\n",
    "        y=y_val_fold\n",
    "    )\n",
    "    \n",
    "    print(f\"  Candidate ensemble ROC-AUC: {candidate_score:.6f}\")\n",
    "    \n",
    "    # Calculate diversity\n",
    "    all_predictions = []\n",
    "    for model in candidate_ensemble:\n",
    "        if hasattr(model, 'predict_proba'):\n",
    "            pred = model.predict_proba(X_val_fold)[:, 1]\n",
    "        else:\n",
    "            pred = model.decision_function(X_val_fold)\n",
    "        all_predictions.append(pred)\n",
    "    \n",
    "    all_predictions = np.column_stack(all_predictions)\n",
    "    diversity_score = calculate_ensemble_diversity(all_predictions)\n",
    "    print(f\"  Diversity score: {diversity_score:.6f}\")\n",
    "    \n",
    "    # Simulated annealing acceptance\n",
    "    accept, reason = adaptive_simulated_annealing_acceptance(\n",
    "        current_score=best_ensemble_score,\n",
    "        candidate_score=candidate_score,\n",
    "        temperature=temperature,\n",
    "        random_state=RANDOM_STATE + iteration\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nDecision: {'âœ“ ACCEPT' if accept else 'âœ— REJECT'}\")\n",
    "    print(f\"  Reason: {reason}\")\n",
    "    \n",
    "    # Log iteration\n",
    "    pipeline_hash = compute_pipeline_hash(optimized_pipeline, metadata)\n",
    "    ensemble_id = f\"iter_{iteration}_fold_{current_fold}\"\n",
    "    log_iteration(\n",
    "        iteration=iteration,\n",
    "        fold=current_fold,\n",
    "        accepted=accept,\n",
    "        rejection_reason=reason,\n",
    "        pipeline_hash=pipeline_hash,\n",
    "        stage1_cv_score=cv_score,\n",
    "        stage1_val_auc=val_auc,\n",
    "        stage2_val_auc=candidate_score,\n",
    "        ensemble_size=len(candidate_ensemble) if accept else len(ensemble_models),\n",
    "        diversity_score=diversity_score,\n",
    "        temperature=temperature,\n",
    "        metadata=metadata,\n",
    "        ensemble_id=ensemble_id\n",
    "    )\n",
    "    \n",
    "    # Update ensemble if accepted\n",
    "    if accept:\n",
    "        ensemble_models.append(optimized_pipeline)\n",
    "        acceptance_history.append(True)\n",
    "        consecutive_rejections = 0\n",
    "        \n",
    "        # Save model\n",
    "        model_path = ENSEMBLE_DIR / f'model_{iteration}.joblib'\n",
    "        joblib.dump(optimized_pipeline, model_path)\n",
    "        \n",
    "        # Check if this is the best score\n",
    "        if candidate_score > best_ensemble_score:\n",
    "            print(f\"  ðŸŽ‰ New best score: {candidate_score:.6f} (Î”={candidate_score - best_ensemble_score:.6f})\")\n",
    "            best_ensemble_score = candidate_score\n",
    "            iterations_since_improvement = 0\n",
    "        else:\n",
    "            iterations_since_improvement += 1\n",
    "    else:\n",
    "        acceptance_history.append(False)\n",
    "        consecutive_rejections += 1\n",
    "        iterations_since_improvement += 1\n",
    "    \n",
    "    # Update temperature\n",
    "    temperature = update_temperature(\n",
    "        iteration=iteration,\n",
    "        acceptance_history=acceptance_history,\n",
    "        current_temperature=temperature,\n",
    "        base_temperature=BASE_TEMPERATURE,\n",
    "        decay_rate=TEMPERATURE_DECAY\n",
    "    )\n",
    "    \n",
    "    # Check termination conditions\n",
    "    if iterations_since_improvement >= PLATEAU_ITERATIONS:\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"TERMINATING: No improvement for {PLATEAU_ITERATIONS} iterations\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"HILL CLIMBING COMPLETE\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Final ensemble size: {len(ensemble_models)}\")\n",
    "print(f\"Best ensemble ROC-AUC: {best_ensemble_score:.6f}\")\n",
    "print(f\"Total iterations: {iteration + 1}\")\n",
    "print(f\"Acceptance rate: {sum(acceptance_history) / len(acceptance_history):.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55640a1c",
   "metadata": {},
   "source": [
    "## Save Final Checkpoint and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78127513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint\n",
    "save_checkpoint(\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    ensemble_models=ensemble_models,\n",
    "    stage2_model=stage2_model,\n",
    "    iteration=iteration,\n",
    "    temperature=temperature,\n",
    "    current_fold=current_fold,\n",
    "    best_score=best_ensemble_score,\n",
    "    acceptance_history=acceptance_history,\n",
    "    metadata={\n",
    "        'total_iterations': iteration + 1,\n",
    "        'final_ensemble_size': len(ensemble_models),\n",
    "        'acceptance_rate': sum(acceptance_history) / len(acceptance_history),\n",
    "        'best_score': best_ensemble_score\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save ensemble metadata as JSON\n",
    "import json\n",
    "\n",
    "metadata_path = MODELS_DIR / 'ensemble_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'ensemble_size': len(ensemble_models),\n",
    "        'total_iterations': iteration + 1,\n",
    "        'best_score': best_ensemble_score,\n",
    "        'acceptance_rate': sum(acceptance_history) / len(acceptance_history),\n",
    "        'training_completed': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'n_folds': N_FOLDS,\n",
    "        'random_state': RANDOM_STATE\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nFinal checkpoint saved: {CHECKPOINT_PATH}\")\n",
    "print(f\"Metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9ed4c",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fdfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"ENSEMBLE TRAINING SUMMARY\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"  Ensemble size: {len(ensemble_models)}\")\n",
    "print(f\"  Best validation ROC-AUC: {best_ensemble_score:.6f}\")\n",
    "print(f\"  Total iterations: {iteration + 1}\")\n",
    "print(f\"  Accepted models: {sum(acceptance_history)}\")\n",
    "print(f\"  Rejected models: {len(acceptance_history) - sum(acceptance_history)}\")\n",
    "print(f\"  Acceptance rate: {sum(acceptance_history) / len(acceptance_history):.1%}\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  Training log: {TRAINING_LOG_PATH}\")\n",
    "print(f\"  Stage 2 log: {STAGE2_LOG_PATH}\")\n",
    "print(f\"  Ensemble models: {ENSEMBLE_DIR}\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "print(f\"\\n{'=' * 80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
