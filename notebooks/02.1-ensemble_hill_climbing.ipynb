{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "23b636d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 02:54:38.952523: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765076078.974403  122710 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765076078.981230  122710 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Available CPUs: 24\n",
      "GPU disabled: CUDA drivers not available in dev container\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Disable GPU (CUDA drivers not available in dev container)\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # Suppress TensorFlow warnings\n",
    "\n",
    "# Set environment variable to limit thread usage per worker\n",
    "os.environ['OMP_NUM_THREADS'] = '2'\n",
    "\n",
    "# Third party imports\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Add models directory to path for custom transformers\n",
    "models_path = Path('../models').resolve()\n",
    "sys.path.insert(0, str(models_path))\n",
    "\n",
    "# Import custom transformers from models directory\n",
    "from logistic_regression_transformers import IDColumnDropper, IQRClipper, ConstantFeatureRemover\n",
    "\n",
    "# Import ensemble modules from functions package\n",
    "from functions import ensemble_database\n",
    "from functions.ensemble_hill_climbing import (\n",
    "    generate_random_pipeline,\n",
    "    calculate_ensemble_diversity,\n",
    "    quick_optimize_pipeline,\n",
    "    adaptive_simulated_annealing_acceptance,\n",
    "    update_temperature,\n",
    "    compute_pipeline_hash,\n",
    "    log_iteration\n",
    ")\n",
    "from functions.ensemble_stage2_model import (\n",
    "    build_stage2_dnn,\n",
    "    train_stage2_dnn,\n",
    "    save_checkpoint,\n",
    "    load_checkpoint,\n",
    "    evaluate_ensemble\n",
    ")\n",
    "\n",
    "# Configure TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Detect available CPUs\n",
    "n_cpus = cpu_count()\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Available CPUs: {n_cpus}\")\n",
    "print(f\"GPU disabled: CUDA drivers not available in dev container\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c63dc5d",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5d004dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Resetting database for fresh training run...\n",
      "Database initialized at: /workspaces/diabetes-prediction/data/ensemble_training.db\n",
      "WARNING: Database has been reset. All previous data has been permanently deleted.\n",
      "Database initialized at: /workspaces/diabetes-prediction/data/ensemble_training.db\n",
      "\n",
      "Configuration:\n",
      "  Random state: 315 (for fixed data splits only)\n",
      "  Parallel batch size: 10 candidates\n",
      "  Parallel workers: 10\n",
      "  Max iterations: 500\n",
      "  Plateau threshold: 100\n",
      "  Stage 2 batch size: 10 models\n",
      "  Resume from checkpoint: False\n",
      "  Ensemble directory: ../models/ensemble_stage1_models\n",
      "  Database: /workspaces/diabetes-prediction/data/ensemble_training.db\n"
     ]
    }
   ],
   "source": [
    "# Random state for reproducibility (only for fixed data splits)\n",
    "RANDOM_STATE = 315\n",
    "\n",
    "# Parallel training configuration\n",
    "BATCH_SIZE = 10  # Train this many candidates in parallel\n",
    "N_WORKERS = min(10, n_cpus)  # Use up to 10 workers (one per candidate)\n",
    "\n",
    "# Hill climbing configuration\n",
    "MAX_ITERATIONS = 500\n",
    "PLATEAU_ITERATIONS = 100  # Stop if no improvement for this many iterations\n",
    "BASE_TEMPERATURE = 0.01\n",
    "TEMPERATURE_DECAY = 0.995\n",
    "\n",
    "# Stage 2 DNN configuration\n",
    "STAGE2_BATCH_SIZE_MODELS = 10  # Retrain DNN every N accepted models\n",
    "STAGE2_EPOCHS = 100\n",
    "STAGE2_BATCH_SIZE = 128\n",
    "STAGE2_PATIENCE = 10\n",
    "\n",
    "# Checkpoint configuration\n",
    "RESUME_FROM_CHECKPOINT = False  # Set to True to resume from saved checkpoint\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "MODELS_DIR = Path('../models')\n",
    "ENSEMBLE_DIR = MODELS_DIR / 'ensemble_stage1_models'\n",
    "CHECKPOINT_PATH = MODELS_DIR / 'ensemble_checkpoint.pkl'\n",
    "\n",
    "# Create directories\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ENSEMBLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Reset and initialize SQLite database for logging\n",
    "print(\"Resetting database for fresh training run...\")\n",
    "ensemble_database.reset_database()\n",
    "ensemble_database.init_database()\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Random state: {RANDOM_STATE} (for fixed data splits only)\")\n",
    "print(f\"  Parallel batch size: {BATCH_SIZE} candidates\")\n",
    "print(f\"  Parallel workers: {N_WORKERS}\")\n",
    "print(f\"  Max iterations: {MAX_ITERATIONS}\")\n",
    "print(f\"  Plateau threshold: {PLATEAU_ITERATIONS}\")\n",
    "print(f\"  Stage 2 batch size: {STAGE2_BATCH_SIZE_MODELS} models\")\n",
    "print(f\"  Resume from checkpoint: {RESUME_FROM_CHECKPOINT}\")\n",
    "print(f\"  Ensemble directory: {ENSEMBLE_DIR}\")\n",
    "print(f\"  Database: {ensemble_database.DB_PATH}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d227c51",
   "metadata": {},
   "source": [
    "### Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3ddfa2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (700000, 26)\n",
      "Class distribution:\n",
      "diagnosed_diabetes\n",
      "1.0    0.623296\n",
      "0.0    0.376704\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_df_path = 'https://gperdrizet.github.io/FSA_devops/assets/data/unit3/diabetes_prediction_train.csv'\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(train_df['diagnosed_diabetes'].value_counts(normalize=True))\n",
    "\n",
    "# Define label and features\n",
    "label = 'diagnosed_diabetes'\n",
    "\n",
    "# Feature definitions (from logistic regression notebook)\n",
    "numerical_features = [\n",
    "    'age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week',\n",
    "    'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi',\n",
    "    'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate',\n",
    "    'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides',\n",
    "    'family_history_diabetes', 'hypertension_history', 'cardiovascular_history'\n",
    "]\n",
    "\n",
    "ordinal_features = ['education_level', 'income_level']\n",
    "education_categories = [['No formal', 'Highschool', 'Graduate', 'Postgraduate']]\n",
    "income_categories = [['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']]\n",
    "\n",
    "nominal_features = ['gender', 'ethnicity', 'smoking_status', 'employment_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81a2b790",
   "metadata": {},
   "source": [
    "### Create Base Preprocessor\n",
    "\n",
    "This preprocessor will be shared across all stage 1 models for consistent encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e2ae61db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base preprocessor created\n",
      "  Numerical features: 18\n",
      "  Ordinal features: 2\n",
      "  Nominal features: 4\n"
     ]
    }
   ],
   "source": [
    "# Create numerical pipeline\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('clipper', IQRClipper(iqr_multiplier=2.0)),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create encoders\n",
    "ordinal_encoder = OrdinalEncoder(\n",
    "    categories=education_categories + income_categories,\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "onehot_encoder = OneHotEncoder(\n",
    "    drop='first',\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore'\n",
    ")\n",
    "\n",
    "# Create base preprocessor\n",
    "base_preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('ord', ordinal_encoder, ordinal_features),\n",
    "        ('nom', onehot_encoder, nominal_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(\"Base preprocessor created\")\n",
    "print(f\"  Numerical features: {len(numerical_features)}\")\n",
    "print(f\"  Ordinal features: {len(ordinal_features)}\")\n",
    "print(f\"  Nominal features: {len(nominal_features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972055fd",
   "metadata": {},
   "source": [
    "## Initialize or Resume Ensemble\n",
    "\n",
    "### Option 1: Initialize Founder Model (if starting fresh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c9609e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "INITIALIZING FOUNDER MODEL\n",
      "================================================================================\n",
      "\n",
      "Fixed data split:\n",
      "--------------------------------------------------------------------------------\n",
      "  Training pool: 420,000 samples (60%)\n",
      "  Stage 1 validation: 140,000 samples (20%) - for stage 1 eval & stage 2 training\n",
      "  Stage 2 validation: 140,000 samples (20%) - for stage 2 eval (HELD OUT)\n",
      "\n",
      "Training founder model\n",
      "--------------------------------------------------------------------------------\n",
      "  Training samples: 39,283\n",
      "  Pipeline config:\n",
      "    Classifier: extra_trees\n",
      "    Transformers: ['ratio', 'log']\n",
      "    Use PCA: True\n",
      "  Training pipeline...\n",
      "  Training complete (1.1s)\n",
      "  Stage 1 validation AUC: 0.537828\n",
      "\n",
      "================================================================================\n",
      "FOUNDER MODEL COMPLETE\n",
      "================================================================================\n",
      "Stage 1 validation AUC: 0.537828\n"
     ]
    }
   ],
   "source": [
    "if not RESUME_FROM_CHECKPOINT:\n",
    "    print(\"=\" * 80)\n",
    "    print(\"INITIALIZING FOUNDER MODEL\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    ensemble_models = []\n",
    "    \n",
    "    # Prepare features and labels\n",
    "    X_full = train_df.drop(columns=[label])\n",
    "    y_full = train_df[label]\n",
    "    \n",
    "    # Create FIXED three-way split:\n",
    "    # 1. Training pool (60%) - for training stage 1 models (random samples from this)\n",
    "    # 2. Stage 1 validation (20%) - for evaluating stage 1 models and training stage 2\n",
    "    # 3. Stage 2 validation (20%) - for evaluating stage 2 model (held out)\n",
    "    \n",
    "    # First split: training pool vs validation\n",
    "    X_train_pool, X_val_combined, y_train_pool, y_val_combined = train_test_split(\n",
    "        X_full, \n",
    "        y_full, \n",
    "        test_size=0.4,  # 40% for validation (will split into 2x 20%)\n",
    "        random_state=RANDOM_STATE,  # Fixed split\n",
    "        stratify=y_full\n",
    "    )\n",
    "    \n",
    "    # Second split: stage 1 validation vs stage 2 validation\n",
    "    X_val_s1, X_val_s2, y_val_s1, y_val_s2 = train_test_split(\n",
    "        X_val_combined,\n",
    "        y_val_combined,\n",
    "        test_size=0.5,  # Split 40% into 2x 20%\n",
    "        random_state=RANDOM_STATE,  # Fixed split\n",
    "        stratify=y_val_combined\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nFixed data split:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Training pool: {len(X_train_pool):,} samples (60%)\")\n",
    "    print(f\"  Stage 1 validation: {len(X_val_s1):,} samples (20%) - for stage 1 eval & stage 2 training\")\n",
    "    print(f\"  Stage 2 validation: {len(X_val_s2):,} samples (20%) - for stage 2 eval (HELD OUT)\")\n",
    "    \n",
    "    # Random sample size for founder training\n",
    "    rng = np.random.RandomState(RANDOM_STATE)\n",
    "    founder_sample_size = rng.randint(10000, 50001)\n",
    "    \n",
    "    # Sample from training pool (no random_state = different sample each time)\n",
    "    X_train, _, y_train, _ = train_test_split(\n",
    "        X_train_pool,\n",
    "        y_train_pool,\n",
    "        train_size=founder_sample_size,\n",
    "        stratify=y_train_pool\n",
    "    )\n",
    "    \n",
    "    print(f\"\\nTraining founder model\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"  Training samples: {len(X_train):,}\")\n",
    "    \n",
    "    # Generate random pipeline for founder\n",
    "    pipeline, metadata = generate_random_pipeline(\n",
    "        iteration=0,\n",
    "        random_state=RANDOM_STATE,\n",
    "        base_preprocessor=base_preprocessor\n",
    "    )\n",
    "    \n",
    "    print(f\"  Pipeline config:\")\n",
    "    print(f\"    Classifier: {metadata['classifier_type']}\")\n",
    "    print(f\"    Transformers: {metadata['transformers_used']}\")\n",
    "    print(f\"    Use PCA: {metadata['use_pca']}\")\n",
    "    \n",
    "    # Train on training sample\n",
    "    print(f\"  Training pipeline...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Just fit the model - no CV, no hyperparameter optimization\n",
    "    fitted_pipeline = pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"  Training complete ({training_time:.1f}s)\")\n",
    "    \n",
    "    # Generate predictions on FIXED stage 1 validation set\n",
    "    if hasattr(fitted_pipeline, 'predict_proba'):\n",
    "        val_pred_s1 = fitted_pipeline.predict_proba(X_val_s1)[:, 1]\n",
    "    else:\n",
    "        val_pred_s1 = fitted_pipeline.decision_function(X_val_s1)\n",
    "    \n",
    "    # Calculate stage 1 validation AUC\n",
    "    val_auc_s1 = roc_auc_score(y_val_s1, val_pred_s1)\n",
    "    \n",
    "    print(f\"  Stage 1 validation AUC: {val_auc_s1:.6f}\")\n",
    "    \n",
    "    # Save founder model\n",
    "    ensemble_models.append(fitted_pipeline)\n",
    "    model_path = ENSEMBLE_DIR / 'founder_model.joblib'\n",
    "    joblib.dump(fitted_pipeline, model_path)\n",
    "    \n",
    "    # Log founder\n",
    "    pipeline_hash = compute_pipeline_hash(fitted_pipeline, metadata)\n",
    "    ensemble_id = \"founder\"\n",
    "    log_iteration(\n",
    "        iteration=0,\n",
    "        accepted=True,\n",
    "        rejection_reason='founder',\n",
    "        pipeline_hash=pipeline_hash,\n",
    "        stage1_val_auc=val_auc_s1,\n",
    "        stage2_val_auc=val_auc_s1,  # No stage 2 yet, use stage 1 score\n",
    "        ensemble_size=1,\n",
    "        diversity_score=0.0,\n",
    "        temperature=BASE_TEMPERATURE,\n",
    "        metadata=metadata,\n",
    "        ensemble_id=ensemble_id\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"FOUNDER MODEL COMPLETE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Stage 1 validation AUC: {val_auc_s1:.6f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a5d339f",
   "metadata": {},
   "source": [
    "### Initialize Ensemble (if starting fresh)\n",
    "\n",
    "Uses batch-based DNN training: simple mean for first 10 models, then DNN training every 10 accepted models with transfer learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bcfe2f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "INITIALIZING ENSEMBLE\n",
      "================================================================================\n",
      "\n",
      "Founder model performance:\n",
      "  Single model ROC-AUC: 0.539989\n",
      "  Stage 2 DNN will be trained after 10 accepted models\n"
     ]
    }
   ],
   "source": [
    "if not RESUME_FROM_CHECKPOINT:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(\"INITIALIZING ENSEMBLE\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    \n",
    "    # No stage 2 DNN yet - will train after first batch of models\n",
    "    stage2_model = None\n",
    "    \n",
    "    # Evaluate founder using simple prediction (no ensemble yet)\n",
    "    if hasattr(fitted_pipeline, 'predict_proba'):\n",
    "        val_pred_s2 = fitted_pipeline.predict_proba(X_val_s2)[:, 1]\n",
    "    else:\n",
    "        val_pred_s2 = fitted_pipeline.decision_function(X_val_s2)\n",
    "    \n",
    "    founder_auc = roc_auc_score(y_val_s2, val_pred_s2)\n",
    "    \n",
    "    print(f\"\\nFounder model performance:\")\n",
    "    print(f\"  Single model ROC-AUC: {founder_auc:.6f}\")\n",
    "    print(f\"  Stage 2 DNN will be trained after {STAGE2_BATCH_SIZE_MODELS} accepted models\")\n",
    "    \n",
    "    # Set initial best score\n",
    "    best_ensemble_score = founder_auc\n",
    "    \n",
    "    # Initialize hill climbing variables\n",
    "    start_iteration = 1  # Start from iteration 1 (founder is 0)\n",
    "    temperature = BASE_TEMPERATURE\n",
    "    acceptance_history = []\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4dbf52",
   "metadata": {},
   "source": [
    "## Hill Climbing Loop\n",
    "\n",
    "Iteratively add diverse models with simulated annealing acceptance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e19f8da",
   "metadata": {},
   "source": [
    "## Helper Functions for Parallel Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "278756a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Helper functions defined\n"
     ]
    }
   ],
   "source": [
    "def train_single_candidate(args):\n",
    "    \"\"\"Train a single candidate pipeline in a separate process.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    args : tuple\n",
    "        (iteration, X_train, y_train, X_val_s1, y_val_s1, base_preprocessor, random_state)\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dict : Dictionary containing:\n",
    "        - iteration: iteration number\n",
    "        - fitted_pipeline: trained pipeline\n",
    "        - metadata: pipeline configuration\n",
    "        - val_auc_s1: stage 1 validation AUC\n",
    "        - pipeline_hash: unique pipeline hash\n",
    "        - training_time: time to train (seconds)\n",
    "    \"\"\"\n",
    "    iteration, X_train, y_train, X_val_s1, y_val_s1, base_preprocessor, random_state = args\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Generate random pipeline\n",
    "    pipeline, metadata = generate_random_pipeline(\n",
    "        iteration=iteration,\n",
    "        random_state=random_state,\n",
    "        base_preprocessor=base_preprocessor\n",
    "    )\n",
    "    \n",
    "    # Train pipeline\n",
    "    fitted_pipeline = pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate on stage 1 validation\n",
    "    if hasattr(fitted_pipeline, 'predict_proba'):\n",
    "        val_pred_s1 = fitted_pipeline.predict_proba(X_val_s1)[:, 1]\n",
    "    else:\n",
    "        val_pred_s1 = fitted_pipeline.decision_function(X_val_s1)\n",
    "    \n",
    "    val_auc_s1 = roc_auc_score(y_val_s1, val_pred_s1)\n",
    "    \n",
    "    # Compute hash\n",
    "    pipeline_hash = compute_pipeline_hash(fitted_pipeline, metadata)\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    \n",
    "    return {\n",
    "        'iteration': iteration,\n",
    "        'fitted_pipeline': fitted_pipeline,\n",
    "        'metadata': metadata,\n",
    "        'val_auc_s1': val_auc_s1,\n",
    "        'pipeline_hash': pipeline_hash,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "\n",
    "print(\"Helper functions defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06eb40c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING PARALLEL HILL CLIMBING LOOP\n",
      "================================================================================\n",
      "Batch size: 10 candidates trained in parallel\n",
      "Workers: 10 parallel processes\n",
      "\n",
      "================================================================================\n",
      "BATCH Starting at iteration 2\n",
      "================================================================================\n",
      "Current ensemble size: 1\n",
      "Best score: 0.539989\n",
      "Temperature: 0.010000\n",
      "Iterations since improvement: 0/100\n",
      "\n",
      "Training 10 candidates in parallel...\n",
      "  âœ“ Iteration 3: gradient_boosting AUC=0.568942 (1.3s)\n",
      "  âœ“ Iteration 2: gradient_boosting AUC=0.641222 (1.5s)\n",
      "  âœ“ Iteration 6: gradient_boosting AUC=0.564536 (1.0s)\n",
      "  âœ“ Iteration 5: gradient_boosting AUC=0.609992 (1.2s)\n",
      "  âœ“ Iteration 4: gradient_boosting AUC=0.594499 (1.5s)\n",
      "  âœ“ Iteration 9: gradient_boosting AUC=0.634028 (1.0s)\n",
      "  âœ“ Iteration 7: gradient_boosting AUC=0.630017 (1.5s)\n",
      "  âœ“ Iteration 10: gradient_boosting AUC=0.626263 (1.1s)\n",
      "  âœ“ Iteration 8: gradient_boosting AUC=0.579121 (1.5s)\n",
      "  âœ“ Iteration 11: gradient_boosting AUC=0.599180 (1.1s)\n",
      "\n",
      "Batch training complete (3.3s, 0.3s per model)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 2\n",
      "  Classifier: gradient_boosting\n",
      "  Transformers: ratio\n",
      "  Stage 1 validation AUC: 0.641222\n",
      "  Evaluating ensemble with candidate...\n",
      "  Candidate ensemble AUC (simple mean): 0.644771\n",
      "  Diversity score: 0.148368\n",
      "  Decision: âœ“ ACCEPT (improvement: Î”=0.104781)\n",
      "  ðŸŽ‰ New best score: 0.644771 (Î”=0.104781)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 3\n",
      "  Classifier: gradient_boosting\n",
      "  Transformers: ratio\n",
      "  Stage 1 validation AUC: 0.568942\n",
      "  Evaluating ensemble with candidate...\n",
      "  Candidate ensemble AUC (simple mean): 0.640900\n",
      "  Diversity score: 0.262968\n",
      "  Decision: âœ“ ACCEPT (simulated_annealing: Î”=-0.003871, P=0.677719)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 4\n",
      "  Classifier: gradient_boosting\n",
      "  Transformers: ratio\n",
      "  Stage 1 validation AUC: 0.594499\n",
      "  Evaluating ensemble with candidate...\n",
      "  Candidate ensemble AUC (simple mean): 0.638110\n",
      "  Diversity score: 0.366640\n",
      "  Decision: âœ— REJECT (rejected: Î”=-0.006661, P=0.510262)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 5\n",
      "  Classifier: gradient_boosting\n",
      "  Transformers: ratio\n",
      "  Stage 1 validation AUC: 0.609992\n",
      "  Evaluating ensemble with candidate...\n",
      "  Candidate ensemble AUC (simple mean): 0.644773\n",
      "  Diversity score: 0.298526\n",
      "  Decision: âœ“ ACCEPT (improvement: Î”=0.000002)\n",
      "  ðŸŽ‰ New best score: 0.644773 (Î”=0.000002)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 6\n",
      "  Classifier: gradient_boosting\n",
      "  Transformers: ratio\n",
      "  Stage 1 validation AUC: 0.564536\n",
      "  Evaluating ensemble with candidate...\n",
      "  Candidate ensemble AUC (simple mean): 0.642755\n",
      "  Diversity score: 0.304055\n",
      "  Decision: âœ“ ACCEPT (simulated_annealing: Î”=-0.002018, P=0.813955)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 7\n",
      "  Classifier: gradient_boosting\n",
      "  Transformers: ratio\n",
      "  Stage 1 validation AUC: 0.630017\n",
      "  Evaluating ensemble with candidate...\n",
      "  Candidate ensemble AUC (simple mean): 0.645105\n",
      "  Diversity score: 0.371373\n",
      "  Decision: âœ“ ACCEPT (improvement: Î”=0.000332)\n",
      "  ðŸŽ‰ New best score: 0.645105 (Î”=0.000332)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 8\n",
      "  Classifier: gradient_boosting\n",
      "  Transformers: ratio\n",
      "  Stage 1 validation AUC: 0.579121\n",
      "  Evaluating ensemble with candidate...\n",
      "  Candidate ensemble AUC (simple mean): 0.643397\n",
      "  Diversity score: 0.387500\n",
      "  Decision: âœ“ ACCEPT (simulated_annealing: Î”=-0.001708, P=0.838619)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 9\n",
      "  Classifier: gradient_boosting\n",
      "  Transformers: ratio\n",
      "  Stage 1 validation AUC: 0.634028\n",
      "  Evaluating ensemble with candidate...\n",
      "  Candidate ensemble AUC (simple mean): 0.648501\n",
      "  Diversity score: 0.408515\n",
      "  Decision: âœ“ ACCEPT (improvement: Î”=0.003396)\n",
      "  ðŸŽ‰ New best score: 0.648501 (Î”=0.003396)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 10\n",
      "  Classifier: gradient_boosting\n",
      "  Transformers: ratio\n",
      "  Stage 1 validation AUC: 0.626263\n",
      "  Evaluating ensemble with candidate...\n",
      "  Candidate ensemble AUC (simple mean): 0.650993\n",
      "  Diversity score: 0.426551\n",
      "  Decision: âœ“ ACCEPT (improvement: Î”=0.002492)\n",
      "  ðŸŽ‰ New best score: 0.650993 (Î”=0.002492)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 11\n",
      "  Classifier: gradient_boosting\n",
      "  Transformers: ratio\n",
      "  Stage 1 validation AUC: 0.599180\n",
      "  Evaluating ensemble with candidate...\n",
      "  Candidate ensemble AUC (simple mean): 0.649614\n",
      "  Diversity score: 0.435761\n",
      "  Decision: âœ— REJECT (rejected: Î”=-0.001379, P=0.865658)\n",
      "\n",
      "================================================================================\n",
      "BATCH Starting at iteration 12\n",
      "================================================================================\n",
      "Current ensemble size: 9\n",
      "Best score: 0.650993\n",
      "Temperature: 0.009511\n",
      "Iterations since improvement: 1/100\n",
      "\n",
      "Training 10 candidates in parallel...\n",
      "  âœ“ Iteration 13: extra_trees AUC=0.542726 (2.2s)\n",
      "  âœ“ Iteration 15: extra_trees AUC=0.575535 (2.0s)\n",
      "  âœ“ Iteration 12: extra_trees AUC=0.587408 (2.6s)\n",
      "  âœ“ Iteration 17: extra_trees AUC=0.579562 (1.8s)\n",
      "  âœ“ Iteration 14: extra_trees AUC=0.589431 (2.5s)\n",
      "  âœ“ Iteration 19: extra_trees AUC=0.551688 (1.9s)\n",
      "  âœ“ Iteration 16: extra_trees AUC=0.619047 (2.8s)\n",
      "  âœ“ Iteration 21: extra_trees AUC=0.564413 (2.0s)\n",
      "  âœ“ Iteration 18: extra_trees AUC=0.565965 (2.8s)\n",
      "  âœ“ Iteration 20: extra_trees AUC=0.596123 (2.5s)\n",
      "\n",
      "Batch training complete (4.7s, 0.5s per model)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 12\n",
      "  Classifier: extra_trees\n",
      "  Transformers: binning, log, difference\n",
      "  Stage 1 validation AUC: 0.587408\n",
      "  Evaluating ensemble with candidate...\n",
      "  Candidate ensemble AUC (simple mean): 0.651176\n",
      "  Diversity score: 0.416973\n",
      "  Decision: âœ“ ACCEPT (improvement: Î”=0.000183)\n",
      "\n",
      "================================================================================\n",
      "BATCH COMPLETE: Training stage 2 DNN on 10 models\n",
      "================================================================================\n",
      "\n",
      "  Building initial stage 2 DNN...\n",
      "\n",
      "  Training stage 2 DNN...\n",
      "    Training samples: 40,000\n",
      "    Validation samples: 10,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 02:56:43.745957: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Stage 2 DNN trained!\n",
      "  DNN ensemble AUC: 0.653950\n",
      "================================================================================\n",
      "\n",
      "  ðŸŽ‰ New best score: 0.651176 (Î”=0.000183)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Processing Iteration 13\n",
      "  Classifier: extra_trees\n",
      "  Transformers: binning, log, difference\n",
      "  Stage 1 validation AUC: 0.542726\n",
      "  Evaluating ensemble with candidate...\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node sequential_1/dense_1/Relu defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/local/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/usr/local/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n\n  File \"/tmp/ipykernel_122710/3483921516.py\", line 106, in <module>\n\n  File \"/workspaces/diabetes-prediction/notebooks/functions/ensemble_stage2_model.py\", line 342, in evaluate_ensemble\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 588, in predict\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 282, in one_step_on_data_distributed\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 271, in one_step_on_data\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 110, in predict_step\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 941, in __call__\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 59, in __call__\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/ops/function.py\", line 206, in _run_through_graph\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/models/functional.py\", line 644, in call\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 941, in __call__\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 59, in __call__\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/layers/core/dense.py\", line 191, in call\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/activations/activations.py\", line 47, in relu\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/activations/activations.py\", line 101, in static_call\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py\", line 15, in relu\n\nMatrix size-incompatible: In[0]: [32,11], In[1]: [10,32]\n\t [[{{node sequential_1/dense_1/Relu}}]] [Op:__inference_one_step_on_data_distributed_156524]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mInvalidArgumentError\u001b[39m                      Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 106\u001b[39m\n\u001b[32m    103\u001b[39m     aggregation_method = \u001b[33m\"\u001b[39m\u001b[33msimple mean\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    104\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    105\u001b[39m     \u001b[38;5;66;03m# Use trained DNN for evaluation\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m106\u001b[39m     candidate_score = \u001b[43mevaluate_ensemble\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    107\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstage1_models\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcandidate_ensemble\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    108\u001b[39m \u001b[43m        \u001b[49m\u001b[43mstage2_model\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstage2_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    109\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_val_s2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my_val_s2\u001b[49m\n\u001b[32m    111\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    112\u001b[39m     aggregation_method = \u001b[33m\"\u001b[39m\u001b[33mDNN weighted\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    114\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  Candidate ensemble AUC (\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maggregation_method\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m): \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcandidate_score\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.6f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/workspaces/diabetes-prediction/notebooks/functions/ensemble_stage2_model.py:342\u001b[39m, in \u001b[36mevaluate_ensemble\u001b[39m\u001b[34m(stage1_models, stage2_model, X, y)\u001b[39m\n\u001b[32m    339\u001b[39m stage1_predictions = np.column_stack(stage1_predictions)\n\u001b[32m    341\u001b[39m \u001b[38;5;66;03m# Get stage 2 predictions\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m342\u001b[39m stage2_predictions = \u001b[43mstage2_model\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstage1_predictions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m.flatten()\n\u001b[32m    344\u001b[39m \u001b[38;5;66;03m# Calculate ROC-AUC\u001b[39;00m\n\u001b[32m    345\u001b[39m roc_auc = roc_auc_score(y, stage2_predictions)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mInvalidArgumentError\u001b[39m: Graph execution error:\n\nDetected at node sequential_1/dense_1/Relu defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 758, in start\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/local/lib/python3.12/asyncio/base_events.py\", line 645, in run_forever\n\n  File \"/usr/local/lib/python3.12/asyncio/base_events.py\", line 1999, in _run_once\n\n  File \"/usr/local/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 614, in shell_main\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 471, in dispatch_shell\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 366, in execute_request\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 827, in execute_request\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 458, in do_execute\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 663, in run_cell\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3123, in run_cell\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3178, in _run_cell\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_cell_async\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3641, in run_ast_nodes\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3701, in run_code\n\n  File \"/tmp/ipykernel_122710/3483921516.py\", line 106, in <module>\n\n  File \"/workspaces/diabetes-prediction/notebooks/functions/ensemble_stage2_model.py\", line 342, in evaluate_ensemble\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 588, in predict\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 282, in one_step_on_data_distributed\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 125, in wrapper\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 271, in one_step_on_data\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/trainer.py\", line 110, in predict_step\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 941, in __call__\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 59, in __call__\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/models/sequential.py\", line 220, in call\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/models/functional.py\", line 183, in call\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/ops/function.py\", line 206, in _run_through_graph\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/models/functional.py\", line 644, in call\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/layers/layer.py\", line 941, in __call__\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 117, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/ops/operation.py\", line 59, in __call__\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/utils/traceback_utils.py\", line 156, in error_handler\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/layers/core/dense.py\", line 191, in call\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/activations/activations.py\", line 47, in relu\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/activations/activations.py\", line 101, in static_call\n\n  File \"/home/vscode/.local/lib/python3.12/site-packages/keras/src/backend/tensorflow/nn.py\", line 15, in relu\n\nMatrix size-incompatible: In[0]: [32,11], In[1]: [10,32]\n\t [[{{node sequential_1/dense_1/Relu}}]] [Op:__inference_one_step_on_data_distributed_156524]"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"STARTING PARALLEL HILL CLIMBING LOOP\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} candidates trained in parallel\")\n",
    "print(f\"Workers: {N_WORKERS} parallel processes\")\n",
    "\n",
    "iterations_since_improvement = 0\n",
    "iteration = start_iteration\n",
    "\n",
    "while iteration < MAX_ITERATIONS and iterations_since_improvement < PLATEAU_ITERATIONS:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"BATCH Starting at iteration {iteration + 1}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Current ensemble size: {len(ensemble_models)}\")\n",
    "    print(f\"Best score: {best_ensemble_score:.6f}\")\n",
    "    print(f\"Temperature: {temperature:.6f}\")\n",
    "    print(f\"Iterations since improvement: {iterations_since_improvement}/{PLATEAU_ITERATIONS}\")\n",
    "    \n",
    "    # Prepare batch of training jobs\n",
    "    batch_jobs = []\n",
    "    for i in range(BATCH_SIZE):\n",
    "        current_iter = iteration + i\n",
    "        if current_iter >= MAX_ITERATIONS:\n",
    "            break\n",
    "        \n",
    "        # Random sample size for this iteration\n",
    "        rng = np.random.RandomState(RANDOM_STATE + current_iter)\n",
    "        iteration_sample_size = rng.randint(10000, 50001)\n",
    "        \n",
    "        # Sample from training pool (no random_state = different sample each time)\n",
    "        X_train, _, y_train, _ = train_test_split(\n",
    "            X_train_pool,\n",
    "            y_train_pool,\n",
    "            train_size=iteration_sample_size,\n",
    "            stratify=y_train_pool\n",
    "        )\n",
    "        \n",
    "        batch_jobs.append((\n",
    "            current_iter,\n",
    "            X_train,\n",
    "            y_train,\n",
    "            X_val_s1,\n",
    "            y_val_s1,\n",
    "            base_preprocessor,\n",
    "            RANDOM_STATE + current_iter\n",
    "        ))\n",
    "    \n",
    "    print(f\"\\nTraining {len(batch_jobs)} candidates in parallel...\")\n",
    "    batch_start_time = time.time()\n",
    "    \n",
    "    # Train candidates in parallel\n",
    "    trained_candidates = []\n",
    "    with ProcessPoolExecutor(max_workers=N_WORKERS) as executor:\n",
    "        futures = {executor.submit(train_single_candidate, job): job for job in batch_jobs}\n",
    "        \n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                result = future.result()\n",
    "                trained_candidates.append(result)\n",
    "                print(f\"  âœ“ Iteration {result['iteration'] + 1}: {result['metadata']['classifier_type']} \"\n",
    "                      f\"AUC={result['val_auc_s1']:.6f} ({result['training_time']:.1f}s)\")\n",
    "            except Exception as e:\n",
    "                job = futures[future]\n",
    "                print(f\"  âœ— Iteration {job[0] + 1} failed: {e}\")\n",
    "    \n",
    "    batch_time = time.time() - batch_start_time\n",
    "    print(f\"\\nBatch training complete ({batch_time:.1f}s, {batch_time/len(trained_candidates):.1f}s per model)\")\n",
    "    \n",
    "    # Sort by iteration number for consistent processing\n",
    "    trained_candidates.sort(key=lambda x: x['iteration'])\n",
    "    \n",
    "    # Process each trained candidate sequentially for acceptance/rejection\n",
    "    for result in trained_candidates:\n",
    "        current_iter = result['iteration']\n",
    "        fitted_pipeline = result['fitted_pipeline']\n",
    "        metadata = result['metadata']\n",
    "        val_auc_s1 = result['val_auc_s1']\n",
    "        pipeline_hash = result['pipeline_hash']\n",
    "        \n",
    "        print(f\"\\n{'-' * 80}\")\n",
    "        print(f\"Processing Iteration {current_iter + 1}\")\n",
    "        print(f\"  Classifier: {metadata['classifier_type']}\")\n",
    "        print(f\"  Transformers: {', '.join(metadata['transformers_used']) if metadata['transformers_used'] else 'None'}\")\n",
    "        print(f\"  Stage 1 validation AUC: {val_auc_s1:.6f}\")\n",
    "        \n",
    "        # Add to candidate pool and evaluate ensemble\n",
    "        print(f\"  Evaluating ensemble with candidate...\")\n",
    "        candidate_ensemble = ensemble_models + [fitted_pipeline]\n",
    "        \n",
    "        # Determine how many models have been trained with the current DNN\n",
    "        if stage2_model is None:\n",
    "            n_dnn_trained = 0\n",
    "        else:\n",
    "            n_dnn_trained = stage2_model.input_shape[1]\n",
    "        \n",
    "        # Calculate number of \"new\" models (accepted but not yet in DNN)\n",
    "        n_new_models = len(candidate_ensemble) - n_dnn_trained\n",
    "        \n",
    "        # Hybrid scoring: DNN for old models + simple mean for new models\n",
    "        if n_dnn_trained == 0:\n",
    "            # No DNN yet - use simple mean for all\n",
    "            all_preds = []\n",
    "            for model in candidate_ensemble:\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    pred = model.predict_proba(X_val_s2)[:, 1]\n",
    "                else:\n",
    "                    pred = model.decision_function(X_val_s2)\n",
    "                all_preds.append(pred)\n",
    "            \n",
    "            ensemble_pred = np.mean(all_preds, axis=0)\n",
    "            candidate_score = roc_auc_score(y_val_s2, ensemble_pred)\n",
    "            aggregation_method = \"simple mean (all)\"\n",
    "        \n",
    "        elif n_new_models == 0:\n",
    "            # All models are in the DNN - use DNN only\n",
    "            candidate_score = evaluate_ensemble(\n",
    "                stage1_models=candidate_ensemble,\n",
    "                stage2_model=stage2_model,\n",
    "                X=X_val_s2,\n",
    "                y=y_val_s2\n",
    "            )\n",
    "            aggregation_method = \"DNN (all)\"\n",
    "        \n",
    "        else:\n",
    "            # Hybrid: DNN for first n_dnn_trained models + simple mean for new models\n",
    "            # Get DNN predictions for old models\n",
    "            dnn_models = ensemble_models[:n_dnn_trained]\n",
    "            \n",
    "            # Generate stage 1 predictions for DNN models\n",
    "            dnn_stage1_preds = []\n",
    "            for model in dnn_models:\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    pred = model.predict_proba(X_val_s2)[:, 1]\n",
    "                else:\n",
    "                    pred = model.decision_function(X_val_s2)\n",
    "                dnn_stage1_preds.append(pred)\n",
    "            \n",
    "            dnn_stage1_preds = np.column_stack(dnn_stage1_preds)\n",
    "            \n",
    "            # Get DNN output (these are the weighted predictions)\n",
    "            dnn_output = stage2_model.predict(dnn_stage1_preds, verbose=0).flatten()\n",
    "            \n",
    "            # Get simple mean for new models (including candidate)\n",
    "            new_models = candidate_ensemble[n_dnn_trained:]\n",
    "            new_preds = []\n",
    "            for model in new_models:\n",
    "                if hasattr(model, 'predict_proba'):\n",
    "                    pred = model.predict_proba(X_val_s2)[:, 1]\n",
    "                else:\n",
    "                    pred = model.decision_function(X_val_s2)\n",
    "                new_preds.append(pred)\n",
    "            \n",
    "            new_mean_pred = np.mean(new_preds, axis=0)\n",
    "            \n",
    "            # Combine: weighted average\n",
    "            # DNN output represents the aggregated prediction for n_dnn_trained models\n",
    "            # New mean represents the average prediction for n_new_models models\n",
    "            combined_pred = (dnn_output * n_dnn_trained + new_mean_pred * n_new_models) / len(candidate_ensemble)\n",
    "            candidate_score = roc_auc_score(y_val_s2, combined_pred)\n",
    "            aggregation_method = f\"hybrid (DNNÃ—{n_dnn_trained} + meanÃ—{n_new_models})\"\n",
    "        \n",
    "        print(f\"  Candidate ensemble AUC ({aggregation_method}): {candidate_score:.6f}\")\n",
    "        \n",
    "        # Calculate diversity on stage 1 validation set\n",
    "        all_predictions = []\n",
    "        for model in candidate_ensemble:\n",
    "            if hasattr(model, 'predict_proba'):\n",
    "                pred = model.predict_proba(X_val_s1)[:, 1]\n",
    "            else:\n",
    "                pred = model.decision_function(X_val_s1)\n",
    "            all_predictions.append(pred)\n",
    "        \n",
    "        all_predictions = np.column_stack(all_predictions)\n",
    "        diversity_score = calculate_ensemble_diversity(all_predictions)\n",
    "        print(f\"  Diversity score: {diversity_score:.6f}\")\n",
    "        \n",
    "        # Simulated annealing acceptance\n",
    "        accept, reason = adaptive_simulated_annealing_acceptance(\n",
    "            current_score=best_ensemble_score,\n",
    "            candidate_score=candidate_score,\n",
    "            temperature=temperature,\n",
    "            random_state=RANDOM_STATE + current_iter\n",
    "        )\n",
    "        \n",
    "        print(f\"  Decision: {'âœ“ ACCEPT' if accept else 'âœ— REJECT'} ({reason})\")\n",
    "        \n",
    "        # Log iteration\n",
    "        ensemble_id = f\"iter_{current_iter}\"\n",
    "        log_iteration(\n",
    "            iteration=current_iter,\n",
    "            accepted=accept,\n",
    "            rejection_reason=reason,\n",
    "            pipeline_hash=pipeline_hash,\n",
    "            stage1_val_auc=val_auc_s1,\n",
    "            stage2_val_auc=candidate_score,\n",
    "            ensemble_size=len(candidate_ensemble) if accept else len(ensemble_models),\n",
    "            diversity_score=diversity_score,\n",
    "            temperature=temperature,\n",
    "            metadata=metadata,\n",
    "            ensemble_id=ensemble_id\n",
    "        )\n",
    "        \n",
    "        # Update ensemble if accepted\n",
    "        if accept:\n",
    "            ensemble_models.append(fitted_pipeline)\n",
    "            \n",
    "            # Save model\n",
    "            model_path = ENSEMBLE_DIR / f'model_{current_iter}.joblib'\n",
    "            joblib.dump(fitted_pipeline, model_path)\n",
    "            \n",
    "            # Check if we should train/retrain stage 2 DNN\n",
    "            if len(ensemble_models) % STAGE2_BATCH_SIZE_MODELS == 0:\n",
    "                print(f\"\\n{'=' * 80}\")\n",
    "                print(f\"BATCH COMPLETE: Training stage 2 DNN on {len(ensemble_models)} models\")\n",
    "                print(f\"{'=' * 80}\")\n",
    "                \n",
    "                # Get all predictions on stage 1 validation set\n",
    "                all_stage1_preds = []\n",
    "                for model in ensemble_models:\n",
    "                    if hasattr(model, 'predict_proba'):\n",
    "                        pred = model.predict_proba(X_val_s1)[:, 1]\n",
    "                    else:\n",
    "                        pred = model.decision_function(X_val_s1)\n",
    "                    all_stage1_preds.append(pred)\n",
    "                \n",
    "                X_stage2_train_full = np.column_stack(all_stage1_preds)\n",
    "                y_stage2_train_full = y_val_s1.values\n",
    "                \n",
    "                # Sample for training\n",
    "                sample_size = min(50000, len(X_stage2_train_full))\n",
    "                sample_indices = np.random.choice(len(X_stage2_train_full), size=sample_size, replace=False)\n",
    "                X_stage2_sample = X_stage2_train_full[sample_indices]\n",
    "                y_stage2_sample = y_stage2_train_full[sample_indices]\n",
    "                \n",
    "                # Train/val split\n",
    "                split_idx = int(len(X_stage2_sample) * 0.8)\n",
    "                X_train_s2 = X_stage2_sample[:split_idx]\n",
    "                y_train_s2 = y_stage2_sample[:split_idx]\n",
    "                X_val_s2_internal = X_stage2_sample[split_idx:]\n",
    "                y_val_s2_internal = y_stage2_sample[split_idx:]\n",
    "                \n",
    "                if stage2_model is None:\n",
    "                    # First DNN training\n",
    "                    print(f\"\\n  Building initial stage 2 DNN...\")\n",
    "                    stage2_model = build_stage2_dnn(\n",
    "                        n_models=len(ensemble_models),\n",
    "                        n_layers=1,\n",
    "                        units_per_layer=32,\n",
    "                        dropout=0.2,\n",
    "                        batch_norm=False,\n",
    "                        activation='relu',\n",
    "                        learning_rate=0.001\n",
    "                    )\n",
    "                else:\n",
    "                    # Transfer learning: build new DNN with more inputs, copy weights where possible\n",
    "                    print(f\"\\n  Transfer learning: expanding DNN from {stage2_model.input_shape[1]} to {len(ensemble_models)} inputs...\")\n",
    "                    \n",
    "                    # Save old weights\n",
    "                    old_weights = stage2_model.get_weights()\n",
    "                    \n",
    "                    # Build new model\n",
    "                    new_model = build_stage2_dnn(\n",
    "                        n_models=len(ensemble_models),\n",
    "                        n_layers=1,\n",
    "                        units_per_layer=32,\n",
    "                        dropout=0.2,\n",
    "                        batch_norm=False,\n",
    "                        activation='relu',\n",
    "                        learning_rate=0.001\n",
    "                    )\n",
    "                    \n",
    "                    # Transfer weights: copy input layer weights for existing models, initialize new ones randomly\n",
    "                    new_weights = new_model.get_weights()\n",
    "                    # Input layer weights: shape (n_inputs, 32), bias: shape (32,)\n",
    "                    # Copy existing input weights for old models, new model inputs already randomly initialized\n",
    "                    old_n_models = old_weights[0].shape[0]\n",
    "                    new_weights[0][:old_n_models, :] = old_weights[0]  # Copy old input weights\n",
    "                    new_weights[1] = old_weights[1]  # Copy input bias\n",
    "                    \n",
    "                    # For subsequent layers, only copy if architectures match exactly\n",
    "                    # Since we're using 1 hidden layer with 32 units consistently, we can copy\n",
    "                    # But we need to verify the shapes match before copying\n",
    "                    if len(old_weights) > 2 and len(new_weights) > 2:\n",
    "                        # Hidden layer weights and biases (layers after input)\n",
    "                        for i in range(2, len(old_weights)):\n",
    "                            if old_weights[i].shape == new_weights[i].shape:\n",
    "                                new_weights[i] = old_weights[i]\n",
    "                    \n",
    "                    new_model.set_weights(new_weights)\n",
    "                    stage2_model = new_model\n",
    "                \n",
    "                print(f\"\\n  Training stage 2 DNN...\")\n",
    "                print(f\"    Training samples: {len(X_train_s2):,}\")\n",
    "                print(f\"    Validation samples: {len(X_val_s2_internal):,}\")\n",
    "                \n",
    "                ensemble_id = f\"batch_{len(ensemble_models)}\"\n",
    "                stage2_model, history = train_stage2_dnn(\n",
    "                    model=stage2_model,\n",
    "                    X_train=X_train_s2,\n",
    "                    y_train=y_train_s2,\n",
    "                    X_val=X_val_s2_internal,\n",
    "                    y_val=y_val_s2_internal,\n",
    "                    epochs=STAGE2_EPOCHS,\n",
    "                    batch_size=STAGE2_BATCH_SIZE,\n",
    "                    patience=STAGE2_PATIENCE,\n",
    "                    log_path=ensemble_id,\n",
    "                    iteration=current_iter\n",
    "                )\n",
    "                \n",
    "                # Evaluate on held out stage 2 validation\n",
    "                final_score = evaluate_ensemble(\n",
    "                    stage1_models=ensemble_models,\n",
    "                    stage2_model=stage2_model,\n",
    "                    X=X_val_s2,\n",
    "                    y=y_val_s2\n",
    "                )\n",
    "                \n",
    "                print(f\"\\n  Stage 2 DNN trained!\")\n",
    "                print(f\"  DNN ensemble AUC: {final_score:.6f}\")\n",
    "                print(f\"{'=' * 80}\\n\")\n",
    "            \n",
    "            # Check if this is the best score\n",
    "            if candidate_score > best_ensemble_score:\n",
    "                print(f\"  ðŸŽ‰ New best score: {candidate_score:.6f} (Î”={candidate_score - best_ensemble_score:.6f})\")\n",
    "                best_ensemble_score = candidate_score\n",
    "                iterations_since_improvement = 0\n",
    "            else:\n",
    "                iterations_since_improvement += 1\n",
    "        else:\n",
    "            iterations_since_improvement += 1\n",
    "        \n",
    "        # Update temperature\n",
    "        temperature = update_temperature(\n",
    "            iteration=current_iter,\n",
    "            acceptance_history=[accept],  # Single acceptance for this iteration\n",
    "            current_temperature=temperature,\n",
    "            base_temperature=BASE_TEMPERATURE,\n",
    "            decay_rate=TEMPERATURE_DECAY\n",
    "        )\n",
    "    \n",
    "    # Move to next batch\n",
    "    iteration += len(trained_candidates)\n",
    "    \n",
    "    # Check termination conditions\n",
    "    if iterations_since_improvement >= PLATEAU_ITERATIONS:\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"TERMINATING: No improvement for {PLATEAU_ITERATIONS} iterations\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        break\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"HILL CLIMBING COMPLETE\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Final ensemble size: {len(ensemble_models)}\")\n",
    "print(f\"Best ensemble AUC: {best_ensemble_score:.6f}\")\n",
    "print(f\"Total iterations: {iteration}\")\n",
    "\n",
    "# Calculate acceptance rate from database\n",
    "conn = ensemble_database.sqlite3.connect(ensemble_database.DB_PATH)\n",
    "acceptance_stats = conn.execute(\"SELECT COUNT(*) as total, SUM(accepted) as accepted FROM ensemble_log\").fetchone()\n",
    "conn.close()\n",
    "acceptance_rate = acceptance_stats[1] / acceptance_stats[0] if acceptance_stats[0] > 0 else 0.0\n",
    "print(f\"Acceptance rate: {acceptance_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55640a1c",
   "metadata": {},
   "source": [
    "## Save Final Checkpoint and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78127513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint\n",
    "save_checkpoint(\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    ensemble_models=ensemble_models,\n",
    "    stage2_model=stage2_model,\n",
    "    iteration=iteration - 1,  # Last completed iteration\n",
    "    temperature=temperature,\n",
    "    best_score=best_ensemble_score,\n",
    "    acceptance_history=[],  # Not tracking per-iteration history in parallel mode\n",
    "    metadata={\n",
    "        'total_iterations': iteration,\n",
    "        'final_ensemble_size': len(ensemble_models),\n",
    "        'acceptance_rate': acceptance_rate,\n",
    "        'best_score': best_ensemble_score,\n",
    "        'parallel_batch_size': BATCH_SIZE,\n",
    "        'n_workers': N_WORKERS\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save ensemble metadata as JSON\n",
    "import json\n",
    "\n",
    "metadata_path = MODELS_DIR / 'ensemble_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'ensemble_size': len(ensemble_models),\n",
    "        'total_iterations': iteration,\n",
    "        'best_score': best_ensemble_score,\n",
    "        'acceptance_rate': acceptance_rate,\n",
    "        'training_completed': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'parallel_batch_size': BATCH_SIZE,\n",
    "        'n_workers': N_WORKERS\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nFinal checkpoint saved: {CHECKPOINT_PATH}\")\n",
    "print(f\"Metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef9ed4c",
   "metadata": {},
   "source": [
    "## Summary Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "856fdfb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"ENSEMBLE TRAINING SUMMARY\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"  Ensemble size: {len(ensemble_models)}\")\n",
    "print(f\"  Best validation AUC: {best_ensemble_score:.6f}\")\n",
    "print(f\"  Total iterations: {iteration}\")\n",
    "print(f\"  Acceptance rate: {acceptance_rate:.1%}\")\n",
    "print(f\"  Parallel configuration:\")\n",
    "print(f\"    Batch size: {BATCH_SIZE} candidates\")\n",
    "print(f\"    Workers: {N_WORKERS} processes\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  SQLite database: {ensemble_database.DB_PATH}\")\n",
    "print(f\"  Ensemble models: {ENSEMBLE_DIR}\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "print(f\"\\n{'=' * 80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
