{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45695246",
   "metadata": {},
   "source": [
    "# Logistic regression model\n",
    "\n",
    "One, two, you know what to do...\n",
    "\n",
    "## Notebook \n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5c3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "# Third party imports\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import uniform, loguniform\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder, PolynomialFeatures\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a530e36",
   "metadata": {},
   "source": [
    "### Run configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a991bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set to True when running on Kaggle, False when running locally\n",
    "KAGGLE = False\n",
    "\n",
    "# Set to True to run runtime experiment, False to load from disk\n",
    "MEASURE_RUNTIME = True\n",
    "\n",
    "# Optimization runtime\n",
    "RUNTIME_LIMIT = 60  # minutes\n",
    "\n",
    "# Random state for reproducibility\n",
    "RANDOM_STATE = 315"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf85c9",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed701ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>31</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol_consumption_per_week</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physical_activity_minutes_per_week</th>\n",
       "      <td>45</td>\n",
       "      <td>73</td>\n",
       "      <td>158</td>\n",
       "      <td>77</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diet_score</th>\n",
       "      <td>7.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleep_hours_per_day</th>\n",
       "      <td>6.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen_time_hours_per_day</th>\n",
       "      <td>6.1</td>\n",
       "      <td>5.8</td>\n",
       "      <td>9.1</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>33.4</td>\n",
       "      <td>23.8</td>\n",
       "      <td>24.1</td>\n",
       "      <td>26.6</td>\n",
       "      <td>28.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waist_to_hip_ratio</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>systolic_bp</th>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>95</td>\n",
       "      <td>121</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diastolic_bp</th>\n",
       "      <td>70</td>\n",
       "      <td>77</td>\n",
       "      <td>89</td>\n",
       "      <td>69</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_rate</th>\n",
       "      <td>62</td>\n",
       "      <td>71</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cholesterol_total</th>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>188</td>\n",
       "      <td>182</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hdl_cholesterol</th>\n",
       "      <td>58</td>\n",
       "      <td>50</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ldl_cholesterol</th>\n",
       "      <td>114</td>\n",
       "      <td>121</td>\n",
       "      <td>114</td>\n",
       "      <td>85</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triglycerides</th>\n",
       "      <td>102</td>\n",
       "      <td>124</td>\n",
       "      <td>108</td>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <td>Hispanic</td>\n",
       "      <td>White</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_level</th>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income_level</th>\n",
       "      <td>Lower-Middle</td>\n",
       "      <td>Upper-Middle</td>\n",
       "      <td>Lower-Middle</td>\n",
       "      <td>Lower-Middle</td>\n",
       "      <td>Upper-Middle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoking_status</th>\n",
       "      <td>Current</td>\n",
       "      <td>Never</td>\n",
       "      <td>Never</td>\n",
       "      <td>Current</td>\n",
       "      <td>Never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employment_status</th>\n",
       "      <td>Employed</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Retired</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_history_diabetes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypertension_history</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cardiovascular_history</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diagnosed_diabetes</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               0             1             2  \\\n",
       "id                                             0             1             2   \n",
       "age                                           31            50            32   \n",
       "alcohol_consumption_per_week                   1             2             3   \n",
       "physical_activity_minutes_per_week            45            73           158   \n",
       "diet_score                                   7.7           5.7           8.5   \n",
       "sleep_hours_per_day                          6.8           6.5           7.4   \n",
       "screen_time_hours_per_day                    6.1           5.8           9.1   \n",
       "bmi                                         33.4          23.8          24.1   \n",
       "waist_to_hip_ratio                          0.93          0.83          0.83   \n",
       "systolic_bp                                  112           120            95   \n",
       "diastolic_bp                                  70            77            89   \n",
       "heart_rate                                    62            71            73   \n",
       "cholesterol_total                            199           199           188   \n",
       "hdl_cholesterol                               58            50            59   \n",
       "ldl_cholesterol                              114           121           114   \n",
       "triglycerides                                102           124           108   \n",
       "gender                                    Female        Female          Male   \n",
       "ethnicity                               Hispanic         White      Hispanic   \n",
       "education_level                       Highschool    Highschool    Highschool   \n",
       "income_level                        Lower-Middle  Upper-Middle  Lower-Middle   \n",
       "smoking_status                           Current         Never         Never   \n",
       "employment_status                       Employed      Employed       Retired   \n",
       "family_history_diabetes                        0             0             0   \n",
       "hypertension_history                           0             0             0   \n",
       "cardiovascular_history                         0             0             0   \n",
       "diagnosed_diabetes                           1.0           1.0           0.0   \n",
       "\n",
       "                                               3             4  \n",
       "id                                             3             4  \n",
       "age                                           54            54  \n",
       "alcohol_consumption_per_week                   3             1  \n",
       "physical_activity_minutes_per_week            77            55  \n",
       "diet_score                                   4.6           5.7  \n",
       "sleep_hours_per_day                          7.0           6.2  \n",
       "screen_time_hours_per_day                    9.2           5.1  \n",
       "bmi                                         26.6          28.8  \n",
       "waist_to_hip_ratio                          0.83           0.9  \n",
       "systolic_bp                                  121           108  \n",
       "diastolic_bp                                  69            60  \n",
       "heart_rate                                    74            85  \n",
       "cholesterol_total                            182           206  \n",
       "hdl_cholesterol                               54            49  \n",
       "ldl_cholesterol                               85           131  \n",
       "triglycerides                                123           124  \n",
       "gender                                    Female          Male  \n",
       "ethnicity                                  White         White  \n",
       "education_level                       Highschool    Highschool  \n",
       "income_level                        Lower-Middle  Upper-Middle  \n",
       "smoking_status                           Current         Never  \n",
       "employment_status                       Employed       Retired  \n",
       "family_history_diabetes                        0             0  \n",
       "hypertension_history                           1             1  \n",
       "cardiovascular_history                         0             0  \n",
       "diagnosed_diabetes                           1.0           1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set file paths based on environment\n",
    "if KAGGLE:\n",
    "    # Kaggle paths - data is in /kaggle/input/\n",
    "    train_df_path = '/kaggle/input/playground-series-s5e12/train.csv'\n",
    "    test_df_path = '/kaggle/input/playground-series-s5e12/test.csv'\n",
    "\n",
    "else:\n",
    "    # Otherwise, load data from course GitHub repository\n",
    "    train_df_path = 'https://gperdrizet.github.io/FSA_devops/assets/data/unit3/diabetes_prediction_train.csv'\n",
    "    test_df_path = 'https://gperdrizet.github.io/FSA_devops/assets/data/unit3/diabetes_prediction_test.csv'\n",
    "\n",
    "# Load the training and testing datasets\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "test_df = pd.read_csv(test_df_path)\n",
    "\n",
    "# Display first few rows of training data\n",
    "train_df.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff30c8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 700000 entries, 0 to 699999\n",
      "Data columns (total 26 columns):\n",
      " #   Column                              Non-Null Count   Dtype  \n",
      "---  ------                              --------------   -----  \n",
      " 0   id                                  700000 non-null  int64  \n",
      " 1   age                                 700000 non-null  int64  \n",
      " 2   alcohol_consumption_per_week        700000 non-null  int64  \n",
      " 3   physical_activity_minutes_per_week  700000 non-null  int64  \n",
      " 4   diet_score                          700000 non-null  float64\n",
      " 5   sleep_hours_per_day                 700000 non-null  float64\n",
      " 6   screen_time_hours_per_day           700000 non-null  float64\n",
      " 7   bmi                                 700000 non-null  float64\n",
      " 8   waist_to_hip_ratio                  700000 non-null  float64\n",
      " 9   systolic_bp                         700000 non-null  int64  \n",
      " 10  diastolic_bp                        700000 non-null  int64  \n",
      " 11  heart_rate                          700000 non-null  int64  \n",
      " 12  cholesterol_total                   700000 non-null  int64  \n",
      " 13  hdl_cholesterol                     700000 non-null  int64  \n",
      " 14  ldl_cholesterol                     700000 non-null  int64  \n",
      " 15  triglycerides                       700000 non-null  int64  \n",
      " 16  gender                              700000 non-null  object \n",
      " 17  ethnicity                           700000 non-null  object \n",
      " 18  education_level                     700000 non-null  object \n",
      " 19  income_level                        700000 non-null  object \n",
      " 20  smoking_status                      700000 non-null  object \n",
      " 21  employment_status                   700000 non-null  object \n",
      " 22  family_history_diabetes             700000 non-null  int64  \n",
      " 23  hypertension_history                700000 non-null  int64  \n",
      " 24  cardiovascular_history              700000 non-null  int64  \n",
      " 25  diagnosed_diabetes                  700000 non-null  float64\n",
      "dtypes: float64(6), int64(14), object(6)\n",
      "memory usage: 138.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display dataset information (columns, dtypes, non-null counts)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3963201",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fe4e4337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label\n",
    "label = 'diagnosed_diabetes'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9bca249d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the ID column\n",
    "train_df.drop(columns=['id'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1f2d61e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using sample of 10000 rows\n"
     ]
    }
   ],
   "source": [
    "# Sample data if requested\n",
    "if SAMPLE is not None:\n",
    "    train_df = train_df.sample(n=SAMPLE, random_state=RANDOM_STATE)\n",
    "    print(f\"Using sample of {SAMPLE} rows\")\n",
    "\n",
    "else:\n",
    "    print(f\"Using full dataset: {len(train_df)} rows\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747278f9",
   "metadata": {},
   "source": [
    "### 1.1. Clean numerical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "49bca4cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define numerical features to apply IQR clipping\n",
    "numerical_features = ['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate', 'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides', 'family_history_diabetes', 'hypertension_history', 'cardiovascular_history']\n",
    "\n",
    "# Define IQR clipping transformer\n",
    "class IQRClipper(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Clips features to a multiple of the interquartile range (IQR).\"\"\"\n",
    "    \n",
    "    def __init__(self, iqr_multiplier=2.0):\n",
    "        self.iqr_multiplier = iqr_multiplier\n",
    "        self.lower_bounds_ = None\n",
    "        self.upper_bounds_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Calculate the clipping bounds based on IQR.\"\"\"\n",
    "\n",
    "        Q1 = np.percentile(X, 25, axis=0)\n",
    "        Q3 = np.percentile(X, 75, axis=0)\n",
    "        IQR = Q3 - Q1\n",
    "        self.lower_bounds_ = Q1 - self.iqr_multiplier * IQR\n",
    "        self.upper_bounds_ = Q3 + self.iqr_multiplier * IQR\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply clipping to the data.\"\"\"\n",
    "\n",
    "        return np.clip(X, self.lower_bounds_, self.upper_bounds_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10511a4f",
   "metadata": {},
   "source": [
    "### 1.2. Encode categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ea21243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define ordinal features to encode\n",
    "ordinal_features = ['education_level', 'income_level']\n",
    "\n",
    "# Define ordinal categories in order\n",
    "education_categories = [['No formal', 'Highschool', 'Graduate', 'Postgraduate']]\n",
    "income_categories = [['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']]\n",
    "\n",
    "# Create ordinal encoder with categories\n",
    "ordinal_encoder = OrdinalEncoder(\n",
    "    categories=education_categories + income_categories,\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f42aa3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define features for one-hot encoding\n",
    "nominal_features = ['gender', 'ethnicity', 'smoking_status', 'employment_status']\n",
    "\n",
    "# Create one-hot encoder (will learn categories from data)\n",
    "onehot_encoder = OneHotEncoder(\n",
    "    drop='first',\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfd166",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "\n",
    "### 2.1. Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b3b1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create polynomial features transformer\n",
    "poly_transformer = PolynomialFeatures()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a3c03",
   "metadata": {},
   "source": [
    "### 2.2. PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f474492",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create PCA transformer\n",
    "pca = PCA()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38adb359",
   "metadata": {},
   "source": [
    "## 3. Model optimization\n",
    "\n",
    "### 3.1. Build model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3e8deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline for numerical features\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('clipper', IQRClipper()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create column transformer to apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('ord', ordinal_encoder, ordinal_features),\n",
    "        ('nom', onehot_encoder, nominal_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create full feature engineering & estimator pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', poly_transformer),\n",
    "    ('pca', pca),\n",
    "    ('logit', LogisticRegression())\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0134cf",
   "metadata": {},
   "source": [
    "### 3.2. Optimize model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74c9fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions for optimization\n",
    "param_distributions = [\n",
    "    {\n",
    "        'preprocessor__num__clipper__iqr_multiplier': uniform(loc=1.25, scale=1.25),  # Uniform between 1.25 and 2.5\n",
    "        'poly__degree': [1, 2, 3, 4],\n",
    "        'poly__include_bias': [True, False],\n",
    "        'poly__interaction_only': [False, True],\n",
    "        'pca__n_components': uniform(loc=0.25, scale=0.75),  # Uniform between 0.25 and 1.0\n",
    "        'logit__C': loguniform(0.001, 100),  # Log-uniform between 0.001 and 100\n",
    "        'logit__penalty': ['l2'],\n",
    "        'logit__max_iter': [1000],\n",
    "        'logit__class_weight': ['balanced'],\n",
    "    },\n",
    "    {\n",
    "        'preprocessor__num__clipper__iqr_multiplier': uniform(loc=1.25, scale=1.25),  # Uniform between 1.25 and 2.5\n",
    "        'poly__degree': [1, 2, 3, 4],\n",
    "        'poly__include_bias': [True, False],\n",
    "        'poly__interaction_only': [False, True],\n",
    "        'pca__n_components': uniform(loc=0.25, scale=0.75),  # Uniform between 0.25 and 1.0\n",
    "        'logit__penalty': [None],\n",
    "        'logit__max_iter': [1000],\n",
    "        'logit__class_weight': ['balanced'],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c9b08",
   "metadata": {},
   "source": [
    "#### 3.2.1. Runtime experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbf35d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running runtime experiments...\n",
      "============================================================\n",
      "Sample size:  3000 | Iterations:   5 | Time:  87.83s\n",
      "Sample size:  3000 | Iterations:   5 | Time:  87.83s\n",
      "Sample size:  3000 | Iterations:  10 | Time: 201.43s\n",
      "Sample size:  3000 | Iterations:  10 | Time: 201.43s\n",
      "Sample size:  3000 | Iterations:  20 | Time: 386.04s\n"
     ]
    }
   ],
   "source": [
    "# Define experimental parameters\n",
    "sample_sizes = [3000, 6000, 12000]\n",
    "n_iters = [5, 10, 20]\n",
    "\n",
    "# Define path for saving/loading results\n",
    "data_dir = Path('../data')\n",
    "runtime_results_path = data_dir / 'runtime_experiment_results.csv'\n",
    "\n",
    "if MEASURE_RUNTIME:\n",
    "    # Store results\n",
    "    results = []\n",
    "\n",
    "    # Get full dataset size for later prediction\n",
    "    full_dataset_size = len(pd.read_csv(train_df_path))\n",
    "\n",
    "    print(\"Running runtime experiments...\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Run experiments\n",
    "    for sample_size in sample_sizes:\n",
    "        for n_iter in n_iters:\n",
    "\n",
    "            # Sample the data\n",
    "            train_sample = pd.read_csv(train_df_path).drop(columns=['id']).sample(\n",
    "                n=sample_size, \n",
    "                random_state=RANDOM_STATE\n",
    "            )\n",
    "\n",
    "            X_sample = train_sample.drop(columns=[label])\n",
    "            y_sample = train_sample[label]\n",
    "            \n",
    "            # Create a simple search\n",
    "            simple_search = RandomizedSearchCV(\n",
    "                estimator=pipeline,\n",
    "                param_distributions=param_distributions,\n",
    "                n_iter=n_iter,\n",
    "                scoring='roc_auc',\n",
    "                n_jobs=-1,\n",
    "                cv=3,\n",
    "                random_state=RANDOM_STATE,\n",
    "                verbose=0\n",
    "            )\n",
    "            \n",
    "            # Time the fitting\n",
    "            start_time = time.time()\n",
    "            simple_search.fit(X_sample, y_sample)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            \n",
    "            results.append({\n",
    "                'sample_size': sample_size,\n",
    "                'n_iter': n_iter,\n",
    "                'runtime_seconds': elapsed_time\n",
    "            })\n",
    "            \n",
    "            print(f\"Sample size: {sample_size:5d} | Iterations: {n_iter:3d} | Time: {elapsed_time:6.2f}s\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"Experiment complete!\")\n",
    "\n",
    "    # Convert results to DataFrame\n",
    "    results_df = pd.DataFrame(results)\n",
    "    \n",
    "    # Save results to disk\n",
    "    data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    results_df.to_csv(runtime_results_path, index=False)\n",
    "    print(f\"\\nResults saved to: {runtime_results_path}\")\n",
    "    \n",
    "else:\n",
    "    # Load results from disk\n",
    "    print(\"Loading runtime experiment results from disk...\")\n",
    "    results_df = pd.read_csv(runtime_results_path)\n",
    "    print(f\"Loaded {len(results_df)} results from: {runtime_results_path}\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "# Display results\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f21650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build linear regression model to predict runtime\n",
    "# Features: sample_size and n_iter\n",
    "# Target: runtime_seconds\n",
    "\n",
    "X_train_runtime = results_df[['sample_size', 'n_iter']].values\n",
    "y_train_runtime = results_df['runtime_seconds'].values\n",
    "\n",
    "# Fit linear model\n",
    "runtime_model = LinearRegression()\n",
    "runtime_model.fit(X_train_runtime, y_train_runtime)\n",
    "\n",
    "# Display model coefficients\n",
    "print(\"Runtime Model Coefficients:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Intercept: {runtime_model.intercept_:.4f} seconds\")\n",
    "print(f\"Sample size coefficient: {runtime_model.coef_[0]:.6f} seconds per sample\")\n",
    "print(f\"Iteration coefficient: {runtime_model.coef_[1]:.4f} seconds per iteration\")\n",
    "print()\n",
    "\n",
    "# Calculate R² score\n",
    "r2_score = runtime_model.score(X_train_runtime, y_train_runtime)\n",
    "print(f\"R² score: {r2_score:.4f}\")\n",
    "print()\n",
    "\n",
    "# Show predictions vs actual\n",
    "results_df['predicted_runtime'] = runtime_model.predict(X_train_runtime)\n",
    "results_df['residual'] = results_df['runtime_seconds'] - results_df['predicted_runtime']\n",
    "\n",
    "print(\"\\nActual vs Predicted Runtimes:\")\n",
    "print(results_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7300d71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict runtime for full dataset with varying iteration counts\n",
    "iteration_range = np.arange(10, 501, 10)\n",
    "full_size_predictions = []\n",
    "\n",
    "for n_iter in iteration_range:\n",
    "    predicted_time = runtime_model.predict([[full_dataset_size, n_iter]])[0]\n",
    "    full_size_predictions.append(predicted_time)\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot predicted runtime vs iterations for full dataset\n",
    "ax.plot(iteration_range, full_size_predictions, 'b-', linewidth=2, label='Predicted Runtime')\n",
    "\n",
    "# Add horizontal line for 1 hour and 2 hours\n",
    "ax.axhline(y=3600, color='r', linestyle='--', linewidth=1.5, label='1 hour', alpha=0.7)\n",
    "ax.axhline(y=7200, color='orange', linestyle='--', linewidth=1.5, label='2 hours', alpha=0.7)\n",
    "\n",
    "# Formatting\n",
    "ax.set_xlabel('Number of Iterations', fontsize=12)\n",
    "ax.set_ylabel('Predicted Runtime (seconds)', fontsize=12)\n",
    "ax.set_title(f'Predicted Runtime vs Iterations\\n(Full Dataset: {full_dataset_size:,} samples)', fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "ax.legend(fontsize=10)\n",
    "\n",
    "# Add secondary y-axis for minutes\n",
    "ax2 = ax.secondary_yaxis('right', functions=(lambda x: x/60, lambda x: x*60))\n",
    "ax2.set_ylabel('Runtime (minutes)', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some specific predictions\n",
    "print(\"\\nPredicted runtimes for full dataset:\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "for n_iter in [50, 100, 200, 300, 500]:\n",
    "    pred_time = runtime_model.predict([[full_dataset_size, n_iter]])[0]\n",
    "    print(f\"{n_iter:3d} iterations: {pred_time:7.1f} seconds ({pred_time/60:5.1f} minutes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac1751e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate optimal number of iterations for full dataset based on RUNTIME_LIMIT\n",
    "# Runtime model: runtime = intercept + coef[0] * sample_size + coef[1] * n_iter\n",
    "# Solving for n_iter: n_iter = (target_runtime - intercept - coef[0] * sample_size) / coef[1]\n",
    "\n",
    "target_runtime_seconds = RUNTIME_LIMIT * 60  # Convert minutes to seconds\n",
    "full_dataset_size = len(pd.read_csv(train_df_path))\n",
    "\n",
    "# Calculate optimal iterations\n",
    "optimal_n_iter = (target_runtime_seconds - runtime_model.intercept_ - \n",
    "                  runtime_model.coef_[0] * full_dataset_size) / runtime_model.coef_[1]\n",
    "\n",
    "# Round down to be conservative and ensure we stay within the time limit\n",
    "optimal_n_iter = int(np.floor(optimal_n_iter))\n",
    "\n",
    "# Predict actual runtime for this number of iterations\n",
    "predicted_runtime = runtime_model.predict([[full_dataset_size, optimal_n_iter]])[0]\n",
    "\n",
    "print(\"Optimization Parameter Calculation:\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Runtime limit: {RUNTIME_LIMIT} minutes ({target_runtime_seconds:.0f} seconds)\")\n",
    "print(f\"Full dataset size: {full_dataset_size:,} samples\")\n",
    "print(f\"\\nOptimal number of iterations: {optimal_n_iter}\")\n",
    "print(f\"Predicted runtime: {predicted_runtime:.1f} seconds ({predicted_runtime/60:.1f} minutes)\")\n",
    "print(f\"Time margin: {(target_runtime_seconds - predicted_runtime)/60:.1f} minutes\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00578e0a",
   "metadata": {},
   "source": [
    "#### 3.2.2. Optimize model pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5bc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RandomizedSearchCV with optimal number of iterations\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=optimal_n_iter,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    cv=3,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Fit the randomized search\n",
    "print(f\"Starting randomized search optimization with {optimal_n_iter} iterations...\")\n",
    "print(f\"Expected runtime: ~{predicted_runtime/60:.1f} minutes\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "start_optimization = time.time()\n",
    "random_search.fit(train_df.drop(columns=[label]), train_df[label])\n",
    "optimization_time = time.time() - start_optimization\n",
    "\n",
    "# Display best parameters and score\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Randomized Search Results\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Actual runtime: {optimization_time:.1f} seconds ({optimization_time/60:.1f} minutes)\")\n",
    "print(f\"Best cross-validation ROC-AUC score: {random_search.best_score_:.4f}\")\n",
    "print(f\"\\nBest parameters:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99adb7e2",
   "metadata": {},
   "source": [
    "## 4. Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c03f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload full training dataset\n",
    "train_df_full = pd.read_csv(train_df_path)\n",
    "train_df_full.drop(columns=['id'], inplace=True)\n",
    "\n",
    "# Separate features and target\n",
    "X_train_full = train_df_full.drop(columns=[label])\n",
    "y_train_full = train_df_full[label]\n",
    "\n",
    "# Get the best model from randomized search (already trained on best params)\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Retrain on full dataset with best parameters\n",
    "print(\"Retraining model on full dataset with best parameters...\")\n",
    "best_model.fit(X_train_full, y_train_full)\n",
    "\n",
    "print(f\"Model retrained on {len(X_train_full)} samples\")\n",
    "print(f\"\\nBest parameters used:\")\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f\"  {param}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cc18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test data\n",
    "test_ids = test_df['id'].copy()\n",
    "X_test = test_df.drop(columns=['id'])\n",
    "\n",
    "# Make predictions\n",
    "print(\"Making predictions on test set...\")\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Create submission dataframe\n",
    "submission_df = pd.DataFrame({\n",
    "    'id': test_ids,\n",
    "    'diagnosed_diabetes': y_pred\n",
    "})\n",
    "\n",
    "print(f\"\\nPredictions completed for {len(submission_df)} test samples\")\n",
    "print(f\"\\nSubmission dataframe shape: {submission_df.shape}\")\n",
    "print(f\"\\nFirst few predictions:\")\n",
    "print(submission_df.head())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
