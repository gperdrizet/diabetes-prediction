{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45695246",
   "metadata": {},
   "source": [
    "# Logistic regression model\n",
    "\n",
    "One, two, you know what to do...\n",
    "\n",
    "**Note**: For Kaggle compatibility, use scikit-learn 1.2.2 when training this model. Kaggle's environment uses sklearn 1.2.2, and models saved with newer versions may not deserialize properly.\n",
    "\n",
    "## Notebook set up\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb5c3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import tracemalloc\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "# Set environment variable to limit thread usage by underlying linear algebra libraries.\n",
    "# Note: this must be set before importing numpy, pandas, or sklearn.\n",
    "os.environ['OMP_NUM_THREADS'] = '1'\n",
    "\n",
    "# Third party imports\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import uniform, loguniform, randint\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, OrdinalEncoder, OneHotEncoder, PolynomialFeatures\n",
    "\n",
    "# Add models directory to path for custom transformers\n",
    "models_path = Path('../models').resolve()\n",
    "sys.path.insert(0, str(models_path))\n",
    "\n",
    "# Import custom transformers from models directory\n",
    "from logistic_regression_transformers import (\n",
    "    IDColumnDropper,\n",
    "    IQRClipper,\n",
    "    ConstantFeatureRemover\n",
    ")\n",
    "\n",
    "# Import utility functions from notebooks/functions\n",
    "from functions.logistic_regression import (\n",
    "    random_search_test,\n",
    "    build_runtime_model,\n",
    "    build_memory_model,\n",
    "    calculate_optimal_iterations,\n",
    "    calculate_optimal_sample_size,\n",
    "    plot_runtime_model,\n",
    "    plot_memory_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a530e36",
   "metadata": {},
   "source": [
    "### Run configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8a991bf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lucky random state for reproducibility\n",
    "RANDOM_STATE = 315\n",
    "\n",
    "# Set to true to run individual preprocessing/feature engineering steps\n",
    "CHECK_PIPELINE_STEPS = True\n",
    "\n",
    "# Number of cross-validation folds\n",
    "CV_FOLDS = 3\n",
    "\n",
    "# Number of threads to use for model optimization\n",
    "N_JOBS = 10\n",
    "\n",
    "# Set to True to run random search resource test\n",
    "RANDOM_SEARCH_TEST = False\n",
    "\n",
    "# Target runtime limit for optimization\n",
    "RUNTIME_LIMIT_MIN = 3 * 8 * 60\n",
    "\n",
    "# Memory target\n",
    "MEMORY_LIMIT_GB = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf85c9",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ed701ba3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>age</th>\n",
       "      <td>31</td>\n",
       "      <td>50</td>\n",
       "      <td>32</td>\n",
       "      <td>54</td>\n",
       "      <td>54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alcohol_consumption_per_week</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>physical_activity_minutes_per_week</th>\n",
       "      <td>45</td>\n",
       "      <td>73</td>\n",
       "      <td>158</td>\n",
       "      <td>77</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diet_score</th>\n",
       "      <td>7.7</td>\n",
       "      <td>5.7</td>\n",
       "      <td>8.5</td>\n",
       "      <td>4.6</td>\n",
       "      <td>5.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sleep_hours_per_day</th>\n",
       "      <td>6.8</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7.4</td>\n",
       "      <td>7.0</td>\n",
       "      <td>6.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>screen_time_hours_per_day</th>\n",
       "      <td>6.1</td>\n",
       "      <td>5.8</td>\n",
       "      <td>9.1</td>\n",
       "      <td>9.2</td>\n",
       "      <td>5.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmi</th>\n",
       "      <td>33.4</td>\n",
       "      <td>23.8</td>\n",
       "      <td>24.1</td>\n",
       "      <td>26.6</td>\n",
       "      <td>28.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>waist_to_hip_ratio</th>\n",
       "      <td>0.93</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.83</td>\n",
       "      <td>0.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>systolic_bp</th>\n",
       "      <td>112</td>\n",
       "      <td>120</td>\n",
       "      <td>95</td>\n",
       "      <td>121</td>\n",
       "      <td>108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diastolic_bp</th>\n",
       "      <td>70</td>\n",
       "      <td>77</td>\n",
       "      <td>89</td>\n",
       "      <td>69</td>\n",
       "      <td>60</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>heart_rate</th>\n",
       "      <td>62</td>\n",
       "      <td>71</td>\n",
       "      <td>73</td>\n",
       "      <td>74</td>\n",
       "      <td>85</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cholesterol_total</th>\n",
       "      <td>199</td>\n",
       "      <td>199</td>\n",
       "      <td>188</td>\n",
       "      <td>182</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hdl_cholesterol</th>\n",
       "      <td>58</td>\n",
       "      <td>50</td>\n",
       "      <td>59</td>\n",
       "      <td>54</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ldl_cholesterol</th>\n",
       "      <td>114</td>\n",
       "      <td>121</td>\n",
       "      <td>114</td>\n",
       "      <td>85</td>\n",
       "      <td>131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>triglycerides</th>\n",
       "      <td>102</td>\n",
       "      <td>124</td>\n",
       "      <td>108</td>\n",
       "      <td>123</td>\n",
       "      <td>124</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gender</th>\n",
       "      <td>Female</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "      <td>Female</td>\n",
       "      <td>Male</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ethnicity</th>\n",
       "      <td>Hispanic</td>\n",
       "      <td>White</td>\n",
       "      <td>Hispanic</td>\n",
       "      <td>White</td>\n",
       "      <td>White</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>education_level</th>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "      <td>Highschool</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>income_level</th>\n",
       "      <td>Lower-Middle</td>\n",
       "      <td>Upper-Middle</td>\n",
       "      <td>Lower-Middle</td>\n",
       "      <td>Lower-Middle</td>\n",
       "      <td>Upper-Middle</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>smoking_status</th>\n",
       "      <td>Current</td>\n",
       "      <td>Never</td>\n",
       "      <td>Never</td>\n",
       "      <td>Current</td>\n",
       "      <td>Never</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>employment_status</th>\n",
       "      <td>Employed</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Retired</td>\n",
       "      <td>Employed</td>\n",
       "      <td>Retired</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>family_history_diabetes</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>hypertension_history</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cardiovascular_history</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>diagnosed_diabetes</th>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               0             1             2  \\\n",
       "id                                             0             1             2   \n",
       "age                                           31            50            32   \n",
       "alcohol_consumption_per_week                   1             2             3   \n",
       "physical_activity_minutes_per_week            45            73           158   \n",
       "diet_score                                   7.7           5.7           8.5   \n",
       "sleep_hours_per_day                          6.8           6.5           7.4   \n",
       "screen_time_hours_per_day                    6.1           5.8           9.1   \n",
       "bmi                                         33.4          23.8          24.1   \n",
       "waist_to_hip_ratio                          0.93          0.83          0.83   \n",
       "systolic_bp                                  112           120            95   \n",
       "diastolic_bp                                  70            77            89   \n",
       "heart_rate                                    62            71            73   \n",
       "cholesterol_total                            199           199           188   \n",
       "hdl_cholesterol                               58            50            59   \n",
       "ldl_cholesterol                              114           121           114   \n",
       "triglycerides                                102           124           108   \n",
       "gender                                    Female        Female          Male   \n",
       "ethnicity                               Hispanic         White      Hispanic   \n",
       "education_level                       Highschool    Highschool    Highschool   \n",
       "income_level                        Lower-Middle  Upper-Middle  Lower-Middle   \n",
       "smoking_status                           Current         Never         Never   \n",
       "employment_status                       Employed      Employed       Retired   \n",
       "family_history_diabetes                        0             0             0   \n",
       "hypertension_history                           0             0             0   \n",
       "cardiovascular_history                         0             0             0   \n",
       "diagnosed_diabetes                           1.0           1.0           0.0   \n",
       "\n",
       "                                               3             4  \n",
       "id                                             3             4  \n",
       "age                                           54            54  \n",
       "alcohol_consumption_per_week                   3             1  \n",
       "physical_activity_minutes_per_week            77            55  \n",
       "diet_score                                   4.6           5.7  \n",
       "sleep_hours_per_day                          7.0           6.2  \n",
       "screen_time_hours_per_day                    9.2           5.1  \n",
       "bmi                                         26.6          28.8  \n",
       "waist_to_hip_ratio                          0.83           0.9  \n",
       "systolic_bp                                  121           108  \n",
       "diastolic_bp                                  69            60  \n",
       "heart_rate                                    74            85  \n",
       "cholesterol_total                            182           206  \n",
       "hdl_cholesterol                               54            49  \n",
       "ldl_cholesterol                               85           131  \n",
       "triglycerides                                123           124  \n",
       "gender                                    Female          Male  \n",
       "ethnicity                                  White         White  \n",
       "education_level                       Highschool    Highschool  \n",
       "income_level                        Lower-Middle  Upper-Middle  \n",
       "smoking_status                           Current         Never  \n",
       "employment_status                       Employed       Retired  \n",
       "family_history_diabetes                        0             0  \n",
       "hypertension_history                           1             1  \n",
       "cardiovascular_history                         0             0  \n",
       "diagnosed_diabetes                           1.0           1.0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df_path = 'https://gperdrizet.github.io/FSA_devops/assets/data/unit3/diabetes_prediction_train.csv'\n",
    "\n",
    "# Load the training dataset\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "\n",
    "# Display first few rows of training data\n",
    "train_df.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ff30c8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 700000 entries, 0 to 699999\n",
      "Data columns (total 26 columns):\n",
      " #   Column                              Non-Null Count   Dtype  \n",
      "---  ------                              --------------   -----  \n",
      " 0   id                                  700000 non-null  int64  \n",
      " 1   age                                 700000 non-null  int64  \n",
      " 2   alcohol_consumption_per_week        700000 non-null  int64  \n",
      " 3   physical_activity_minutes_per_week  700000 non-null  int64  \n",
      " 4   diet_score                          700000 non-null  float64\n",
      " 5   sleep_hours_per_day                 700000 non-null  float64\n",
      " 6   screen_time_hours_per_day           700000 non-null  float64\n",
      " 7   bmi                                 700000 non-null  float64\n",
      " 8   waist_to_hip_ratio                  700000 non-null  float64\n",
      " 9   systolic_bp                         700000 non-null  int64  \n",
      " 10  diastolic_bp                        700000 non-null  int64  \n",
      " 11  heart_rate                          700000 non-null  int64  \n",
      " 12  cholesterol_total                   700000 non-null  int64  \n",
      " 13  hdl_cholesterol                     700000 non-null  int64  \n",
      " 14  ldl_cholesterol                     700000 non-null  int64  \n",
      " 15  triglycerides                       700000 non-null  int64  \n",
      " 16  gender                              700000 non-null  object \n",
      " 17  ethnicity                           700000 non-null  object \n",
      " 18  education_level                     700000 non-null  object \n",
      " 19  income_level                        700000 non-null  object \n",
      " 20  smoking_status                      700000 non-null  object \n",
      " 21  employment_status                   700000 non-null  object \n",
      " 22  family_history_diabetes             700000 non-null  int64  \n",
      " 23  hypertension_history                700000 non-null  int64  \n",
      " 24  cardiovascular_history              700000 non-null  int64  \n",
      " 25  diagnosed_diabetes                  700000 non-null  float64\n",
      "dtypes: float64(6), int64(14), object(6)\n",
      "memory usage: 138.9+ MB\n"
     ]
    }
   ],
   "source": [
    "# Display dataset information (columns, dtypes, non-null counts)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3963201",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing\n",
    "\n",
    "### 1.1. Column definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "791edd04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label\n",
    "label = 'diagnosed_diabetes'\n",
    "\n",
    "# Define numerical features to apply IQR clipping\n",
    "numerical_features = ['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate', 'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides', 'family_history_diabetes', 'hypertension_history', 'cardiovascular_history']\n",
    "\n",
    "# Define ordinal features to encode\n",
    "ordinal_features = ['education_level', 'income_level']\n",
    "\n",
    "# Define ordinal categories in order\n",
    "education_categories = [['No formal', 'Highschool', 'Graduate', 'Postgraduate']]\n",
    "income_categories = [['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']]\n",
    "\n",
    "# Define features for one-hot encoding\n",
    "nominal_features = ['gender', 'ethnicity', 'smoking_status', 'employment_status']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f04ed1d7",
   "metadata": {},
   "source": [
    "### 1.2. Remove ID column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b00d402",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ID column removed\n",
      "Remaining columns: ['age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi', 'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate', 'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides', 'gender', 'ethnicity', 'education_level', 'income_level', 'smoking_status', 'employment_status', 'family_history_diabetes', 'hypertension_history', 'cardiovascular_history', 'diagnosed_diabetes']\n"
     ]
    }
   ],
   "source": [
    "# Apply ID column dropper\n",
    "if CHECK_PIPELINE_STEPS:\n",
    "\n",
    "    # Drop ID column using custom transformer\n",
    "    id_dropper = IDColumnDropper(id_column='id')\n",
    "    train_df = id_dropper.fit_transform(train_df)\n",
    "    \n",
    "    print(f'ID column removed')\n",
    "    print(f'Remaining columns: {list(train_df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747278f9",
   "metadata": {},
   "source": [
    "### 1.3. Numerical features\n",
    "\n",
    "#### 1.3.1. Outlier clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04c9fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply IQR clipping to numerical features\n",
    "if CHECK_PIPELINE_STEPS:\n",
    "\n",
    "    # Clip outliers at 2.0*IQR (Note: threshold will be optimized later)\n",
    "    clipper = IQRClipper(iqr_multiplier=2.0)\n",
    "    numerical_data = train_df[numerical_features].values\n",
    "    numerical_clipped = clipper.fit_transform(numerical_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e657847",
   "metadata": {},
   "source": [
    "#### 1.3.2. Standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "eb616fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply standard scaling\n",
    "if CHECK_PIPELINE_STEPS:\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    numerical_scaled = scaler.fit_transform(numerical_clipped)\n",
    "\n",
    "    # Replace original features with clipped and scaled values\n",
    "    train_df[numerical_features] = numerical_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0338d37e",
   "metadata": {},
   "source": [
    "#### 1.3.3. Final numerical feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90aad507",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution plots for scaled data\n",
    "if CHECK_PIPELINE_STEPS:\n",
    "    \n",
    "    num_cols = 4\n",
    "    num_rows = len(numerical_features) // num_cols + (len(numerical_features) % num_cols > 0)\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols*2.5, num_rows*2.3))\n",
    "    axes = axes.flatten()\n",
    "    fig.suptitle('Distribution of numerical features: clipped & scaled', fontsize=16)\n",
    "\n",
    "    for i, feature in enumerate(numerical_features):\n",
    "        \n",
    "        # Plot scaled data from training dataframe\n",
    "        axes[i].hist(train_df[feature], bins=30, edgecolor='black', color='grey')\n",
    "        axes[i].set_title(f'{feature}')\n",
    "        axes[i].set_xlabel('Value')\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].set_yscale('log')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "    # Turn off unused axes\n",
    "    for i in range(len(numerical_features), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10511a4f",
   "metadata": {},
   "source": [
    "### 1.4. Categorical features\n",
    "\n",
    "#### 1.4.1. Ordinal feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ea21243",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECK_PIPELINE_STEPS:\n",
    "\n",
    "    # Create ordinal encoder with categories\n",
    "    ordinal_encoder = OrdinalEncoder(\n",
    "        categories=education_categories + income_categories,\n",
    "        handle_unknown='use_encoded_value',\n",
    "        unknown_value=-1\n",
    "    )\n",
    "\n",
    "    # Fit and transform ordinal features\n",
    "    ordinal_encoded = ordinal_encoder.fit_transform(train_df[ordinal_features])\n",
    "    train_df[ordinal_features] = ordinal_encoded\n",
    "\n",
    "    # Remove original ordinal features and add encoded versions\n",
    "    train_df.drop(columns=ordinal_features, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c721d679",
   "metadata": {},
   "source": [
    "#### 1.4.2. Nominal feature encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f42aa3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECK_PIPELINE_STEPS:\n",
    "\n",
    "    # Create one-hot encoder\n",
    "    onehot_encoder = OneHotEncoder(\n",
    "        drop='first',\n",
    "        sparse_output=False,\n",
    "        handle_unknown='ignore'\n",
    "    )\n",
    "\n",
    "    # Convert encoded features to DataFrame\n",
    "    encoded_features_df = pd.DataFrame(\n",
    "        onehot_encoder.fit_transform(train_df[nominal_features]),\n",
    "        columns=onehot_encoder.get_feature_names_out(nominal_features)\n",
    "    )\n",
    "\n",
    "    # Remove original nominal features and add encoded versions\n",
    "    train_df = pd.concat([train_df.drop(columns=nominal_features), encoded_features_df], axis=1)\n",
    "    train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842fa2f9",
   "metadata": {},
   "source": [
    "#### 1.4.3. Final feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb197636",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Final base feature set shape: {train_df.drop(columns=[label]).shape}\\n')\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85bfd166",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "\n",
    "### 2.1. Polynomial feature transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b3b1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECK_PIPELINE_STEPS:\n",
    "\n",
    "    # Create polynomial features transformer with degree 2 (Note: degree will be optimized later)\n",
    "    poly_transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "    # Get all features except the label\n",
    "    feature_columns = [col for col in train_df.columns if col != label]\n",
    "    X_features = train_df[feature_columns]\n",
    "\n",
    "    # Apply polynomial feature transformation\n",
    "    X_poly = poly_transformer.fit_transform(X_features)\n",
    "\n",
    "    # Get feature names for the polynomial features\n",
    "    poly_feature_names = poly_transformer.get_feature_names_out(feature_columns)\n",
    "\n",
    "    # Create DataFrame with polynomial features\n",
    "    poly_df = pd.DataFrame(X_poly, columns=poly_feature_names, index=train_df.index)\n",
    "\n",
    "    print(f'Original polynomial features: {len(poly_feature_names)}')\n",
    "    print(f'Training data shape: {poly_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78fc4ddf",
   "metadata": {},
   "source": [
    "### 2.2. Constant feature removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3a905d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECK_PIPELINE_STEPS:\n",
    "\n",
    "    # Remove constant-valued features from polynomial features\n",
    "    constant_remover = ConstantFeatureRemover()\n",
    "    poly_filtered = constant_remover.fit_transform(poly_df.values)\n",
    "\n",
    "    # Get filtered feature names\n",
    "    filtered_feature_mask = np.ones(len(poly_feature_names), dtype=bool)\n",
    "    filtered_feature_mask[constant_remover.constant_features_] = False\n",
    "    filtered_feature_names = poly_feature_names[filtered_feature_mask]\n",
    "\n",
    "    # Create DataFrame with filtered features\n",
    "    poly_df_filtered = pd.DataFrame(poly_filtered, columns=filtered_feature_names, index=train_df.index)\n",
    "\n",
    "    print(f'Constant features removed: {len(constant_remover.constant_features_)}')\n",
    "    print(f'Filtered polynomial features: {len(filtered_feature_names)}')\n",
    "    print(f'Training data shape: {poly_df_filtered.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70967d1",
   "metadata": {},
   "source": [
    "### 2.3. Standard scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ad13ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECK_PIPELINE_STEPS:\n",
    "\n",
    "    # Apply standard scaling to filtered polynomial features\n",
    "    scaler_post_poly = StandardScaler()\n",
    "    poly_scaled = scaler_post_poly.fit_transform(poly_df_filtered.values)\n",
    "\n",
    "    # Create final DataFrame with scaled features\n",
    "    poly_df_final = pd.DataFrame(poly_scaled, columns=filtered_feature_names, index=train_df.index)\n",
    "\n",
    "    # Replace original features with filtered and scaled polynomial features\n",
    "    train_df = pd.concat([train_df[[label]], poly_df_final], axis=1)\n",
    "\n",
    "    print(f'Scaled polynomial features: {len(filtered_feature_names)}')\n",
    "    print(f'Final training data shape: {train_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2966a4a4",
   "metadata": {},
   "source": [
    "### 2.4. Polynomial feature distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "839b13ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution plots for a random sample of clipped and scaled polynomial features\n",
    "if CHECK_PIPELINE_STEPS:\n",
    "\n",
    "    sample_size = 40\n",
    "    sampled_features = np.random.choice(filtered_feature_names, size=min(sample_size, len(filtered_feature_names)), replace=False)\n",
    "\n",
    "    num_cols = 4\n",
    "    num_rows = len(sampled_features) // num_cols + (len(sampled_features) % num_cols > 0)\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols*2.5, num_rows*2.3))\n",
    "    axes = axes.flatten()\n",
    "    fig.suptitle(f'Distribution of polynomial features: scaled (random sample of {len(sampled_features)})', fontsize=16)\n",
    "\n",
    "    for i, feature in enumerate(sampled_features):\n",
    "        \n",
    "        # Plot scaled polynomial feature data from training dataframe\n",
    "        axes[i].hist(train_df[feature], bins=30, edgecolor='black', color='grey')\n",
    "        axes[i].set_title(f'{feature}', fontsize=8)\n",
    "        axes[i].set_xlabel('Value')\n",
    "        axes[i].set_ylabel('Count')\n",
    "        axes[i].set_yscale('log')\n",
    "        axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "    # Turn off unused axes\n",
    "    for i in range(len(sampled_features), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048a3c03",
   "metadata": {},
   "source": [
    "### 2.5. PCA dimensionality reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f474492",
   "metadata": {},
   "outputs": [],
   "source": [
    "if CHECK_PIPELINE_STEPS:\n",
    "\n",
    "    # Create PCA transformer with 40 components (Note: number of components will be optimized later)\n",
    "    pca = PCA(n_components=40)\n",
    "\n",
    "    # Apply PCA to the scaled polynomial features\n",
    "    pca_features = pca.fit_transform(poly_df_final.values)\n",
    "\n",
    "    # Create DataFrame with PCA features\n",
    "    pca_feature_names = [f'PC{i+1}' for i in range(pca.n_components)]\n",
    "    pca_df = pd.DataFrame(pca_features, columns=pca_feature_names, index=train_df.index)\n",
    "\n",
    "    # Replace features with PCA components\n",
    "    train_df = pd.concat([train_df[[label]], pca_df], axis=1)\n",
    "\n",
    "    print(f'PCA components: {pca.n_components}')\n",
    "    print(f'Explained variance ratio (cumulative): {pca.explained_variance_ratio_.cumsum()[-1]:.4f}')\n",
    "    print(f'Final training data shape: {train_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a73664",
   "metadata": {},
   "source": [
    "### 2.6. Principal component distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e21e6274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution plots for PCA components\n",
    "if CHECK_PIPELINE_STEPS:\n",
    "\n",
    "    num_cols = 4\n",
    "    num_rows = len(pca_feature_names) // num_cols + (len(pca_feature_names) % num_cols > 0)\n",
    "    fig, axes = plt.subplots(num_rows, num_cols, figsize=(num_cols*2.5, num_rows*2.3))\n",
    "    axes = axes.flatten()\n",
    "    fig.suptitle(f'Distribution of PCA components: {pca.n_components} components', fontsize=16)\n",
    "\n",
    "    for i, feature in enumerate(pca_feature_names):\n",
    "        \n",
    "        # Plot PCA component data from training dataframe\n",
    "        axes[i].hist(train_df[feature], bins=30, edgecolor='black', color='grey')\n",
    "        axes[i].set_title(f'{feature}', fontsize=8)\n",
    "        axes[i].set_xlabel('Value')\n",
    "        axes[i].set_ylabel('Count')\n",
    "\n",
    "    # Turn off unused axes\n",
    "    for i in range(len(pca_feature_names), len(axes)):\n",
    "        axes[i].axis('off')\n",
    "\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.98])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38adb359",
   "metadata": {},
   "source": [
    "## 3. Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95679ff7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload training dataset for model optimization\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "\n",
    "# Drop any duplicate rows\n",
    "train_df.drop_duplicates(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a56b27e7",
   "metadata": {},
   "source": [
    "### 3.1. Model pipeline definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e8deff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create preprocessing pipeline for numerical features\n",
    "numerical_pipeline = Pipeline([\n",
    "    ('clipper', IQRClipper()),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Create fresh encoder instances for the pipeline\n",
    "ordinal_encoder_pipeline = OrdinalEncoder(\n",
    "    categories=education_categories + income_categories,\n",
    "    handle_unknown='use_encoded_value',\n",
    "    unknown_value=-1\n",
    ")\n",
    "\n",
    "onehot_encoder_pipeline = OneHotEncoder(\n",
    "    drop='first',\n",
    "    sparse_output=False,\n",
    "    handle_unknown='ignore'\n",
    ")\n",
    "\n",
    "# Create column transformer to apply different transformations to different columns\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numerical_pipeline, numerical_features),\n",
    "        ('ord', ordinal_encoder_pipeline, ordinal_features),\n",
    "        ('nom', onehot_encoder_pipeline, nominal_features)\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Create full feature engineering & estimator pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('id_dropper', IDColumnDropper(id_column='id')),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('poly', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "    ('constant_remover', ConstantFeatureRemover()),\n",
    "    ('scaler_post_poly', StandardScaler()),\n",
    "    ('pca', PCA()),\n",
    "    ('logit', LogisticRegression())\n",
    "])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be0134cf",
   "metadata": {},
   "source": [
    "### 3.2. Model pipeline hyperparameter search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c9fdf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter distributions for optimization\n",
    "param_distributions = [\n",
    "    {\n",
    "        'preprocessor__num__clipper__iqr_multiplier': uniform(loc=1.25, scale=2.25),\n",
    "        'poly__degree': [2],\n",
    "        'poly__include_bias': [True, False],\n",
    "        'poly__interaction_only': [False, True],\n",
    "        'pca__n_components': randint(50, 401),\n",
    "        'pca__svd_solver': ['randomized'],\n",
    "        'pca__whiten': [True, False],\n",
    "        'logit__C': loguniform(0.001, 100),\n",
    "        'logit__penalty': ['l2'],\n",
    "        'logit__max_iter': [1000],\n",
    "        'logit__class_weight': ['balanced'],\n",
    "        'logit__n_jobs': [N_JOBS],\n",
    "    },\n",
    "    {\n",
    "        'preprocessor__num__clipper__iqr_multiplier': uniform(loc=1.25, scale=2.25),\n",
    "        'poly__degree': [2],\n",
    "        'poly__include_bias': [True, False],\n",
    "        'poly__interaction_only': [False, True],\n",
    "        'pca__n_components': randint(50, 401),\n",
    "        'pca__svd_solver': ['randomized'],\n",
    "        'pca__whiten': [True, False],\n",
    "        'logit__penalty': [None],\n",
    "        'logit__max_iter': [1000],\n",
    "        'logit__class_weight': ['balanced'],\n",
    "        'logit__n_jobs': [N_JOBS],\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214c9b08",
   "metadata": {},
   "source": [
    "### 3.3. RandomizedSearchCV resource requirement testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcdbf35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define experimental parameters\n",
    "sample_sizes = [1000, 2000, 4000, 8000]\n",
    "n_iters = [4, 8, 16, 32]\n",
    "\n",
    "# Define path for saving/loading results\n",
    "data_dir = Path('../data')\n",
    "runtime_results_path = data_dir / 'runtime_experiment_results.csv'\n",
    "\n",
    "if RANDOM_SEARCH_TEST:\n",
    "\n",
    "    # Runtime and memory experiment\n",
    "    results_df, full_dataset_size = random_search_test(\n",
    "        train_df_path=train_df_path,\n",
    "        label=label,\n",
    "        pipeline=pipeline,\n",
    "        param_distributions=param_distributions,\n",
    "        sample_sizes=sample_sizes,\n",
    "        n_iters=n_iters,\n",
    "        random_state=RANDOM_STATE,\n",
    "        n_jobs=1,\n",
    "        data_dir=data_dir,\n",
    "        results_filename='runtime_experiment_results.csv'\n",
    "    )\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load results from disk\n",
    "    results_df = pd.read_csv(runtime_results_path)\n",
    "    full_dataset_size = len(pd.read_csv(train_df_path))\n",
    "    \n",
    "    # Use sample sizes from the loaded data\n",
    "    sample_sizes = sorted(results_df['sample_size'].unique().tolist())\n",
    "    \n",
    "    print(f'Loaded {len(results_df)} results from: {runtime_results_path}')\n",
    "    print(f'Sample sizes in data: {sample_sizes}')\n",
    "\n",
    "# Display results\n",
    "results_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b86946",
   "metadata": {},
   "source": [
    "#### 3.3.1. Memory footprint modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451999d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build linear regression model to predict memory usage\n",
    "memory_model, memory_stats, results_with_memory_predictions = build_memory_model(results_df)\n",
    "\n",
    "# Create and display the memory model plots\n",
    "plot_memory_model(memory_model, memory_stats, results_df, results_with_memory_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be44a4e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate optimal sample size based on MEMORY_LIMIT_GB\n",
    "optimal_sample_size, memory_details = calculate_optimal_sample_size(\n",
    "    memory_model,\n",
    "    MEMORY_LIMIT_GB\n",
    ")\n",
    "\n",
    "# Calculate percentage of full dataset\n",
    "percentage = (optimal_sample_size / full_dataset_size) * 100\n",
    "\n",
    "print(f'Memory limit: {MEMORY_LIMIT_GB} GB ({int(memory_details[\"memory_limit_mb\"])} MB)')\n",
    "print(f'Optimal sample size: {optimal_sample_size:,} samples ({percentage:.1f}% of the full dataset)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7dbef1f",
   "metadata": {},
   "source": [
    "#### 3.3.2. Runtime modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb88ab2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build linear regression model to predict runtime\n",
    "runtime_model, runtime_stats, results_with_runtime_predictions = build_runtime_model(results_df)\n",
    "\n",
    "# Create and display the runtime model plots\n",
    "plot_runtime_model(runtime_model, runtime_stats, results_df, results_with_runtime_predictions, sample_sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26428913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate optimal number of iterations for optimal_sample_size based on RUNTIME_LIMIT_MIN\n",
    "optimal_n_iter, runtime_details = calculate_optimal_iterations(\n",
    "    runtime_model,\n",
    "    optimal_sample_size,\n",
    "    RUNTIME_LIMIT_MIN\n",
    ")\n",
    "\n",
    "print(f'Runtime limit: {RUNTIME_LIMIT_MIN} minutes ({int(runtime_details[\"runtime_limit_seconds\"])} seconds)')\n",
    "print(f'Sample size: {optimal_sample_size:,} samples')\n",
    "print(f'Optimal iterations: {optimal_n_iter}')\n",
    "print(f'Predicted runtime: {runtime_details[\"predicted_runtime_seconds\"]:.1f} seconds ({runtime_details[\"predicted_runtime_minutes\"]:.1f} minutes)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755dc25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw a random sample of the optimal size\n",
    "train_sample = train_df.sample(n=optimal_sample_size, random_state=RANDOM_STATE)\n",
    "\n",
    "print(f'Original dataset size: {len(train_df):,} samples')\n",
    "print(f'Sampled dataset size: {len(train_sample):,} samples')\n",
    "print(f'Sample is {(len(train_sample)/len(train_df)*100):.1f}% of original data')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00578e0a",
   "metadata": {},
   "source": [
    "### 3.4. RandomizedSearchCV optimization run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5bc75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create RandomizedSearchCV with optimal number of iterations\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=optimal_n_iter,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=8,\n",
    "    cv=CV_FOLDS,\n",
    "    random_state=RANDOM_STATE,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "start_optimization = time.time()\n",
    "random_search.fit(train_sample.drop(columns=[label]), train_sample[label])\n",
    "optimization_time = time.time() - start_optimization\n",
    "\n",
    "# Display best parameters and score\n",
    "print(f'\\nActual runtime: {optimization_time:.1f} seconds ({optimization_time/60:.1f} minutes)')\n",
    "print(f'Best cross-validation ROC-AUC score: {random_search.best_score_:.4f}')\n",
    "print('\\nBest parameters:')\n",
    "\n",
    "for param, value in random_search.best_params_.items():\n",
    "    print(f'  {param}: {value}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a989583",
   "metadata": {},
   "source": [
    "### 3.5. Final model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features and target from the sample\n",
    "X_train_sample = train_sample.drop(columns=[label])\n",
    "y_train_sample = train_sample[label]\n",
    "\n",
    "# Get the best model from randomized search (already trained on best params)\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Retrain on sampled dataset with best parameters\n",
    "best_model.fit(X_train_sample, y_train_sample)\n",
    "print(f'Model retrained on {len(X_train_sample):,} samples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99adb7e2",
   "metadata": {},
   "source": [
    "## 4. Inference resource testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf67c500",
   "metadata": {},
   "source": [
    "### 4.1. Load test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ea88a89",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_path = 'https://gperdrizet.github.io/FSA_devops/assets/data/unit3/diabetes_prediction_test.csv'\n",
    "\n",
    "# Load the test dataset\n",
    "test_df = pd.read_csv(test_df_path)\n",
    "\n",
    "# Display first few rows of test data\n",
    "print(f'Test dataset shape: {test_df.shape}')\n",
    "test_df.head().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d912ae6c",
   "metadata": {},
   "source": [
    "### 4.2. Test set inference memory requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c03f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start memory tracking\n",
    "tracemalloc.start()\n",
    "\n",
    "# Time the inference call\n",
    "start_time = time.perf_counter()\n",
    "\n",
    "predictions = best_model.predict(test_df)\n",
    "\n",
    "end_time = time.perf_counter()\n",
    "inference_time = end_time - start_time\n",
    "\n",
    "# Get peak memory usage during prediction\n",
    "current_memory, peak_memory = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "# Convert to GB\n",
    "memory_footprint_gb = peak_memory / (1024 * 1024 * 1024)\n",
    "\n",
    "# Calculate throughput\n",
    "samples_per_second = len(test_df) / inference_time\n",
    "\n",
    "print(f'\\nInference performance metrics:')\n",
    "print(f'  Dataset size: {len(test_df):,} samples')\n",
    "print(f'  Inference time: {inference_time:.4f} seconds')\n",
    "print(f'  Throughput: {samples_per_second:,.0f} samples/second')\n",
    "print(f'  Peak memory: {memory_footprint_gb:.4f} GB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b9d34",
   "metadata": {},
   "source": [
    "## 5. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cc18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model directory and ensure it exists\n",
    "model_dir = Path('../models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create model filename\n",
    "model_name = 'logistic_regression'\n",
    "model_path = model_dir / f'{model_name}.joblib'\n",
    "\n",
    "# Save the pipeline\n",
    "joblib.dump(best_model, model_path)\n",
    "print('Model saved to:', model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af43164",
   "metadata": {},
   "source": [
    "## 6. Save model description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6bc70ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model description filename\n",
    "markdown_path = model_dir / f'{model_name}.md'\n",
    "\n",
    "# Create markdown description\n",
    "markdown_content = f\"\"\"# Logistic Regression Model for Diabetes Prediction\n",
    "\n",
    "## Model overview\n",
    "\n",
    "This is a scikit-learn Pipeline object that chains together multiple preprocessing, feature engineering, and modeling steps into a single estimator. Pipelines ensure that all transformations are applied consistently during both training and inference, preventing data leakage and simplifying deployment. The pipeline was optimized using RandomizedSearchCV with resource constraints and serialized using joblib for efficient storage and loading of the fitted transformers and model.\n",
    "\n",
    "Key features:\n",
    "- **End-to-end processing**: Automatically handles all preprocessing from raw data to predictions\n",
    "- **Reproducible transformations**: All fitted parameters (scalers, encoders, PCA components) are preserved\n",
    "- **Hyperparameter optimization**: Parameters across all pipeline steps were jointly optimized\n",
    "- **Resource-aware training**: Model was trained within specified memory ({MEMORY_LIMIT_GB}GB) and runtime ({RUNTIME_LIMIT_MIN} min) constraints\n",
    "\n",
    "For details on model optimization and training, see the [Jupyter notebook on GitHub](https://github.com/gperdrizet/diabetes-prediction/blob/main/notebooks/01.1-logistic_regression_model.ipynb).\n",
    "\n",
    "## Files\n",
    "\n",
    "- **Model file**: `{model_path.name}` (scikit-learn Pipeline object serialized with joblib)\n",
    "- **Custom transformers**: `logistic_regression_transformers.py` (required for model deserialization)\n",
    "- **Documentation**: `{markdown_path.name}`\n",
    "\n",
    "## Training information\n",
    "\n",
    "- **Training date**: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "- **Training samples**: {len(X_train_sample):,}\n",
    "- **Random state**: {RANDOM_STATE}\n",
    "- **Cross-validation score (ROC-AUC)**: {random_search.best_score_:.4f}\n",
    "\n",
    "## Hyperparameter optimization\n",
    "\n",
    "- **Method**: Randomized Search CV\n",
    "- **Cross-validation folds**: {CV_FOLDS}\n",
    "- **Iterations**: {optimal_n_iter}\n",
    "- **Scoring metric**: ROC-AUC\n",
    "- **Optimization runtime**: {optimization_time:.1f} seconds ({optimization_time/60:.1f} minutes)\n",
    "\n",
    "## Inference performance\n",
    "\n",
    "Measured on test dataset with {len(test_df):,} samples using `tracemalloc` to track peak memory allocation:\n",
    "\n",
    "- **Inference time**: {inference_time:.4f} seconds\n",
    "- **Throughput**: {samples_per_second:,.0f} samples/second\n",
    "- **Peak memory**: {memory_footprint_gb:.4f} GB\n",
    "\n",
    "## Pipeline components\n",
    "\n",
    "### 1. Preprocessing\n",
    "\n",
    "#### ID column removal\n",
    "- **ID column dropper**: Automatically removes the 'id' column from input data (custom transformer)\n",
    "\n",
    "#### Numerical features\n",
    "- **IQR clipping**: Outlier clipping using interquartile range (multiplier: {random_search.best_params_.get('preprocessor__num__clipper__iqr_multiplier', 'N/A'):.2f}) *[optimized]* (custom transformer)\n",
    "- **Standardization**: Standard scaling (mean=0, std=1)\n",
    "- **Features**: age, alcohol_consumption_per_week, diet_score, physical_activity_minutes_per_week, sleep_hours_per_day, screen_time_hours_per_day, bmi, waist_to_hip_ratio, systolic_bp, diastolic_bp, heart_rate, cholesterol_total, hdl_cholesterol, ldl_cholesterol, triglycerides\n",
    "\n",
    "#### Ordinal features\n",
    "- **Ordinal encoding**: education_level, income_level\n",
    "\n",
    "#### Nominal features\n",
    "- **One-hot encoding**: gender, ethnicity, smoking_status, employment_status, family_history_diabetes, hypertension_history, cardiovascular_history (drop first category)\n",
    "\n",
    "### 2. Feature engineering\n",
    "\n",
    "- **Polynomial features**:\n",
    "  - Degree: {random_search.best_params_.get('poly__degree', 'N/A')} *[optimized]*\n",
    "  - Include bias: {random_search.best_params_.get('poly__include_bias', 'N/A')} *[optimized]*\n",
    "  - Interaction only: {random_search.best_params_.get('poly__interaction_only', 'N/A')} *[optimized]*\n",
    "\n",
    "- **Constant feature removal**: Removes features with zero variance (custom transformer)\n",
    "\n",
    "- **Post-polynomial standardization**: Standard scaling after polynomial transformation\n",
    "\n",
    "- **PCA dimensionality reduction**:\n",
    "  - Components: {random_search.best_params_.get('pca__n_components', 'N/A')} *[optimized]*\n",
    "  - SVD solver: {random_search.best_params_.get('pca__svd_solver', 'N/A')} *[optimized]*\n",
    "  - Whiten: {random_search.best_params_.get('pca__whiten', 'N/A')} *[optimized]*\n",
    "\n",
    "### 3. Classifier\n",
    "\n",
    "- **Algorithm**: Logistic regression\n",
    "- **Penalty**: {random_search.best_params_.get('logit__penalty', 'N/A')} *[optimized]*\n",
    "- **Regularization (C)**: {f\"{random_search.best_params_.get('logit__C'):.4f}\" if random_search.best_params_.get('logit__C') is not None else 'N/A'} *[optimized]*\n",
    "- **Max iterations**: {random_search.best_params_.get('logit__max_iter', 'N/A')}\n",
    "- **Class weight**: {random_search.best_params_.get('logit__class_weight', 'N/A')}\n",
    "\n",
    "## Custom transformers\n",
    "\n",
    "The model uses three custom scikit-learn transformers defined in `logistic_regression_transformers.py`:\n",
    "\n",
    "### IDColumnDropper\n",
    "Automatically removes the 'id' column from input DataFrames before processing. This allows the model to accept raw test data without manual preprocessing.\n",
    "\n",
    "### IQRClipper\n",
    "Clips outliers in numerical features based on the interquartile range (IQR). During fitting, calculates clipping bounds as Q1 - kIQR and Q3 + kIQR, where k is the optimized multiplier. This reduces the impact of extreme outliers while preserving the overall distribution.\n",
    "\n",
    "### ConstantFeatureRemover\n",
    "Removes features with zero variance after polynomial transformation. This eliminates redundant features that don't contribute to model predictions, reducing dimensionality and improving computational efficiency.\n",
    "\n",
    "**Important**: The `logistic_regression_transformers.py` file must be available in the Python path when loading the model, as joblib stores references to these classes and needs to import them during deserialization.\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the models directory to the path (adjust as needed)\n",
    "sys.path.insert(0, str(Path('models').resolve()))\n",
    "\n",
    "# Load the model (this will import the custom transformers)\n",
    "model = joblib.load('models/{model_path.name}')\n",
    "\n",
    "# Prepare test data (pipeline will automatically handle 'id' column)\n",
    "X_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "```\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Input data can include the 'id' column - it will be automatically removed by the pipeline\n",
    "- The pipeline handles all preprocessing and feature engineering automatically\n",
    "- All transformations are applied in the correct sequence without requiring manual intervention\n",
    "- Model was trained with resource constraints: {MEMORY_LIMIT_GB}GB memory limit, {RUNTIME_LIMIT_MIN} minute runtime limit\n",
    "- The `logistic_regression_transformers.py` file must be in the Python path when loading the model\n",
    "\"\"\"\n",
    "\n",
    "with open(markdown_path, 'w') as f:\n",
    "    f.write(markdown_content)\n",
    "\n",
    "print('Model description saved to:', markdown_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
