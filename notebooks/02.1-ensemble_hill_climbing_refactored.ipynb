{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1e07897",
   "metadata": {},
   "source": [
    "## Setup and configuration\n",
    "\n",
    "### Critical Fixes Applied:\n",
    "\n",
    "1. **Pre-sample Training Data**: Sample training data in main process before sending to workers - workers receive only ~2.5-15% of full pool (much faster serialization)\n",
    "2. **Keep DataFrames**: Data kept as DataFrames (required by sklearn ColumnTransformer for column name access)\n",
    "3. **Founder Simplification**: Founder model establishes baseline score but is NOT added to ensemble, simplifying batch/iteration indexing\n",
    "4. **Batch Size = N_CPUS**: Set batch size to match CPU count for maximum parallelization (20 workers training 20 models simultaneously)\n",
    "5. **Enhanced Error Handling**: Better progress tracking and graceful handling of failed models\n",
    "6. **Empty Ensemble Start**: Ensemble starts empty at iteration 1, avoiding index confusion\n",
    "7. **Zero Features Protection**: ConstantFeatureRemover keeps at least 1 feature to prevent downstream errors\n",
    "\n",
    "### Serialization Impact:\n",
    "- **Before**: 5 workers √ó 40,000 rows √ó 30 cols = ~48 MB per batch\n",
    "- **After**: 20 workers √ó ~2,400 rows √ó 30 cols = ~14 MB per batch (~70% reduction!)\n",
    "\n",
    "### CPU Utilization Strategy:\n",
    "- **Before**: 5 workers, each with multiple cores (only ~2 cores used total)\n",
    "- **After**: 20 workers, each with 1 core = all 20 cores busy simultaneously"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b77f679",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 18:08:22.000810: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1765148902.026365 1058970 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1765148902.037563 1058970 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.18.0\n",
      "Available CPUs: 24\n",
      "GPU disabled: CUDA drivers not available in dev container\n",
      "OMP_NUM_THREADS: Not set (sklearn controls parallelism via n_jobs)\n"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "from multiprocessing import cpu_count\n",
    "\n",
    "# Disable GPU and limit threading\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '-1'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n",
    "# os.environ['OMP_NUM_THREADS'] = '2'  # DISABLED - Let sklearn control parallelism via n_jobs\n",
    "\n",
    "# Third party imports\n",
    "import joblib\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "\n",
    "# Add models directory to path for ensemble_classifier import\n",
    "sys.path.insert(0, str(Path('../models').resolve()))\n",
    "\n",
    "# Import ensemble modules\n",
    "from ensemble_classifier import EnsembleClassifier\n",
    "from functions import ensemble_database\n",
    "from functions.ensemble_initialization import create_data_splits, create_base_preprocessor, train_founder_model\n",
    "from functions.ensemble_parallel import train_single_candidate, prepare_training_batch\n",
    "from functions.ensemble_evaluation import evaluate_candidate_ensemble\n",
    "from functions.ensemble_stage2_training import train_or_expand_stage2_model, save_ensemble_bundle\n",
    "from functions.ensemble_hill_climbing import (\n",
    "    adaptive_simulated_annealing_acceptance,\n",
    "    update_temperature,\n",
    "    log_iteration\n",
    ")\n",
    "from functions.ensemble_stage2_model import save_checkpoint\n",
    "\n",
    "# Configure TensorFlow\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "\n",
    "# Detect available CPUs\n",
    "n_cpus = cpu_count()\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"Available CPUs: {n_cpus}\")\n",
    "print(f\"GPU disabled: CUDA drivers not available in dev container\")\n",
    "print(f\"OMP_NUM_THREADS: Not set (sklearn controls parallelism via n_jobs)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffd99192",
   "metadata": {},
   "source": [
    "### Configuration parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e88b2241",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted existing database: /mnt/arkk/kaggle/diabetes-prediction/data/ensemble_training.db\n",
      "Database initialized at: /mnt/arkk/kaggle/diabetes-prediction/data/ensemble_training.db\n",
      "\n",
      "Configuration:\n",
      "  Total CPUs available: 20\n",
      "  Parallel workers: 20\n",
      "  Batch size: 20\n",
      "  Model timeout: 30 minutes\n",
      "  Note: Each worker trains ONE model. Models use internal parallelism via n_jobs=1\n",
      "        - This maximizes CPU utilization by training 20 models simultaneously\n",
      "        - Models that benefit from parallelism (RF, KNN) get n_jobs > 1 when few workers\n"
     ]
    }
   ],
   "source": [
    "# Random state for reproducibility\n",
    "RANDOM_STATE = 315\n",
    "\n",
    "# CPU allocation for parallel training\n",
    "# Set to None to use all available CPUs, or specify a number to limit\n",
    "N_CPUS = 20  # Will use all available cores by default\n",
    "\n",
    "if N_CPUS is None:\n",
    "    import multiprocessing\n",
    "    N_CPUS = multiprocessing.cpu_count()\n",
    "\n",
    "# Parallel training configuration\n",
    "BATCH_SIZE = 20  # Train this many candidates in parallel (increased to match N_CPUS)\n",
    "N_WORKERS = N_CPUS  # Use all available CPUs as workers\n",
    "MODEL_TIMEOUT_MINUTES = 30  # Maximum time per model (minutes)\n",
    "\n",
    "# Hill climbing configuration\n",
    "MAX_ITERATIONS = 500\n",
    "PLATEAU_ITERATIONS = 100\n",
    "BASE_TEMPERATURE = 0.05  # Increased from 0.01 for better exploration\n",
    "TEMPERATURE_DECAY = 0.998  # Slowed from 0.995 for sustained exploration\n",
    "\n",
    "# Stage 2 DNN configuration\n",
    "STAGE2_BATCH_SIZE_MODELS = 10  # Retrain DNN every N accepted models\n",
    "STAGE2_EPOCHS = 100\n",
    "STAGE2_BATCH_SIZE = 128\n",
    "STAGE2_PATIENCE = 10\n",
    "\n",
    "# Paths\n",
    "DATA_DIR = Path('../data')\n",
    "MODELS_BASE_DIR = Path('../models')\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "MODELS_DIR = MODELS_BASE_DIR / f'run_{timestamp}'\n",
    "ENSEMBLE_DIR = MODELS_DIR / 'ensemble_stage1_models'\n",
    "CHECKPOINT_PATH = MODELS_DIR / 'ensemble_checkpoint.pkl'\n",
    "\n",
    "# Create directories\n",
    "DATA_DIR.mkdir(parents=True, exist_ok=True)\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "ENSEMBLE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Initialize database\n",
    "ensemble_database.reset_database()\n",
    "ensemble_database.init_database()\n",
    "\n",
    "print(f\"\\nConfiguration:\")\n",
    "print(f\"  Total CPUs available: {N_CPUS}\")\n",
    "print(f\"  Parallel workers: {N_WORKERS}\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Model timeout: {MODEL_TIMEOUT_MINUTES} minutes\")\n",
    "print(f\"  Note: Each worker trains ONE model. Models use internal parallelism via n_jobs=1\")\n",
    "print(f\"        - This maximizes CPU utilization by training {N_WORKERS} models simultaneously\")\n",
    "print(f\"        - Models that benefit from parallelism (RF, KNN) get n_jobs > 1 when few workers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d732b6",
   "metadata": {},
   "source": [
    "## Data loading and preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8f0af37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shape: (700000, 26)\n",
      "Class distribution:\n",
      "diagnosed_diabetes\n",
      "1.0    0.623296\n",
      "0.0    0.376704\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Load training data\n",
    "train_df_path = 'https://gperdrizet.github.io/FSA_devops/assets/data/unit3/diabetes_prediction_train.csv'\n",
    "train_df = pd.read_csv(train_df_path)\n",
    "train_df.drop_duplicates(inplace=True)\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(train_df['diagnosed_diabetes'].value_counts(normalize=True))\n",
    "\n",
    "# Define features\n",
    "label = 'diagnosed_diabetes'\n",
    "numerical_features = [\n",
    "    'age', 'alcohol_consumption_per_week', 'physical_activity_minutes_per_week',\n",
    "    'diet_score', 'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi',\n",
    "    'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate',\n",
    "    'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides',\n",
    "    'family_history_diabetes', 'hypertension_history', 'cardiovascular_history'\n",
    "]\n",
    "ordinal_features = ['education_level', 'income_level']\n",
    "education_categories = [['No formal', 'Highschool', 'Graduate', 'Postgraduate']]\n",
    "income_categories = [['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']]\n",
    "nominal_features = ['gender', 'ethnicity', 'smoking_status', 'employment_status']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4c44282",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fixed data split:\n",
      "--------------------------------------------------------------------------------\n",
      "  Training pool: 420,000 samples (60%)\n",
      "  Stage 1 validation: 140,000 samples (20%) - for stage 1 eval & stage 2 training\n",
      "  Stage 2 validation: 140,000 samples (20%) - for stage 2 eval (HELD OUT)\n"
     ]
    }
   ],
   "source": [
    "# Create fixed three-way data split\n",
    "X_train_pool, X_val_s1, X_val_s2, y_train_pool, y_val_s1, y_val_s2 = create_data_splits(\n",
    "    train_df, label, RANDOM_STATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70c122c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data info before batch preparation:\n",
      "  X_train_pool: <class 'pandas.core.frame.DataFrame'> - (420000, 25)\n",
      "  y_train_pool: <class 'pandas.core.series.Series'> - (420000,)\n",
      "  X_val_s1: <class 'pandas.core.frame.DataFrame'> - (140000, 25)\n",
      "\n",
      "Optimizations:\n",
      "  1. Data kept as DataFrames (required by ColumnTransformer)\n",
      "  2. Each worker receives PRE-SAMPLED data (~2.5-27.5% of pool)\n",
      "  3. Estimated data per worker: ~63000 rows (vs 420000 full pool)\n",
      "  4. CPU strategy: Train 20 models in parallel, each with n_jobs=1\n",
      "     - Maximizes CPU utilization: 20 processes √ó 1 core = 20 cores used\n"
     ]
    }
   ],
   "source": [
    "print(\"Data info before batch preparation:\")\n",
    "print(f\"  X_train_pool: {type(X_train_pool)} - {X_train_pool.shape if hasattr(X_train_pool, 'shape') else 'N/A'}\")\n",
    "print(f\"  y_train_pool: {type(y_train_pool)} - {y_train_pool.shape if hasattr(y_train_pool, 'shape') else 'N/A'}\")\n",
    "print(f\"  X_val_s1: {type(X_val_s1)} - {X_val_s1.shape if hasattr(X_val_s1, 'shape') else 'N/A'}\")\n",
    "print(f\"\\nOptimizations:\")\n",
    "print(f\"  1. Data kept as DataFrames (required by ColumnTransformer)\")\n",
    "print(f\"  2. Each worker receives PRE-SAMPLED data (~2.5-27.5% of pool)\")\n",
    "print(f\"  3. Estimated data per worker: ~{len(X_train_pool) * 0.15:.0f} rows (vs {len(X_train_pool)} full pool)\")\n",
    "print(f\"  4. CPU strategy: Train {BATCH_SIZE} models in parallel, each with n_jobs=1\")\n",
    "print(f\"     - Maximizes CPU utilization: {BATCH_SIZE} processes √ó 1 core = {BATCH_SIZE} cores used\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7eda43",
   "metadata": {},
   "source": [
    "### Diagnostic: Check data types before parallel processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "230c09f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Base preprocessor created\n",
      "  Numerical features: 18\n",
      "  Ordinal features: 2\n",
      "  Nominal features: 4\n"
     ]
    }
   ],
   "source": [
    "# Create base preprocessor\n",
    "base_preprocessor = create_base_preprocessor(\n",
    "    numerical_features, ordinal_features, nominal_features,\n",
    "    education_categories, income_categories\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd576f7",
   "metadata": {},
   "source": [
    "## Initialize ensemble with founder model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73218e97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "TRAINING FOUNDER MODEL (baseline only - NOT added to ensemble)\n",
      "================================================================================\n",
      "\n",
      "Training founder model\n",
      "--------------------------------------------------------------------------------\n",
      "  Training samples: 42,000 (10% of 420,000 pool)\n",
      "  Pipeline config:\n",
      "    Classifier: random_forest\n",
      "    Transformers: []\n",
      "    Dimensionality reduction: factor_analysis\n",
      "  Training pipeline...\n",
      "  Training complete (15.5s)\n",
      "  Stage 1 validation AUC: 0.592439\n",
      "  Stage 2 validation AUC: 0.593767\n",
      "\n",
      "================================================================================\n",
      "FOUNDER MODEL COMPLETE - Baseline score established\n",
      "================================================================================\n",
      "\n",
      "Founder baseline AUC: 0.593767\n",
      "Ensemble starts empty - first batch will be iterations 1-20\n",
      "Stage 2 DNN will be trained after 10 accepted models\n"
     ]
    }
   ],
   "source": [
    "# Train founder model (baseline only - NOT added to ensemble)\n",
    "founder_auc = train_founder_model(\n",
    "    X_train_pool, X_val_s1, X_val_s2, y_train_pool, y_val_s1, y_val_s2,\n",
    "    base_preprocessor, RANDOM_STATE, BASE_TEMPERATURE, ENSEMBLE_DIR\n",
    ")\n",
    "\n",
    "# Initialize ensemble (EMPTY - founder not included)\n",
    "ensemble_models = []\n",
    "stage2_model = None\n",
    "best_ensemble_score = founder_auc\n",
    "\n",
    "# Initialize hill climbing variables (start at iteration 1, not 0)\n",
    "start_iteration = 1\n",
    "temperature = BASE_TEMPERATURE\n",
    "\n",
    "print(f\"\\nFounder baseline AUC: {founder_auc:.6f}\")\n",
    "print(f\"Ensemble starts empty - first batch will be iterations 1-{BATCH_SIZE}\")\n",
    "print(f\"Stage 2 DNN will be trained after {STAGE2_BATCH_SIZE_MODELS} accepted models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8df88a",
   "metadata": {},
   "source": [
    "## Parallel hill climbing loop\n",
    "\n",
    "Iteratively trains batches of candidate models in parallel, evaluates with hybrid scoring,\n",
    "and accepts/rejects using simulated annealing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1074db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "STARTING PARALLEL HILL CLIMBING LOOP\n",
      "================================================================================\n",
      "Batch size: 20 candidates trained in parallel\n",
      "Workers: 20 parallel processes\n",
      "Total CPUs: 20 (distributed intelligently across models)\n",
      "\n",
      "================================================================================\n",
      "BATCH Starting at iteration 1\n",
      "================================================================================\n",
      "Ensemble size: 0 | Best score: 0.593767 | Temperature: 0.010000 | No improvement: 0/100\n",
      "\n",
      "Preparing batch jobs...\n",
      "Batch prepared: 20 jobs ready\n",
      "\n",
      "Training 20 candidates in parallel (30 min timeout per model)...\n",
      "Batch prepared: 20 jobs ready\n",
      "\n",
      "Training 20 candidates in parallel (30 min timeout per model)...\n",
      "\n",
      "[Iteration 1] Training qda\n",
      "  Sample size: 53215 rows (12.0%)\n",
      "  Feature sampling: 63.2%\n",
      "  Transformers: ratio\n",
      "\n",
      "[Iteration 1] Training qda\n",
      "  Sample size: 53215 rows (12.0%)\n",
      "  Feature sampling: 63.2%\n",
      "  Transformers: ratio\n",
      "\n",
      "[Iteration 2] Training logistic_regression\n",
      "  Sample size: 22826 rows (9.2%)\n",
      "  Feature sampling: 42.2%\n",
      "  Transformers: sum, noise_injector, rbf_sampler\n",
      "\n",
      "[Iteration 2] Training logistic_regression\n",
      "  Sample size: 22826 rows (9.2%)\n",
      "  Feature sampling: 42.2%\n",
      "  Transformers: sum, noise_injector, rbf_sampler\n",
      "\n",
      "[Iteration 3] Training extra_trees\n",
      "  Sample size: 39133 rows (13.7%)\n",
      "  Feature sampling: 53.5%\n",
      "  Transformers: kde, kmeans, sqrt\n",
      "\n",
      "[Iteration 3] Training extra_trees\n",
      "  Sample size: 39133 rows (13.7%)\n",
      "  Feature sampling: 53.5%\n",
      "  Transformers: kde, kmeans, sqrt\n",
      "\n",
      "[Iteration 4] Training qda\n",
      "[Iteration 4] Training qda\n",
      "  Sample size: 62910 rows (5.4%)\n",
      "  Sample size: 62910 rows (5.4%)\n",
      "  Feature sampling: 69.9%\n",
      "\n",
      "  Feature sampling: 69.9%\n",
      "  Transformers: binning, rbf_sampler, kde\n",
      "  Transformers: binning, rbf_sampler, kde\n",
      "\n",
      "[Iteration 5] Training random_forest\n",
      "  Sample size: 46503 rows (11.0%)\n",
      "  Feature sampling: 58.6%\n",
      "[Iteration 5] Training random_forest\n",
      "  Sample size: 46503 rows (11.0%)\n",
      "  Feature sampling: 58.6%\n",
      "  Transformers: None\n",
      "\n",
      "  Transformers: None\n",
      "  [1/20] ‚úì Iteration 1: qda AUC=0.548914 (1.6s)\n",
      "  [1/20] ‚úì Iteration 1: qda AUC=0.548914 (1.6s)\n",
      "\n",
      "[Iteration 6] Training qda\n",
      "  Sample size: 56413 rows (8.5%)\n",
      "  Feature sampling: 65.4%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 6] Training qda\n",
      "  Sample size: 56413 rows (8.5%)\n",
      "  Feature sampling: 65.4%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 7] Training adaboost\n",
      "  Sample size: 29076 rows (4.0%)\n",
      "  Feature sampling: 46.5%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 7] Training adaboost\n",
      "  Sample size: 29076 rows (4.0%)\n",
      "  Feature sampling: 46.5%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 8] Training qda\n",
      "  Sample size: 61463 rows (3.9%)\n",
      "  Feature sampling: 68.9%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 8] Training qda\n",
      "  Sample size: 61463 rows (3.9%)\n",
      "  Feature sampling: 68.9%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 9] Training sgd_classifier\n",
      "  Sample size: 8733 rows (1.9%)\n",
      "  Feature sampling: 32.4%\n",
      "  Transformers: kmeans\n",
      "\n",
      "[Iteration 9] Training sgd_classifier\n",
      "  Sample size: 8733 rows (1.9%)\n",
      "  Feature sampling: 32.4%\n",
      "  Transformers: kmeans\n",
      "\n",
      "[Iteration 10] Training logistic_regression\n",
      "\n",
      "[Iteration 10] Training logistic_regression\n",
      "  Sample size: 58826 rows (9.2%)\n",
      "  Sample size: 58826 rows (9.2%)\n",
      "  Feature sampling: 67.1%\n",
      "  Feature sampling: 67.1%\n",
      "  Transformers: ratio, product, standard_scaler  Transformers: ratio, product, standard_scaler\n",
      "\n",
      "\n",
      "[Iteration 11] Training sgd_classifier\n",
      "  Sample size: 35252 rows (10.0%)\n",
      "  Feature sampling: 50.8%\n",
      "\n",
      "[Iteration 11] Training sgd_classifier\n",
      "  Sample size: 35252 rows (10.0%)\n",
      "  Feature sampling: 50.8%\n",
      "  Transformers: reciprocal, sum\n",
      "  Transformers: reciprocal, sum\n",
      "\n",
      "[Iteration 12] Training logistic_regression\n",
      "  Sample size: 31119 rows (4.0%)\n",
      "[Iteration 12] Training logistic_regression\n",
      "  Sample size: 31119 rows (4.0%)\n",
      "  Feature sampling: 47.9%\n",
      "\n",
      "  Feature sampling: 47.9%\n",
      "  Transformers: log, reciprocal, product\n",
      "  Transformers: log, reciprocal, product\n",
      "  [2/20] ‚úì Iteration 6: qda AUC=0.639879 (3.2s)\n",
      "  [2/20] ‚úì Iteration 6: qda AUC=0.639879 (3.2s)\n",
      "\n",
      "[Iteration 13] Training adaboost\n",
      "[Iteration 13] Training adaboost\n",
      "  Sample size: 19259 rows (2.1%)\n",
      "\n",
      "  Sample size: 19259 rows (2.1%)\n",
      "  Feature sampling: 39.7%\n",
      "  Transformers: None\n",
      "  Feature sampling: 39.7%\n",
      "  Transformers: None\n",
      "  [3/20] ‚úì Iteration 2: logistic_regression AUC=0.578989 (6.1s)\n",
      "  [3/20] ‚úì Iteration 2: logistic_regression AUC=0.578989 (6.1s)\n",
      "\n",
      "[Iteration 14] Training adaboost\n",
      "  Sample size: 38388 rows (1.8%)\n",
      "  Feature sampling: 53.0%\n",
      "[Iteration 14] Training adaboost\n",
      "  Sample size: 38388 rows (1.8%)\n",
      "  Feature sampling: 53.0%\n",
      "  Transformers: difference\n",
      "  Transformers: difference\n",
      "\n",
      "\n",
      "[Iteration 15] Training random_forest\n",
      "  Sample size: 33881 rows (11.6%)\n",
      "  Feature sampling: 49.8%\n",
      "  Transformers: None\n",
      "[Iteration 15] Training random_forest\n",
      "  Sample size: 33881 rows (11.6%)\n",
      "  Feature sampling: 49.8%\n",
      "  Transformers: None\n",
      "\n",
      "  [4/20] ‚úì Iteration 10: logistic_regression AUC=0.534238 (3.5s)\n",
      "  [4/20] ‚úì Iteration 10: logistic_regression AUC=0.534238 (3.5s)\n",
      "\n",
      "[Iteration 16] Training logistic_regression\n",
      "\n",
      "[Iteration 16] Training logistic_regression\n",
      "  Sample size: 59920 rows (4.6%)  Sample size: 59920 rows (4.6%)\n",
      "  Feature sampling: 67.9%\n",
      "\n",
      "  Feature sampling: 67.9%\n",
      "  Transformers: None\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 17] Training linear_svc\n",
      "  Sample size: 58274 rows (2.6%)\n",
      "\n",
      "[Iteration 17] Training linear_svc\n",
      "  Sample size: 58274 rows (2.6%)\n",
      "  Feature sampling: 66.7%\n",
      "  Feature sampling: 66.7%\n",
      "  Transformers: ratio\n",
      "  Transformers: ratio\n",
      "\n",
      "[Iteration 18] Training linear_svc\n",
      "\n",
      "[Iteration 18] Training linear_svc\n",
      "  Sample size: 36625 rows (1.8%)\n",
      "  [5/20] ‚úì Iteration 13: adaboost AUC=0.621146 (3.0s)\n",
      "  Sample size: 36625 rows (1.8%)\n",
      "  [5/20] ‚úì Iteration 13: adaboost AUC=0.621146 (3.0s)\n",
      "  Feature sampling: 51.7%\n",
      "  Transformers: noise_injector, nystroem, difference\n",
      "  Feature sampling: 51.7%\n",
      "  Transformers: noise_injector, nystroem, difference\n",
      "\n",
      "[Iteration 19] Training extra_trees\n",
      "\n",
      "[Iteration 19] Training extra_trees\n",
      "  Sample size: 19002 rows (2.7%)\n",
      "  Feature sampling: 39.5%  Sample size: 19002 rows (2.7%)\n",
      "  Feature sampling: 39.5%\n",
      "  Transformers: None\n",
      "\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 20] Training logistic_regression\n",
      "  Sample size: 49169 rows (8.3%)\n",
      "[Iteration 20] Training logistic_regression\n",
      "  Sample size: 49169 rows (8.3%)\n",
      "  Feature sampling: 60.4%\n",
      "\n",
      "  Feature sampling: 60.4%\n",
      "  Transformers: None\n",
      "  Transformers: None\n",
      "  [6/20] ‚úì Iteration 12: logistic_regression AUC=0.501087 (5.5s)\n",
      "  [6/20] ‚úì Iteration 12: logistic_regression AUC=0.501087 (5.5s)\n",
      "  [7/20] ‚úó Iteration 9 FAILED: Found array with 0 feature(s) (shape=(8733, 0)) while a minimum of 1 is required by StandardScaler.\n",
      "  [8/20] ‚úì Iteration 16: logistic_regression AUC=0.614789 (3.8s)\n",
      "  [7/20] ‚úó Iteration 9 FAILED: Found array with 0 feature(s) (shape=(8733, 0)) while a minimum of 1 is required by StandardScaler.\n",
      "  [8/20] ‚úì Iteration 16: logistic_regression AUC=0.614789 (3.8s)\n",
      "  [9/20] ‚úì Iteration 17: linear_svc AUC=0.544742 (5.0s)\n",
      "  [9/20] ‚úì Iteration 17: linear_svc AUC=0.544742 (5.0s)\n",
      "  [10/20] ‚úì Iteration 18: linear_svc AUC=0.645165 (5.3s)\n",
      "  [10/20] ‚úì Iteration 18: linear_svc AUC=0.645165 (5.3s)\n",
      "  [11/20] ‚úì Iteration 20: logistic_regression AUC=0.595120 (5.1s)\n",
      "  [11/20] ‚úì Iteration 20: logistic_regression AUC=0.595120 (5.1s)\n",
      "  [12/20] ‚úì Iteration 11: sgd_classifier AUC=0.521589 (26.6s)\n",
      "  [12/20] ‚úì Iteration 11: sgd_classifier AUC=0.521589 (26.6s)\n",
      "  [13/20] ‚úì Iteration 8: qda AUC=0.579761 (31.0s)\n",
      "  [13/20] ‚úì Iteration 8: qda AUC=0.579761 (31.0s)\n",
      "  [14/20] ‚úó Iteration 3 TIMEOUT: sgd_classifier exceeded 30 minutes\n",
      "  [14/20] ‚úó Iteration 3 TIMEOUT: sgd_classifier exceeded 30 minutes\n",
      "  [15/20] ‚úó Iteration 4 TIMEOUT: random_forest exceeded 30 minutes\n",
      "  [15/20] ‚úó Iteration 4 TIMEOUT: random_forest exceeded 30 minutes\n",
      "  [16/20] ‚úó Iteration 5 TIMEOUT: linear_svc exceeded 30 minutes\n",
      "  [16/20] ‚úó Iteration 5 TIMEOUT: linear_svc exceeded 30 minutes\n",
      "  [17/20] ‚úó Iteration 7 TIMEOUT: logistic exceeded 30 minutes\n",
      "  [17/20] ‚úó Iteration 7 TIMEOUT: logistic exceeded 30 minutes\n",
      "  [18/20] ‚úó Iteration 14 TIMEOUT: ridge exceeded 30 minutes\n",
      "  [18/20] ‚úó Iteration 14 TIMEOUT: ridge exceeded 30 minutes\n",
      "  [19/20] ‚úó Iteration 15 TIMEOUT: logistic exceeded 30 minutes\n",
      "  [19/20] ‚úó Iteration 15 TIMEOUT: logistic exceeded 30 minutes\n",
      "  [20/20] ‚úó Iteration 19 TIMEOUT: linear_svc exceeded 30 minutes\n",
      "\n",
      "‚ö†Ô∏è  8/20 models failed during training\n",
      "  [20/20] ‚úó Iteration 19 TIMEOUT: linear_svc exceeded 30 minutes\n",
      "\n",
      "‚ö†Ô∏è  8/20 models failed during training\n",
      "\n",
      "Batch complete (1811.5s, 151.0s per model)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 1: qda | Stage 1 AUC: 0.548914\n",
      "  Ensemble AUC (single_model): 0.548914 (first model)\n",
      "  Decision: ‚úó REJECT (rejected: Œî=-0.044853, P=0.011274)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 2: logistic_regression | Stage 1 AUC: 0.578989\n",
      "  Ensemble AUC (single_model): 0.578989 (first model)\n",
      "  Decision: ‚úó REJECT (rejected: Œî=-0.014778, P=0.226457)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 6: qda | Stage 1 AUC: 0.639879\n",
      "  Ensemble AUC (single_model): 0.639879 (first model)\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.046112)\n",
      "  üéâ New best score: 0.639879 (Œî=0.046112)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 8: qda | Stage 1 AUC: 0.579761\n",
      "\n",
      "Batch complete (1811.5s, 151.0s per model)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 1: qda | Stage 1 AUC: 0.548914\n",
      "  Ensemble AUC (single_model): 0.548914 (first model)\n",
      "  Decision: ‚úó REJECT (rejected: Œî=-0.044853, P=0.011274)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 2: logistic_regression | Stage 1 AUC: 0.578989\n",
      "  Ensemble AUC (single_model): 0.578989 (first model)\n",
      "  Decision: ‚úó REJECT (rejected: Œî=-0.014778, P=0.226457)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 6: qda | Stage 1 AUC: 0.639879\n",
      "  Ensemble AUC (single_model): 0.639879 (first model)\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.046112)\n",
      "  üéâ New best score: 0.639879 (Œî=0.046112)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 8: qda | Stage 1 AUC: 0.579761\n",
      "  Ensemble AUC (simple mean (all)): 0.636919 | Diversity: 0.405911\n",
      "  Decision: ‚úó REJECT (rejected: Œî=-0.002959, P=0.737165)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 10: logistic_regression | Stage 1 AUC: 0.534238\n",
      "  Ensemble AUC (simple mean (all)): 0.636919 | Diversity: 0.405911\n",
      "  Decision: ‚úó REJECT (rejected: Œî=-0.002959, P=0.737165)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 10: logistic_regression | Stage 1 AUC: 0.534238\n",
      "  Ensemble AUC (simple mean (all)): 0.643451 | Diversity: 0.169090\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.003572)\n",
      "  üéâ New best score: 0.643451 (Œî=0.003572)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 11: sgd_classifier | Stage 1 AUC: 0.521589\n",
      "  Ensemble AUC (simple mean (all)): 0.643451 | Diversity: 0.169090\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.003572)\n",
      "  üéâ New best score: 0.643451 (Œî=0.003572)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 11: sgd_classifier | Stage 1 AUC: 0.521589\n",
      "  Ensemble AUC (simple mean (all)): 0.643100 | Diversity: 0.080680\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000351, P=0.963754)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 12: logistic_regression | Stage 1 AUC: 0.501087\n",
      "  Ensemble AUC (simple mean (all)): 0.643100 | Diversity: 0.080680\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000351, P=0.963754)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 12: logistic_regression | Stage 1 AUC: 0.501087\n",
      "  Ensemble AUC (simple mean (all)): 0.643022 | Diversity: 0.050286\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000429, P=0.955673)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 13: adaboost | Stage 1 AUC: 0.621146\n",
      "  Ensemble AUC (simple mean (all)): 0.643022 | Diversity: 0.050286\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000429, P=0.955673)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 13: adaboost | Stage 1 AUC: 0.621146\n",
      "  Ensemble AUC (simple mean (all)): 0.645029 | Diversity: 0.124144\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.001578)\n",
      "  üéâ New best score: 0.645029 (Œî=0.001578)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 16: logistic_regression | Stage 1 AUC: 0.614789\n",
      "  Ensemble AUC (simple mean (all)): 0.645029 | Diversity: 0.124144\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.001578)\n",
      "  üéâ New best score: 0.645029 (Œî=0.001578)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 16: logistic_regression | Stage 1 AUC: 0.614789\n",
      "  Ensemble AUC (simple mean (all)): 0.649325 | Diversity: 0.172052\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.004297)\n",
      "  üéâ New best score: 0.649325 (Œî=0.004297)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 17: linear_svc | Stage 1 AUC: 0.544742\n",
      "  Ensemble AUC (simple mean (all)): 0.649325 | Diversity: 0.172052\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.004297)\n",
      "  üéâ New best score: 0.649325 (Œî=0.004297)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 17: linear_svc | Stage 1 AUC: 0.544742\n",
      "  Ensemble AUC (simple mean (all)): 0.646333 | Diversity: 0.186247\n",
      "  Decision: ‚úó REJECT (rejected: Œî=-0.002992, P=0.723078)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 18: linear_svc | Stage 1 AUC: 0.645165\n",
      "  Ensemble AUC (simple mean (all)): 0.646333 | Diversity: 0.186247\n",
      "  Decision: ‚úó REJECT (rejected: Œî=-0.002992, P=0.723078)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 18: linear_svc | Stage 1 AUC: 0.645165\n",
      "  Ensemble AUC (simple mean (all)): 0.652257 | Diversity: 0.245937\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.002932)\n",
      "  üéâ New best score: 0.652257 (Œî=0.002932)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 20: logistic_regression | Stage 1 AUC: 0.595120\n",
      "  Ensemble AUC (simple mean (all)): 0.652257 | Diversity: 0.245937\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.002932)\n",
      "  üéâ New best score: 0.652257 (Œî=0.002932)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 20: logistic_regression | Stage 1 AUC: 0.595120\n",
      "  Ensemble AUC (simple mean (all)): 0.651645 | Diversity: 0.273307\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000612, P=0.935250)\n",
      "\n",
      "================================================================================\n",
      "BATCH Starting at iteration 13\n",
      "================================================================================\n",
      "Ensemble size: 8 | Best score: 0.652257 | Temperature: 0.009046 | No improvement: 1/100\n",
      "\n",
      "Preparing batch jobs...\n",
      "  Ensemble AUC (simple mean (all)): 0.651645 | Diversity: 0.273307\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000612, P=0.935250)\n",
      "\n",
      "================================================================================\n",
      "BATCH Starting at iteration 13\n",
      "================================================================================\n",
      "Ensemble size: 8 | Best score: 0.652257 | Temperature: 0.009046 | No improvement: 1/100\n",
      "\n",
      "Preparing batch jobs...\n",
      "Batch prepared: 20 jobs ready\n",
      "\n",
      "Training 20 candidates in parallel (30 min timeout per model)...\n",
      "Batch prepared: 20 jobs ready\n",
      "\n",
      "Training 20 candidates in parallel (30 min timeout per model)...\n",
      "\n",
      "[Iteration 13] Training adaboost\n",
      "  Sample size: 19259 rows (2.1%)\n",
      "  Feature sampling: 39.7%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 13] Training adaboost\n",
      "  Sample size: 19259 rows (2.1%)\n",
      "  Feature sampling: 39.7%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 14] Training adaboost\n",
      "  Sample size: 38388 rows (1.8%)\n",
      "  Feature sampling: 53.0%\n",
      "  Transformers: difference\n",
      "\n",
      "[Iteration 14] Training adaboost\n",
      "  Sample size: 38388 rows (1.8%)\n",
      "  Feature sampling: 53.0%\n",
      "  Transformers: difference\n",
      "\n",
      "[Iteration 15] Training random_forest\n",
      "  Sample size: 33881 rows (11.6%)\n",
      "  Feature sampling: 49.8%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 15] Training random_forest\n",
      "  Sample size: 33881 rows (11.6%)\n",
      "  Feature sampling: 49.8%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 16] Training logistic_regression\n",
      "  Sample size: 59920 rows (4.6%)\n",
      "  Feature sampling: 67.9%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 16] Training logistic_regression\n",
      "  Sample size: 59920 rows (4.6%)\n",
      "  Feature sampling: 67.9%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 17] Training linear_svc\n",
      "  Sample size: 58274 rows (2.6%)\n",
      "  Feature sampling: 66.7%\n",
      "  Transformers: ratio\n",
      "\n",
      "[Iteration 17] Training linear_svc\n",
      "  Sample size: 58274 rows (2.6%)\n",
      "  Feature sampling: 66.7%\n",
      "  Transformers: ratio\n",
      "\n",
      "[Iteration 18] Training linear_svc\n",
      "  Sample size: 36625 rows (1.8%)\n",
      "  Feature sampling: 51.7%\n",
      "  Transformers: noise_injector, nystroem, difference\n",
      "\n",
      "[Iteration 18] Training linear_svc\n",
      "  Sample size: 36625 rows (1.8%)\n",
      "  Feature sampling: 51.7%\n",
      "  Transformers: noise_injector, nystroem, difference\n",
      "\n",
      "[Iteration 19] Training extra_trees\n",
      "  Sample size: 19002 rows (2.7%)\n",
      "[Iteration 19] Training extra_trees\n",
      "  Sample size: 19002 rows (2.7%)\n",
      "  Feature sampling: 39.5%\n",
      "  Transformers: None\n",
      "\n",
      "  Feature sampling: 39.5%\n",
      "  Transformers: None\n",
      "  [1/20] ‚úì Iteration 13: adaboost AUC=0.621146 (1.1s)\n",
      "  [1/20] ‚úì Iteration 13: adaboost AUC=0.621146 (1.1s)\n",
      "\n",
      "[Iteration 20] Training logistic_regression\n",
      "  Sample size: 49169 rows (8.3%)\n",
      "\n",
      "[Iteration 20] Training logistic_regression\n",
      "  Sample size: 49169 rows (8.3%)\n",
      "  Feature sampling: 60.4%\n",
      "  Transformers: None  Feature sampling: 60.4%\n",
      "  Transformers: None\n",
      "\n",
      "\n",
      "[Iteration 21] Training random_forest\n",
      "\n",
      "[Iteration 21] Training random_forest\n",
      "  Sample size: 30683 rows (4.0%)\n",
      "  Feature sampling: 47.6%\n",
      "  Transformers: quantile_transform  Sample size: 30683 rows (4.0%)\n",
      "  Feature sampling: 47.6%\n",
      "  Transformers: quantile_transform\n",
      "\n",
      "\n",
      "[Iteration 22] Training naive_bayes\n",
      "  Sample size: 51444 rows (7.3%)\n",
      "[Iteration 22] Training naive_bayes\n",
      "  Sample size: 51444 rows (7.3%)\n",
      "  Feature sampling: 62.0%\n",
      "  Transformers: None\n",
      "  Feature sampling: 62.0%\n",
      "  Transformers: None\n",
      "\n",
      "\n",
      "[Iteration 23] Training qda\n",
      "  Sample size: 12037 rows (2.0%)\n",
      "  Feature sampling: 34.7%\n",
      "  Transformers: binning, difference\n",
      "\n",
      "[Iteration 23] Training qda\n",
      "  Sample size: 12037 rows (2.0%)\n",
      "  Feature sampling: 34.7%\n",
      "  Transformers: binning, difference\n",
      "  [2/20] ‚úì Iteration 16: logistic_regression AUC=0.645329 (1.9s)\n",
      "  [2/20] ‚úì Iteration 16: logistic_regression AUC=0.645329 (1.9s)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 8 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 9 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 8 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 9 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iteration 24] Training extra_trees\n",
      "\n",
      "  Sample size: 42079 rows (6.4%)\n",
      "  Feature sampling: 55.5%\n",
      "  Sample size: 42079 rows (6.4%)\n",
      "  Feature sampling: 55.5%\n",
      "  Transformers: None\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 25] Training lda\n",
      "  Sample size: 21501 rows (1.7%)\n",
      "[Iteration 25] Training lda\n",
      "  Sample size: 21501 rows (1.7%)\n",
      "  Feature sampling: 41.3%\n",
      "\n",
      "  Feature sampling: 41.3%\n",
      "  Transformers: quantile_transform, kde\n",
      "  Transformers: quantile_transform, kde\n",
      "  [3/20] ‚úì Iteration 17: linear_svc AUC=0.551956 (2.5s)\n",
      "  [4/20] ‚úì Iteration 20: logistic_regression AUC=0.642634 (1.9s)\n",
      "  [3/20] ‚úì Iteration 17: linear_svc AUC=0.551956 (2.5s)\n",
      "  [4/20] ‚úì Iteration 20: logistic_regression AUC=0.642634 (1.9s)\n",
      "\n",
      "[Iteration 26] Training linear_svc\n",
      "  Sample size: 20563 rows (12.1%)\n",
      "[Iteration 26] Training linear_svc\n",
      "  Sample size: 20563 rows (12.1%)\n",
      "  Feature sampling: 40.6%\n",
      "\n",
      "  Feature sampling: 40.6%\n",
      "  Transformers: kmeans, quantile_transform, standard_scaler  Transformers: kmeans, quantile_transform, standard_scaler\n",
      "\n",
      "\n",
      "[Iteration 27] Training sgd_classifier\n",
      "\n",
      "[Iteration 27] Training sgd_classifier\n",
      "  Sample size: 9005 rows (10.8%)  Sample size: 9005 rows (10.8%)\n",
      "  Feature sampling: 32.6%\n",
      "  Feature sampling: 32.6%\n",
      "\n",
      "  Transformers: quantile_transform, binning, kde\n",
      "  Transformers: quantile_transform, binning, kde\n",
      "\n",
      "[Iteration 28] Training adaboost\n",
      "\n",
      "[Iteration 28] Training adaboost\n",
      "  Sample size: 9584 rows (12.9%)\n",
      "  Feature sampling: 33.0%  Sample size: 9584 rows (12.9%)\n",
      "  Feature sampling: 33.0%\n",
      "  Transformers: kmeans\n",
      "\n",
      "  Transformers: kmeans\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 8 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 8 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [5/20] ‚úì Iteration 18: linear_svc AUC=0.592933 (3.6s)\n",
      "  [6/20] ‚úì Iteration 23: qda AUC=0.565642 (2.0s)\n",
      "\n",
      "[Iteration 29] Training adaboost\n",
      "[Iteration 29] Training adaboost\n",
      "  Sample size: 23376 rows (13.2%)\n",
      "\n",
      "  Sample size: 23376 rows (13.2%)\n",
      "  Feature sampling: 42.6%\n",
      "  Transformers: kmeans, log\n",
      "  Feature sampling: 42.6%\n",
      "  Transformers: kmeans, log\n",
      "\n",
      "[Iteration 30] Training extra_trees\n",
      "  Sample size: 26671 rows (1.6%)\n",
      "\n",
      "[Iteration 30] Training extra_trees\n",
      "  Sample size: 26671 rows (1.6%)\n",
      "  Feature sampling: 44.8%\n",
      "  Transformers: square, difference  Feature sampling: 44.8%\n",
      "  Transformers: square, difference\n",
      "\n",
      "\n",
      "[Iteration 31] Training linear_svc\n",
      "  Sample size: 39531 rows (1.5%)\n",
      "[Iteration 31] Training linear_svc\n",
      "  Sample size: 39531 rows (1.5%)\n",
      "  Feature sampling: 53.7%\n",
      "  Transformers: None\n",
      "  Feature sampling: 53.7%\n",
      "  Transformers: None\n",
      "\n",
      "\n",
      "[Iteration 32] Training sgd_classifier\n",
      "\n",
      "[Iteration 32] Training sgd_classifier\n",
      "  Sample size: 20638 rows (12.8%)\n",
      "  Feature sampling: 40.7%  Sample size: 20638 rows (12.8%)\n",
      "  Feature sampling: 40.7%\n",
      "  Transformers: None\n",
      "  Transformers: None\n",
      "\n",
      "  [7/20] ‚úì Iteration 22: naive_bayes AUC=0.579584 (3.4s)\n",
      "  [7/20] ‚úì Iteration 22: naive_bayes AUC=0.579584 (3.4s)\n",
      "  [8/20] ‚úó Iteration 31 FAILED: n_components(44) must be <= n_features(15).\n",
      "  [8/20] ‚úó Iteration 31 FAILED: n_components(44) must be <= n_features(15).\n",
      "  [9/20] ‚úì Iteration 32: sgd_classifier AUC=0.565817 (1.5s)\n",
      "  [9/20] ‚úì Iteration 32: sgd_classifier AUC=0.565817 (1.5s)\n",
      "  [10/20] ‚úì Iteration 28: adaboost AUC=0.554758 (13.1s)\n",
      "  [10/20] ‚úì Iteration 28: adaboost AUC=0.554758 (13.1s)\n",
      "  [11/20] ‚úó Iteration 14 TIMEOUT: ridge exceeded 30 minutes\n",
      "  [11/20] ‚úó Iteration 14 TIMEOUT: ridge exceeded 30 minutes\n",
      "  [12/20] ‚úó Iteration 15 TIMEOUT: logistic exceeded 30 minutes\n",
      "  [12/20] ‚úó Iteration 15 TIMEOUT: logistic exceeded 30 minutes\n",
      "  [13/20] ‚úó Iteration 19 TIMEOUT: linear_svc exceeded 30 minutes\n",
      "  [13/20] ‚úó Iteration 19 TIMEOUT: linear_svc exceeded 30 minutes\n",
      "  [14/20] ‚úó Iteration 21 TIMEOUT: sgd_classifier exceeded 30 minutes\n",
      "  [14/20] ‚úó Iteration 21 TIMEOUT: sgd_classifier exceeded 30 minutes\n",
      "  [15/20] ‚úó Iteration 24 TIMEOUT: naive_bayes exceeded 30 minutes\n",
      "  [15/20] ‚úó Iteration 24 TIMEOUT: naive_bayes exceeded 30 minutes\n",
      "  [16/20] ‚úó Iteration 25 TIMEOUT: extra_trees exceeded 30 minutes\n",
      "  [16/20] ‚úó Iteration 25 TIMEOUT: extra_trees exceeded 30 minutes\n",
      "  [17/20] ‚úó Iteration 26 TIMEOUT: sgd_classifier exceeded 30 minutes\n",
      "  [17/20] ‚úó Iteration 26 TIMEOUT: sgd_classifier exceeded 30 minutes\n",
      "  [18/20] ‚úó Iteration 27 TIMEOUT: sgd_classifier exceeded 30 minutes\n",
      "  [18/20] ‚úó Iteration 27 TIMEOUT: sgd_classifier exceeded 30 minutes\n",
      "  [19/20] ‚úó Iteration 29 TIMEOUT: adaboost exceeded 30 minutes\n",
      "  [19/20] ‚úó Iteration 29 TIMEOUT: adaboost exceeded 30 minutes\n",
      "  [20/20] ‚úó Iteration 30 TIMEOUT: qda exceeded 30 minutes\n",
      "\n",
      "‚ö†Ô∏è  11/20 models failed during training\n",
      "  [20/20] ‚úó Iteration 30 TIMEOUT: qda exceeded 30 minutes\n",
      "\n",
      "‚ö†Ô∏è  11/20 models failed during training\n",
      "\n",
      "Batch complete (1806.9s, 200.8s per model)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 13: adaboost | Stage 1 AUC: 0.621146\n",
      "\n",
      "Batch complete (1806.9s, 200.8s per model)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 13: adaboost | Stage 1 AUC: 0.621146\n",
      "  Ensemble AUC (simple mean (all)): 0.651821 | Diversity: 0.308976\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000436, P=0.952917)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 16: logistic_regression | Stage 1 AUC: 0.645329\n",
      "  Ensemble AUC (simple mean (all)): 0.651821 | Diversity: 0.308976\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000436, P=0.952917)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 16: logistic_regression | Stage 1 AUC: 0.645329\n",
      "  Ensemble AUC (simple mean (all)): 0.652453 | Diversity: 0.358255\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.000196)\n",
      "\n",
      "================================================================================\n",
      "BATCH COMPLETE: Training stage 2 DNN on 10 models\n",
      "================================================================================\n",
      "  Ensemble AUC (simple mean (all)): 0.652453 | Diversity: 0.358255\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.000196)\n",
      "\n",
      "================================================================================\n",
      "BATCH COMPLETE: Training stage 2 DNN on 10 models\n",
      "================================================================================\n",
      "\n",
      "  Building initial stage 2 DNN...\n",
      "\n",
      "  Training stage 2 DNN...\n",
      "    Training samples: 40,000\n",
      "    Validation samples: 10,000\n",
      "\n",
      "  Building initial stage 2 DNN...\n",
      "\n",
      "  Training stage 2 DNN...\n",
      "    Training samples: 40,000\n",
      "    Validation samples: 10,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-12-07 19:10:36.194375: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Stage 2 DNN trained!\n",
      "  DNN ensemble AUC: 0.651464\n",
      "  Memory used: 35.3 MB\n",
      "  Time elapsed: 34.4s\n",
      "  Bundle checkpoint saved: ensemble_bundle_iter_16.joblib (0.1 MB)\n",
      "================================================================================\n",
      "\n",
      "  üéâ New best score: 0.652453 (Œî=0.000196)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 17: linear_svc | Stage 1 AUC: 0.551956\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó1)): 0.651298 | Diversity: 0.332162\n",
      "  Decision: ‚úó REJECT (rejected: Œî=-0.001155, P=0.882367)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 18: linear_svc | Stage 1 AUC: 0.592933\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó1)): 0.651298 | Diversity: 0.332162\n",
      "  Decision: ‚úó REJECT (rejected: Œî=-0.001155, P=0.882367)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 18: linear_svc | Stage 1 AUC: 0.592933\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó1)): 0.650961 | Diversity: 0.363477\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.001492, P=0.849999)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 20: logistic_regression | Stage 1 AUC: 0.642634\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó1)): 0.650961 | Diversity: 0.363477\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.001492, P=0.849999)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 20: logistic_regression | Stage 1 AUC: 0.642634\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó2)): 0.651566 | Diversity: 0.402266\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000887, P=0.907443)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 22: naive_bayes | Stage 1 AUC: 0.579584\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó2)): 0.651566 | Diversity: 0.402266\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000887, P=0.907443)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 22: naive_bayes | Stage 1 AUC: 0.579584\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó3)): 0.651889 | Diversity: 0.398215\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000564, P=0.939542)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 23: qda | Stage 1 AUC: 0.565642\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó3)): 0.651889 | Diversity: 0.398215\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000564, P=0.939542)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 23: qda | Stage 1 AUC: 0.565642\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó4)): 0.651840 | Diversity: 0.385887\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000613, P=0.933817)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 28: adaboost | Stage 1 AUC: 0.554758\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó4)): 0.651840 | Diversity: 0.385887\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000613, P=0.933817)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 28: adaboost | Stage 1 AUC: 0.554758\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó5)): 0.651743 | Diversity: 0.370896\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000710, P=0.923423)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 32: sgd_classifier | Stage 1 AUC: 0.565817\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó5)): 0.651743 | Diversity: 0.370896\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000710, P=0.923423)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 32: sgd_classifier | Stage 1 AUC: 0.565817\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó6)): 0.650368 | Diversity: 0.365946\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.002085, P=0.786674)\n",
      "\n",
      "================================================================================\n",
      "BATCH Starting at iteration 22\n",
      "================================================================================\n",
      "Ensemble size: 16 | Best score: 0.652453 | Temperature: 0.008518 | No improvement: 7/100\n",
      "\n",
      "Preparing batch jobs...\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó6)): 0.650368 | Diversity: 0.365946\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.002085, P=0.786674)\n",
      "\n",
      "================================================================================\n",
      "BATCH Starting at iteration 22\n",
      "================================================================================\n",
      "Ensemble size: 16 | Best score: 0.652453 | Temperature: 0.008518 | No improvement: 7/100\n",
      "\n",
      "Preparing batch jobs...\n",
      "Batch prepared: 20 jobs ready\n",
      "\n",
      "Training 20 candidates in parallel (30 min timeout per model)...\n",
      "Batch prepared: 20 jobs ready\n",
      "\n",
      "Training 20 candidates in parallel (30 min timeout per model)...\n",
      "\n",
      "[Iteration 22] Training naive_bayes\n",
      "  Sample size: 51444 rows (7.3%)\n",
      "  Feature sampling: 62.0%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 22] Training naive_bayes\n",
      "  Sample size: 51444 rows (7.3%)\n",
      "  Feature sampling: 62.0%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 23] Training qda\n",
      "  Sample size: 12037 rows (2.0%)\n",
      "  Feature sampling: 34.7%\n",
      "  Transformers: binning, difference\n",
      "\n",
      "[Iteration 23] Training qda\n",
      "  Sample size: 12037 rows (2.0%)\n",
      "  Feature sampling: 34.7%\n",
      "  Transformers: binning, difference\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/discriminant_analysis.py:947: UserWarning: Variables are collinear\n",
      "  warnings.warn(\"Variables are collinear\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iteration 24] Training extra_trees\n",
      "  Sample size: 42079 rows (6.4%)\n",
      "  Feature sampling: 55.5%\n",
      "  Sample size: 42079 rows (6.4%)\n",
      "  Feature sampling: 55.5%\n",
      "  Transformers: None\n",
      "\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 25] Training lda\n",
      "\n",
      "[Iteration 25] Training lda\n",
      "  Sample size: 21501 rows (1.7%)\n",
      "  Sample size: 21501 rows (1.7%)\n",
      "  Feature sampling: 41.3%\n",
      "  Transformers: quantile_transform, kde  Feature sampling: 41.3%\n",
      "  Transformers: quantile_transform, kde\n",
      "\n",
      "\n",
      "[Iteration 26] Training linear_svc\n",
      "[Iteration 26] Training linear_svc\n",
      "  Sample size: 20563 rows (12.1%)\n",
      "  Feature sampling: 40.6%\n",
      "  Transformers: kmeans, quantile_transform, standard_scaler\n",
      "\n",
      "  Sample size: 20563 rows (12.1%)\n",
      "  Feature sampling: 40.6%\n",
      "  Transformers: kmeans, quantile_transform, standard_scaler\n",
      "\n",
      "[Iteration 27] Training sgd_classifier\n",
      "  Sample size: 9005 rows (10.8%)\n",
      "  Feature sampling: 32.6%\n",
      "  Transformers: quantile_transform, binning, kde\n",
      "\n",
      "[Iteration 27] Training sgd_classifier\n",
      "  Sample size: 9005 rows (10.8%)\n",
      "  Feature sampling: 32.6%\n",
      "  Transformers: quantile_transform, binning, kde\n",
      "  [1/20] ‚úì Iteration 23: qda AUC=0.554115 (1.1s)\n",
      "  [1/20] ‚úì Iteration 23: qda AUC=0.554115 (1.1s)\n",
      "\n",
      "[Iteration 28] Training adaboost\n",
      "\n",
      "[Iteration 28] Training adaboost\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Sample size: 9584 rows (12.9%)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Feature sampling: 33.0%"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 7 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Transformers: kmeans  Transformers: kmeans"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 8 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "[Iteration 29] Training adaboost\n",
      "  Sample size: 23376 rows (13.2%)\n",
      "\n",
      "[Iteration 29] Training adaboost\n",
      "  Sample size: 23376 rows (13.2%)\n",
      "  Feature sampling: 42.6%\n",
      "  Transformers: kmeans, log\n",
      "  Feature sampling: 42.6%\n",
      "  Transformers: kmeans, log\n",
      "\n",
      "[Iteration 30] Training extra_trees\n",
      "  Sample size: 26671 rows (1.6%)\n",
      "  Feature sampling: 44.8%\n",
      "  Transformers: square, difference\n",
      "\n",
      "[Iteration 30] Training extra_trees\n",
      "  Sample size: 26671 rows (1.6%)\n",
      "  Feature sampling: 44.8%\n",
      "  Transformers: square, difference\n",
      "\n",
      "[Iteration 31] Training linear_svc\n",
      "  Sample size: 39531 rows (1.5%)\n",
      "\n",
      "[Iteration 31] Training linear_svc\n",
      "  Sample size: 39531 rows (1.5%)\n",
      "  Feature sampling: 53.7%\n",
      "  Transformers: None\n",
      "  Feature sampling: 53.7%\n",
      "  Transformers: None\n",
      "  [2/20] ‚úì Iteration 22: naive_bayes AUC=0.588946 (2.0s)\n",
      "  [2/20] ‚úì Iteration 22: naive_bayes AUC=0.588946 (2.0s)\n",
      "\n",
      "[Iteration 32] Training sgd_classifier\n",
      "  Sample size: 20638 rows (12.8%)\n",
      "  Feature sampling: 40.7%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 32] Training sgd_classifier\n",
      "  Sample size: 20638 rows (12.8%)\n",
      "  Feature sampling: 40.7%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 33] Training logistic_regression\n",
      "  Sample size: 21007 rows (2.2%)\n",
      "  Feature sampling: 40.9%\n",
      "  Transformers: kmeans, binning, square\n",
      "\n",
      "[Iteration 33] Training logistic_regression\n",
      "  Sample size: 21007 rows (2.2%)\n",
      "  Feature sampling: 40.9%\n",
      "  Transformers: kmeans, binning, square\n",
      "  [3/20] ‚úó Iteration 31 FAILED: n_components(44) must be <= n_features(13).\n",
      "  [3/20] ‚úó Iteration 31 FAILED: n_components(44) must be <= n_features(13).\n",
      "\n",
      "[Iteration 34] Training logistic_regression\n",
      "  Sample size: 17611 rows (12.5%)\n",
      "  Feature sampling: 38.6%\n",
      "  Transformers: kde, kmeans, binning\n",
      "[Iteration 34] Training logistic_regression\n",
      "  Sample size: 17611 rows (12.5%)\n",
      "  Feature sampling: 38.6%\n",
      "  Transformers: kde, kmeans, binning\n",
      "\n",
      "\n",
      "[Iteration 35] Training linear_svc\n",
      "[Iteration 35] Training linear_svc\n",
      "  Sample size: 30205 rows (10.5%)\n",
      "\n",
      "  Sample size: 30205 rows (10.5%)\n",
      "  Feature sampling: 47.3%  Feature sampling: 47.3%\n",
      "  Transformers: rbf_sampler\n",
      "  Transformers: rbf_sampler\n",
      "\n",
      "\n",
      "[Iteration 36] Training logistic_regression\n",
      "  Sample size: 5900 rows (6.4%)\n",
      "  Feature sampling: 30.5%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 36] Training logistic_regression\n",
      "  Sample size: 5900 rows (6.4%)\n",
      "  Feature sampling: 30.5%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 37] Training random_forest\n",
      "  [4/20] ‚úì Iteration 32: sgd_classifier AUC=0.532896 (1.0s)\n",
      "\n",
      "[Iteration 37] Training random_forest\n",
      "  [4/20] ‚úì Iteration 32: sgd_classifier AUC=0.532896 (1.0s)\n",
      "  Sample size: 55661 rows (8.4%)\n",
      "  Feature sampling: 64.9%  Sample size: 55661 rows (8.4%)\n",
      "  Feature sampling: 64.9%\n",
      "  Transformers: ratio\n",
      "\n",
      "  Transformers: ratio\n",
      "\n",
      "[Iteration 38] Training naive_bayes\n",
      "\n",
      "[Iteration 38] Training naive_bayes\n",
      "  Sample size: 30439 rows (3.2%)\n",
      "  Feature sampling: 47.4%  Sample size: 30439 rows (3.2%)\n",
      "  Feature sampling: 47.4%\n",
      "  Transformers: None\n",
      "\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 39] Training linear_svc\n",
      "\n",
      "[Iteration 39] Training linear_svc\n",
      "  Sample size: 47492 rows (10.2%)  Sample size: 47492 rows (10.2%)\n",
      "  Feature sampling: 59.3%\n",
      "\n",
      "  Feature sampling: 59.3%\n",
      "  Transformers: product, binning\n",
      "  Transformers: product, binning\n",
      "\n",
      "[Iteration 40] Training sgd_classifier\n",
      "[Iteration 40] Training sgd_classifier\n",
      "  Sample size: 18603 rows (10.4%)\n",
      "\n",
      "  Sample size: 18603 rows (10.4%)\n",
      "  Feature sampling: 39.2%\n",
      "  Transformers: reciprocal, nystroem  Feature sampling: 39.2%\n",
      "  Transformers: reciprocal, nystroem\n",
      "\n",
      "  [5/20] ‚úì Iteration 36: logistic_regression AUC=0.609618 (1.4s)\n",
      "  [5/20] ‚úì Iteration 36: logistic_regression AUC=0.609618 (1.4s)\n",
      "\n",
      "[Iteration 41] Training lda\n",
      "  Sample size: 33372 rows (8.1%)\n",
      "[Iteration 41] Training lda\n",
      "  Sample size: 33372 rows (8.1%)\n",
      "  Feature sampling: 49.5%\n",
      "  Feature sampling: 49.5%\n",
      "  Transformers: None\n",
      "\n",
      "  Transformers: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 2 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 4 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 4 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 7 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 8 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 9 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 7 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 8 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 9 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 10 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 11 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 10 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 11 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 12 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 13 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 12 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 13 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 14 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 14 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 15 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 15 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  [6/20] ‚úì Iteration 38: naive_bayes AUC=0.602484 (2.2s)\n",
      "  [7/20] ‚úì Iteration 41: lda AUC=0.594546 (1.9s)\n",
      "  [7/20] ‚úì Iteration 41: lda AUC=0.594546 (1.9s)\n",
      "  [8/20] ‚úì Iteration 35: linear_svc AUC=0.557449 (5.5s)\n",
      "  [8/20] ‚úì Iteration 35: linear_svc AUC=0.557449 (5.5s)\n",
      "  [9/20] ‚úì Iteration 39: linear_svc AUC=0.520385 (5.1s)\n",
      "  [9/20] ‚úì Iteration 39: linear_svc AUC=0.520385 (5.1s)\n",
      "  [10/20] ‚úó Iteration 28 FAILED: Process terminated without result (iteration 28)\n",
      "  [10/20] ‚úó Iteration 28 FAILED: Process terminated without result (iteration 28)\n",
      "  [11/20] ‚úó Iteration 29 FAILED: Process terminated without result (iteration 29)\n",
      "  [11/20] ‚úó Iteration 29 FAILED: Process terminated without result (iteration 29)\n",
      "  [12/20] ‚úó Iteration 33 FAILED: Process terminated without result (iteration 33)\n",
      "  [12/20] ‚úó Iteration 33 FAILED: Process terminated without result (iteration 33)\n",
      "  [13/20] ‚úó Iteration 26 FAILED: Process terminated without result (iteration 26)\n",
      "  [13/20] ‚úó Iteration 26 FAILED: Process terminated without result (iteration 26)\n",
      "  [14/20] ‚úó Iteration 34 FAILED: Process terminated without result (iteration 34)\n",
      "  [14/20] ‚úó Iteration 34 FAILED: Process terminated without result (iteration 34)\n",
      "  [15/20] ‚úó Iteration 24 TIMEOUT: naive_bayes exceeded 30 minutes\n",
      "  [15/20] ‚úó Iteration 24 TIMEOUT: naive_bayes exceeded 30 minutes\n",
      "  [16/20] ‚úó Iteration 25 TIMEOUT: extra_trees exceeded 30 minutes\n",
      "  [16/20] ‚úó Iteration 25 TIMEOUT: extra_trees exceeded 30 minutes\n",
      "  [17/20] ‚úó Iteration 27 TIMEOUT: sgd_classifier exceeded 30 minutes\n",
      "  [17/20] ‚úó Iteration 27 TIMEOUT: sgd_classifier exceeded 30 minutes\n",
      "  [18/20] ‚úó Iteration 30 TIMEOUT: qda exceeded 30 minutes\n",
      "  [18/20] ‚úó Iteration 30 TIMEOUT: qda exceeded 30 minutes\n",
      "  [19/20] ‚úó Iteration 37 TIMEOUT: linear_svc exceeded 30 minutes\n",
      "  [19/20] ‚úó Iteration 37 TIMEOUT: linear_svc exceeded 30 minutes\n",
      "  [20/20] ‚úó Iteration 40 TIMEOUT: linear_svc exceeded 30 minutes\n",
      "\n",
      "‚ö†Ô∏è  12/20 models failed during training\n",
      "  [20/20] ‚úó Iteration 40 TIMEOUT: linear_svc exceeded 30 minutes\n",
      "\n",
      "‚ö†Ô∏è  12/20 models failed during training\n",
      "\n",
      "Batch complete (1806.1s, 225.8s per model)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 22: naive_bayes | Stage 1 AUC: 0.588946\n",
      "\n",
      "Batch complete (1806.1s, 225.8s per model)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 22: naive_bayes | Stage 1 AUC: 0.588946\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó7)): 0.650705 | Diversity: 0.369466\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.001748, P=0.814478)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 23: qda | Stage 1 AUC: 0.554115\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó7)): 0.650705 | Diversity: 0.369466\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.001748, P=0.814478)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 23: qda | Stage 1 AUC: 0.554115\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó8)): 0.650967 | Diversity: 0.348129\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.001487, P=0.847055)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 32: sgd_classifier | Stage 1 AUC: 0.532896\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó8)): 0.650967 | Diversity: 0.348129\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.001487, P=0.847055)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 32: sgd_classifier | Stage 1 AUC: 0.532896\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó9)): 0.650539 | Diversity: 0.324797\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.001915, P=0.806635)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 35: linear_svc | Stage 1 AUC: 0.557449\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó9)): 0.650539 | Diversity: 0.324797\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.001915, P=0.806635)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 35: linear_svc | Stage 1 AUC: 0.557449\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó10)): 0.650007 | Diversity: 0.322058\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.002446, P=0.750402)\n",
      "\n",
      "================================================================================\n",
      "BATCH COMPLETE: Training stage 2 DNN on 20 models\n",
      "================================================================================\n",
      "  Ensemble AUC (hybrid (DNN√ó10 + mean√ó10)): 0.650007 | Diversity: 0.322058\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.002446, P=0.750402)\n",
      "\n",
      "================================================================================\n",
      "BATCH COMPLETE: Training stage 2 DNN on 20 models\n",
      "================================================================================\n",
      "\n",
      "  Transfer learning: expanding DNN from 10 to 20 inputs...\n",
      "\n",
      "  Training stage 2 DNN...\n",
      "    Training samples: 40,000\n",
      "    Validation samples: 10,000\n",
      "\n",
      "  Transfer learning: expanding DNN from 10 to 20 inputs...\n",
      "\n",
      "  Training stage 2 DNN...\n",
      "    Training samples: 40,000\n",
      "    Validation samples: 10,000\n",
      "\n",
      "  Stage 2 DNN trained!\n",
      "  DNN ensemble AUC: 0.652442\n",
      "  Memory used: 5.6 MB\n",
      "  Time elapsed: 83.6s\n",
      "\n",
      "  Stage 2 DNN trained!\n",
      "  DNN ensemble AUC: 0.652442\n",
      "  Memory used: 5.6 MB\n",
      "  Time elapsed: 83.6s\n",
      "  Bundle checkpoint saved: ensemble_bundle_iter_35.joblib (0.1 MB)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 36: logistic_regression | Stage 1 AUC: 0.609618\n",
      "  Bundle checkpoint saved: ensemble_bundle_iter_35.joblib (0.1 MB)\n",
      "================================================================================\n",
      "\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 36: logistic_regression | Stage 1 AUC: 0.609618\n",
      "  Ensemble AUC (hybrid (DNN√ó20 + mean√ó1)): 0.652638 | Diversity: 0.328977\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.000185)\n",
      "  üéâ New best score: 0.652638 (Œî=0.000185)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 38: naive_bayes | Stage 1 AUC: 0.602484\n",
      "  Ensemble AUC (hybrid (DNN√ó20 + mean√ó1)): 0.652638 | Diversity: 0.328977\n",
      "  Decision: ‚úì ACCEPT (improvement: Œî=0.000185)\n",
      "  üéâ New best score: 0.652638 (Œî=0.000185)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 38: naive_bayes | Stage 1 AUC: 0.602484\n",
      "  Ensemble AUC (hybrid (DNN√ó20 + mean√ó2)): 0.652270 | Diversity: 0.338101\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000368, P=0.956845)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 39: linear_svc | Stage 1 AUC: 0.520385\n",
      "  Ensemble AUC (hybrid (DNN√ó20 + mean√ó2)): 0.652270 | Diversity: 0.338101\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000368, P=0.956845)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 39: linear_svc | Stage 1 AUC: 0.520385\n",
      "  Ensemble AUC (hybrid (DNN√ó20 + mean√ó3)): 0.652309 | Diversity: 0.320086\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000329, P=0.961020)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 41: lda | Stage 1 AUC: 0.594546\n",
      "  Ensemble AUC (hybrid (DNN√ó20 + mean√ó3)): 0.652309 | Diversity: 0.320086\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000329, P=0.961020)\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "Iteration 41: lda | Stage 1 AUC: 0.594546\n",
      "  Ensemble AUC (hybrid (DNN√ó20 + mean√ó4)): 0.652194 | Diversity: 0.329975\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000444, P=0.947392)\n",
      "\n",
      "================================================================================\n",
      "BATCH Starting at iteration 30\n",
      "================================================================================\n",
      "Ensemble size: 24 | Best score: 0.652638 | Temperature: 0.008142 | No improvement: 3/100\n",
      "\n",
      "Preparing batch jobs...\n",
      "  Ensemble AUC (hybrid (DNN√ó20 + mean√ó4)): 0.652194 | Diversity: 0.329975\n",
      "  Decision: ‚úì ACCEPT (simulated_annealing: Œî=-0.000444, P=0.947392)\n",
      "\n",
      "================================================================================\n",
      "BATCH Starting at iteration 30\n",
      "================================================================================\n",
      "Ensemble size: 24 | Best score: 0.652638 | Temperature: 0.008142 | No improvement: 3/100\n",
      "\n",
      "Preparing batch jobs...\n",
      "Batch prepared: 20 jobs ready\n",
      "\n",
      "Training 20 candidates in parallel (30 min timeout per model)...\n",
      "Batch prepared: 20 jobs ready\n",
      "\n",
      "Training 20 candidates in parallel (30 min timeout per model)...\n",
      "\n",
      "[Iteration 30] Training extra_trees\n",
      "  Sample size: 26671 rows (1.6%)\n",
      "  Feature sampling: 44.8%\n",
      "  Transformers: square, difference\n",
      "\n",
      "[Iteration 30] Training extra_trees\n",
      "  Sample size: 26671 rows (1.6%)\n",
      "  Feature sampling: 44.8%\n",
      "  Transformers: square, difference\n",
      "\n",
      "[Iteration 31] Training linear_svc\n",
      "  Sample size: 39531 rows (1.5%)\n",
      "[Iteration 31] Training linear_svc\n",
      "  Sample size: 39531 rows (1.5%)\n",
      "  Feature sampling: 53.7%\n",
      "  Transformers: None\n",
      "\n",
      "  Feature sampling: 53.7%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 32] Training sgd_classifier\n",
      "  Sample size: 20638 rows (12.8%)\n",
      "  Feature sampling: 40.7%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 32] Training sgd_classifier\n",
      "  Sample size: 20638 rows (12.8%)\n",
      "  Feature sampling: 40.7%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 33] Training logistic_regression\n",
      "  Sample size: 21007 rows (2.2%)\n",
      "  Feature sampling: 40.9%\n",
      "  Transformers: kmeans, binning, square\n",
      "\n",
      "[Iteration 33] Training logistic_regression\n",
      "  Sample size: 21007 rows (2.2%)\n",
      "  Feature sampling: 40.9%\n",
      "  Transformers: kmeans, binning, square\n",
      "  [1/20] ‚úó Iteration 31 FAILED: n_components(44) must be <= n_features(15).\n",
      "  [1/20] ‚úó Iteration 31 FAILED: n_components(44) must be <= n_features(15).\n",
      "\n",
      "[Iteration 34] Training logistic_regression\n",
      "  Sample size: 17611 rows (12.5%)\n",
      "  Feature sampling: 38.6%\n",
      "  Transformers: kde, kmeans, binning\n",
      "\n",
      "[Iteration 34] Training logistic_regression\n",
      "  Sample size: 17611 rows (12.5%)\n",
      "  Feature sampling: 38.6%\n",
      "  Transformers: kde, kmeans, binning\n",
      "\n",
      "[Iteration 35] Training linear_svc\n",
      "  Sample size: 30205 rows (10.5%)\n",
      "\n",
      "[Iteration 35] Training linear_svc\n",
      "  Sample size: 30205 rows (10.5%)\n",
      "  Feature sampling: 47.3%\n",
      "  Transformers: rbf_sampler\n",
      "  Feature sampling: 47.3%\n",
      "  Transformers: rbf_sampler\n",
      "\n",
      "[Iteration 36] Training logistic_regression\n",
      "  Sample size: 5900 rows (6.4%)\n",
      "  Feature sampling: 30.5%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 36] Training logistic_regression\n",
      "  Sample size: 5900 rows (6.4%)\n",
      "  Feature sampling: 30.5%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 37] Training random_forest\n",
      "\n",
      "[Iteration 37] Training random_forest\n",
      "  Sample size: 55661 rows (8.4%)\n",
      "  Sample size: 55661 rows (8.4%)\n",
      "  Feature sampling: 64.9%  Feature sampling: 64.9%\n",
      "  Transformers: ratio\n",
      "\n",
      "  Transformers: ratio\n",
      "\n",
      "[Iteration 38] Training naive_bayes\n",
      "  Sample size: 30439 rows (3.2%)\n",
      "[Iteration 38] Training naive_bayes\n",
      "  Sample size: 30439 rows (3.2%)\n",
      "  Feature sampling: 47.4%\n",
      "  Feature sampling: 47.4%\n",
      "  Transformers: None\n",
      "  Transformers: None\n",
      "\n",
      "  [2/20] ‚úì Iteration 32: sgd_classifier AUC=0.559274 (1.0s)\n",
      "  [2/20] ‚úì Iteration 32: sgd_classifier AUC=0.559274 (1.0s)\n",
      "\n",
      "[Iteration 39] Training linear_svc\n",
      "[Iteration 39] Training linear_svc\n",
      "  Sample size: 47492 rows (10.2%)\n",
      "  Sample size: 47492 rows (10.2%)\n",
      "  Feature sampling: 59.3%\n",
      "\n",
      "  Feature sampling: 59.3%\n",
      "  Transformers: product, binning\n",
      "  Transformers: product, binning\n",
      "  [3/20] ‚úó Iteration 33 FAILED: Process terminated without result (iteration 33)\n",
      "  [4/20] ‚úì Iteration 36: logistic_regression AUC=0.577520 (1.1s)\n",
      "  [3/20] ‚úó Iteration 33 FAILED: Process terminated without result (iteration 33)\n",
      "  [4/20] ‚úì Iteration 36: logistic_regression AUC=0.577520 (1.1s)\n",
      "\n",
      "[Iteration 40] Training sgd_classifier\n",
      "[Iteration 40] Training sgd_classifier\n",
      "  Sample size: 18603 rows (10.4%)\n",
      "\n",
      "  Sample size: 18603 rows (10.4%)\n",
      "  Feature sampling: 39.2%\n",
      "  Feature sampling: 39.2%\n",
      "\n",
      "  Transformers: reciprocal, nystroem\n",
      "  Transformers: reciprocal, nystroem\n",
      "[Iteration 41] Training lda\n",
      "  Sample size: 33372 rows (8.1%)\n",
      "[Iteration 41] Training lda\n",
      "  Sample size: 33372 rows (8.1%)\n",
      "  Feature sampling: 49.5%\n",
      "\n",
      "  Feature sampling: 49.5%\n",
      "  Transformers: None\n",
      "  Transformers: None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 0 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 2 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 1 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:263: UserWarning: Feature 2 is constant and will be replaced with 0.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 3 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 5 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 6 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 10 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 10 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 12 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 13 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 15 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 12 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 13 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n",
      "/mnt/arkk/kaggle/diabetes-prediction/.venv/lib/python3.12/site-packages/sklearn/preprocessing/_discretization.py:307: UserWarning: Bins whose width are too small (i.e., <= 1e-8) in feature 15 are removed. Consider decreasing the number of bins.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[Iteration 42] Training adaboost\n",
      "  Sample size: 47018 rows (9.9%)\n",
      "  Feature sampling: 58.9%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 43] Training extra_trees  [5/20] ‚úì Iteration 38: naive_bayes AUC=0.595087 (1.8s)\n",
      "\n",
      "  Sample size: 46461 rows (13.0%)\n",
      "  Feature sampling: 58.5%\n",
      "  Transformers: None\n",
      "\n",
      "[Iteration 44] Training lda\n",
      "  Sample size: 56009 rows (4.8%)\n",
      "  Feature sampling: 65.2%\n",
      "  Transformers: sqrt, noise_injector, rbf_sampler\n",
      "\n",
      "[Iteration 45] Training sgd_classifier\n",
      "  Sample size: 33996 rows (4.0%)\n",
      "  Feature sampling: 49.9%\n",
      "  Transformers: power_transform\n",
      "\n",
      "[Iteration 46] Training extra_trees\n",
      "  Sample size: 11613 rows (9.2%)\n",
      "  Feature sampling: 34.4%\n",
      "  Transformers: product\n",
      "  [6/20] ‚úì Iteration 41: lda AUC=0.623516 (2.9s)\n",
      "\n",
      "[Iteration 47] Training sgd_classifier\n",
      "  Sample size: 6171 rows (13.3%)\n",
      "  Feature sampling: 30.6%\n",
      "  Transformers: kmeans, sum, noise_injector\n",
      "\n",
      "[Iteration 48] Training logistic_regression\n",
      "  Sample size: 11337 rows (11.3%)\n",
      "  Feature sampling: 34.2%\n",
      "  Transformers: kde, standard_scaler, log\n",
      "\n",
      "[Iteration 49] Training lda  [7/20] ‚úì Iteration 35: linear_svc AUC=0.602907 (5.6s)\n",
      "\n",
      "  Sample size: 29807 rows (6.6%)\n",
      "  Feature sampling: 47.0%\n",
      "  Transformers: None\n",
      "  [8/20] ‚úì Iteration 46: extra_trees AUC=0.530231 (2.1s)\n",
      "  [9/20] ‚úó Iteration 47 FAILED: Process terminated without result (iteration 47)\n",
      "  [10/20] ‚úì Iteration 39: linear_svc AUC=0.588090 (6.4s)\n",
      "  [11/20] ‚úì Iteration 49: lda AUC=0.578660 (2.2s)\n",
      "  [12/20] ‚úì Iteration 44: lda AUC=0.579750 (4.6s)\n",
      "  [13/20] ‚úì Iteration 42: adaboost AUC=0.643737 (6.3s)\n",
      "  [14/20] ‚úì Iteration 45: sgd_classifier AUC=0.510519 (6.2s)\n",
      "  [15/20] ‚úó Iteration 34 FAILED: Process terminated without result (iteration 34)\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"STARTING PARALLEL HILL CLIMBING LOOP\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Batch size: {BATCH_SIZE} candidates trained in parallel\")\n",
    "print(f\"Workers: {N_WORKERS} parallel processes\")\n",
    "print(f\"Total CPUs: {N_CPUS} (distributed intelligently across models)\")\n",
    "\n",
    "iterations_since_improvement = 0\n",
    "iteration = start_iteration\n",
    "\n",
    "# Calculate timeout values in seconds\n",
    "model_timeout_seconds = MODEL_TIMEOUT_MINUTES * 60\n",
    "batch_timeout_seconds = model_timeout_seconds + 60  # Add 1 minute buffer for batch timeout\n",
    "\n",
    "while iteration < MAX_ITERATIONS and iterations_since_improvement < PLATEAU_ITERATIONS:\n",
    "    print(f\"\\n{'=' * 80}\")\n",
    "    print(f\"BATCH Starting at iteration {iteration}\")\n",
    "    print(f\"{'=' * 80}\")\n",
    "    print(f\"Ensemble size: {len(ensemble_models)} | Best score: {best_ensemble_score:.6f} | \"\n",
    "          f\"Temperature: {temperature:.6f} | No improvement: {iterations_since_improvement}/{PLATEAU_ITERATIONS}\")\n",
    "    \n",
    "    # Prepare batch of training jobs with intelligent CPU allocation\n",
    "    print(f\"\\nPreparing batch jobs...\")\n",
    "    batch_jobs = prepare_training_batch(\n",
    "        iteration, BATCH_SIZE, MAX_ITERATIONS, X_train_pool, y_train_pool,\n",
    "        X_val_s1, y_val_s1, base_preprocessor, RANDOM_STATE, total_cpus=N_CPUS,\n",
    "        timeout_minutes=MODEL_TIMEOUT_MINUTES\n",
    "    )\n",
    "    print(f\"Batch prepared: {len(batch_jobs)} jobs ready\")\n",
    "    \n",
    "    # Pre-determine classifier types for this batch (for timeout logging)\n",
    "    batch_classifier_types = {}\n",
    "    for job in batch_jobs:\n",
    "        job_iteration = job[0]\n",
    "        rng = np.random.RandomState(RANDOM_STATE + job_iteration)\n",
    "        classifier_pool = [\n",
    "            'logistic', 'random_forest', 'linear_svc',\n",
    "            'sgd_classifier', 'extra_trees', 'adaboost',\n",
    "            'naive_bayes', 'lda', 'qda', 'ridge'\n",
    "            # TEMPORARILY DISABLED (too slow):\n",
    "            # 'gradient_boosting', 'mlp', 'knn'\n",
    "        ]\n",
    "        batch_classifier_types[job_iteration] = rng.choice(classifier_pool)\n",
    "    \n",
    "    print(f\"\\nTraining {len(batch_jobs)} candidates in parallel ({MODEL_TIMEOUT_MINUTES} min timeout per model)...\")\n",
    "    batch_start_time = time.time()\n",
    "    \n",
    "    # Train candidates in parallel with timeout\n",
    "    trained_candidates = []\n",
    "    failed_count = 0\n",
    "    timeout_iterations = []  # Track which iterations timed out\n",
    "    with ProcessPoolExecutor(max_workers=N_WORKERS) as executor:\n",
    "        futures = {executor.submit(train_single_candidate, job): job for job in batch_jobs}\n",
    "        \n",
    "        completed = 0\n",
    "        for future in as_completed(futures, timeout=batch_timeout_seconds):\n",
    "            completed += 1\n",
    "            job = futures[future]\n",
    "            try:\n",
    "                # Individual job timeout\n",
    "                result = future.result(timeout=model_timeout_seconds)\n",
    "                trained_candidates.append(result)\n",
    "                print(f\"  [{completed}/{len(batch_jobs)}] ‚úì Iteration {result['iteration']}: \"\n",
    "                      f\"{result['metadata']['classifier_type']} AUC={result['val_auc_s1']:.6f} \"\n",
    "                      f\"({result['training_time']:.1f}s)\")\n",
    "            except TimeoutError:\n",
    "                failed_count += 1\n",
    "                timeout_iterations.append(job[0])  # Store iteration number that timed out\n",
    "                timeout_classifier = batch_classifier_types[job[0]]\n",
    "                print(f\"  [{completed}/{len(batch_jobs)}] ‚úó Iteration {job[0]} TIMEOUT: {timeout_classifier} exceeded {MODEL_TIMEOUT_MINUTES} minutes\")\n",
    "            except Exception as e:\n",
    "                failed_count += 1\n",
    "                print(f\"  [{completed}/{len(batch_jobs)}] ‚úó Iteration {job[0]} FAILED: {e}\")\n",
    "    \n",
    "    if failed_count > 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  {failed_count}/{len(batch_jobs)} models failed during training\")\n",
    "    \n",
    "    # Log timeout iterations to database\n",
    "    for timeout_iter in timeout_iterations:\n",
    "        timeout_classifier = batch_classifier_types[timeout_iter]\n",
    "        log_iteration(\n",
    "            iteration=timeout_iter,\n",
    "            accepted=False,\n",
    "            rejection_reason=\"timeout\",\n",
    "            pipeline_hash=\"timeout\",\n",
    "            stage1_val_auc=0.0,\n",
    "            stage2_val_auc=0.0,\n",
    "            ensemble_size=len(ensemble_models),\n",
    "            diversity_score=0.0,\n",
    "            temperature=temperature,\n",
    "            metadata={'classifier_type': timeout_classifier, 'transformers_used': []},\n",
    "            ensemble_id=f\"iter_{timeout_iter}\",\n",
    "            training_memory_mb=None,\n",
    "            stage2_memory_mb=None,\n",
    "            training_time_sec=None,\n",
    "            stage2_time_sec=None,\n",
    "            timeout=True\n",
    "        )\n",
    "    \n",
    "    if len(trained_candidates) == 0:\n",
    "        print(f\"\\n‚ö†Ô∏è  WARNING: All models in batch failed! Continuing to next batch...\")\n",
    "        iteration += len(batch_jobs)  # Move past failed iterations\n",
    "        continue  # Continue to next batch instead of breaking\n",
    "    \n",
    "    batch_time = time.time() - batch_start_time\n",
    "    print(f\"\\nBatch complete ({batch_time:.1f}s, {batch_time/len(trained_candidates):.1f}s per model)\")\n",
    "    \n",
    "    # Sort by iteration number\n",
    "    trained_candidates.sort(key=lambda x: x['iteration'])\n",
    "    \n",
    "    # Process each trained candidate for acceptance/rejection\n",
    "    for result in trained_candidates:\n",
    "        current_iter = result['iteration']\n",
    "        fitted_pipeline = result['fitted_pipeline']\n",
    "        metadata = result['metadata']\n",
    "        val_auc_s1 = result['val_auc_s1']\n",
    "        pipeline_hash = result['pipeline_hash']\n",
    "        training_memory_mb = result.get('memory_mb', None)\n",
    "        training_time_sec = result.get('training_time_sec', None)\n",
    "        \n",
    "        print(f\"\\n{'-' * 80}\")\n",
    "        print(f\"Iteration {current_iter}: {metadata['classifier_type']} | Stage 1 AUC: {val_auc_s1:.6f}\")\n",
    "        \n",
    "        # Evaluate ensemble with candidate\n",
    "        if len(ensemble_models) == 0:\n",
    "            # First model - just use its score\n",
    "            candidate_score = val_auc_s1\n",
    "            diversity_score = 0.0\n",
    "            aggregation_method = \"single_model\"\n",
    "            print(f\"  Ensemble AUC ({aggregation_method}): {candidate_score:.6f} (first model)\")\n",
    "        else:\n",
    "            # Evaluate as ensemble\n",
    "            candidate_ensemble = ensemble_models + [fitted_pipeline]\n",
    "            candidate_score, diversity_score, aggregation_method = evaluate_candidate_ensemble(\n",
    "                candidate_ensemble, ensemble_models, stage2_model,\n",
    "                X_val_s1, X_val_s2, y_val_s1, y_val_s2\n",
    "            )\n",
    "            print(f\"  Ensemble AUC ({aggregation_method}): {candidate_score:.6f} | Diversity: {diversity_score:.6f}\")\n",
    "        \n",
    "        # Simulated annealing acceptance (with diversity bonus)\n",
    "        accept, reason = adaptive_simulated_annealing_acceptance(\n",
    "            current_score=best_ensemble_score,\n",
    "            candidate_score=candidate_score,\n",
    "            temperature=temperature,\n",
    "            random_state=RANDOM_STATE + current_iter,\n",
    "            diversity_score=diversity_score\n",
    "        )\n",
    "        \n",
    "        print(f\"  Decision: {'‚úì ACCEPT' if accept else '‚úó REJECT'} ({reason})\")\n",
    "        \n",
    "        # Track stage 2 memory and time for this iteration\n",
    "        stage2_memory_mb = None\n",
    "        stage2_time_sec = None\n",
    "        \n",
    "        # Log iteration\n",
    "        log_iteration(\n",
    "            iteration=current_iter,\n",
    "            accepted=accept,\n",
    "            rejection_reason=reason,\n",
    "            pipeline_hash=pipeline_hash,\n",
    "            stage1_val_auc=val_auc_s1,\n",
    "            stage2_val_auc=candidate_score,\n",
    "            ensemble_size=len(ensemble_models) + 1 if accept else len(ensemble_models),\n",
    "            diversity_score=diversity_score,\n",
    "            temperature=temperature,\n",
    "            metadata=metadata,\n",
    "            ensemble_id=f\"iter_{current_iter}\",\n",
    "            training_memory_mb=training_memory_mb,\n",
    "            stage2_memory_mb=stage2_memory_mb,\n",
    "            training_time_sec=training_time_sec,\n",
    "            stage2_time_sec=stage2_time_sec,\n",
    "            timeout=False\n",
    "        )\n",
    "        \n",
    "        # Update ensemble if accepted\n",
    "        if accept:\n",
    "            ensemble_models.append(fitted_pipeline)\n",
    "            \n",
    "            # Save model\n",
    "            model_path = ENSEMBLE_DIR / f'model_{current_iter}.joblib'\n",
    "            joblib.dump(fitted_pipeline, model_path)\n",
    "            \n",
    "            # Check if we should train/retrain stage 2 DNN\n",
    "            if len(ensemble_models) % STAGE2_BATCH_SIZE_MODELS == 0 and len(ensemble_models) > 0:\n",
    "                stage2_model, final_score, stage2_memory_mb, stage2_time_sec = train_or_expand_stage2_model(\n",
    "                    ensemble_models, stage2_model, X_val_s1, y_val_s1, X_val_s2, y_val_s2,\n",
    "                    STAGE2_EPOCHS, STAGE2_BATCH_SIZE, STAGE2_PATIENCE, current_iter\n",
    "                )\n",
    "                \n",
    "                # Save ensemble bundle checkpoint\n",
    "                save_ensemble_bundle(\n",
    "                    ensemble_models, stage2_model, best_ensemble_score, current_iter,\n",
    "                    MODELS_DIR, RANDOM_STATE, BATCH_SIZE, N_WORKERS, base_preprocessor,\n",
    "                    numerical_features, ordinal_features, nominal_features,\n",
    "                    education_categories, income_categories\n",
    "                )\n",
    "                print(f\"{'=' * 80}\\n\")\n",
    "            \n",
    "            # Check if this is the best score\n",
    "            if candidate_score > best_ensemble_score:\n",
    "                print(f\"  üéâ New best score: {candidate_score:.6f} (Œî={candidate_score - best_ensemble_score:.6f})\")\n",
    "                best_ensemble_score = candidate_score\n",
    "                iterations_since_improvement = 0\n",
    "            else:\n",
    "                iterations_since_improvement += 1\n",
    "        else:\n",
    "            iterations_since_improvement += 1\n",
    "        \n",
    "        # Update temperature\n",
    "        temperature = update_temperature(\n",
    "            iteration=current_iter,\n",
    "            acceptance_history=[accept],\n",
    "            current_temperature=temperature,\n",
    "            base_temperature=BASE_TEMPERATURE,\n",
    "            decay_rate=TEMPERATURE_DECAY\n",
    "        )\n",
    "    \n",
    "    # Move to next batch\n",
    "    iteration += len(trained_candidates)\n",
    "    \n",
    "    # Check termination\n",
    "    if iterations_since_improvement >= PLATEAU_ITERATIONS:\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"TERMINATING: No improvement for {PLATEAU_ITERATIONS} iterations\")\n",
    "        print(f\"{'=' * 80}\")\n",
    "        break\n",
    "\n",
    "# Calculate final acceptance rate and timeout rate\n",
    "conn = ensemble_database.sqlite3.connect(ensemble_database.DB_PATH)\n",
    "acceptance_stats = conn.execute(\"SELECT COUNT(*) as total, SUM(accepted) as accepted FROM ensemble_log WHERE iteration_num > 0\").fetchone()\n",
    "timeout_stats = conn.execute(\"SELECT SUM(timeout) as timeouts FROM ensemble_log WHERE iteration_num > 0\").fetchone()\n",
    "conn.close()\n",
    "acceptance_rate = acceptance_stats[1] / acceptance_stats[0] if acceptance_stats[0] > 0 else 0.0\n",
    "timeout_rate = timeout_stats[0] / acceptance_stats[0] if acceptance_stats[0] > 0 else 0.0\n",
    "\n",
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"HILL CLIMBING COMPLETE\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"Final ensemble size: {len(ensemble_models)}\")\n",
    "print(f\"Best ensemble AUC: {best_ensemble_score:.6f}\")\n",
    "print(f\"Total iterations: {iteration - 1}\")\n",
    "print(f\"Acceptance rate: {acceptance_rate:.1%}\")\n",
    "print(f\"Timeout rate: {timeout_rate:.1%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed5e4cad",
   "metadata": {},
   "source": [
    "## Save final checkpoint and bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f505a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final checkpoint\n",
    "save_checkpoint(\n",
    "    checkpoint_path=CHECKPOINT_PATH,\n",
    "    ensemble_models=ensemble_models,\n",
    "    stage2_model=stage2_model,\n",
    "    iteration=iteration - 1,\n",
    "    temperature=temperature,\n",
    "    best_score=best_ensemble_score,\n",
    "    acceptance_history=[],\n",
    "    metadata={\n",
    "        'total_iterations': iteration,\n",
    "        'final_ensemble_size': len(ensemble_models),\n",
    "        'acceptance_rate': acceptance_rate,\n",
    "        'best_score': best_ensemble_score,\n",
    "        'parallel_batch_size': BATCH_SIZE,\n",
    "        'n_workers': N_WORKERS\n",
    "    }\n",
    ")\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = MODELS_DIR / 'ensemble_metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump({\n",
    "        'ensemble_size': len(ensemble_models),\n",
    "        'total_iterations': iteration,\n",
    "        'best_score': best_ensemble_score,\n",
    "        'acceptance_rate': acceptance_rate,\n",
    "        'training_completed': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'parallel_batch_size': BATCH_SIZE,\n",
    "        'n_workers': N_WORKERS\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(f\"\\nCheckpoint saved: {CHECKPOINT_PATH}\")\n",
    "print(f\"Metadata saved: {metadata_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3673a7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save final ensemble bundle for Kaggle\n",
    "ensemble_bundle_path = MODELS_DIR / 'ensemble_bundle.joblib'\n",
    "\n",
    "ensemble_bundle = {\n",
    "    'ensemble_models': ensemble_models,\n",
    "    'stage2_model': stage2_model,\n",
    "    'metadata': {\n",
    "        'ensemble_size': len(ensemble_models),\n",
    "        'total_iterations': iteration,\n",
    "        'best_score': best_ensemble_score,\n",
    "        'acceptance_rate': acceptance_rate,\n",
    "        'training_completed': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'random_state': RANDOM_STATE,\n",
    "        'parallel_batch_size': BATCH_SIZE,\n",
    "        'n_workers': N_WORKERS\n",
    "    },\n",
    "    'base_preprocessor': base_preprocessor,\n",
    "    'feature_info': {\n",
    "        'numerical_features': numerical_features,\n",
    "        'ordinal_features': ordinal_features,\n",
    "        'nominal_features': nominal_features,\n",
    "        'education_categories': education_categories,\n",
    "        'income_categories': income_categories\n",
    "    }\n",
    "}\n",
    "\n",
    "joblib.dump(ensemble_bundle, ensemble_bundle_path, compress=3)\n",
    "\n",
    "print(f\"\\nFinal ensemble bundle saved: {ensemble_bundle_path}\")\n",
    "print(f\"File size: {ensemble_bundle_path.stat().st_size / (1024**2):.1f} MB\")\n",
    "print(f\"\\nTo load on Kaggle:\")\n",
    "print(f\"  ensemble_bundle = joblib.load('ensemble_bundle.joblib')\")\n",
    "print(f\"  ensemble_models = ensemble_bundle['ensemble_models']\")\n",
    "print(f\"  stage2_model = ensemble_bundle['stage2_model']\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f709353d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the wrapper class\n",
    "sys.path.insert(0, str(MODELS_BASE_DIR))\n",
    "\n",
    "# Create wrapped model\n",
    "wrapped_model = EnsembleClassifier(\n",
    "    ensemble_models=ensemble_models,\n",
    "    stage2_model=stage2_model,\n",
    "    aggregation='mean'  # Fallback if stage2_model is None\n",
    ")\n",
    "\n",
    "# Save as single joblib file\n",
    "wrapped_model_path = MODELS_DIR / 'ensemble_model.joblib'\n",
    "joblib.dump(wrapped_model, wrapped_model_path, compress=3)\n",
    "\n",
    "print(f\"\\nWrapped ensemble model saved: {wrapped_model_path}\")\n",
    "print(f\"File size: {wrapped_model_path.stat().st_size / (1024**2):.1f} MB\")\n",
    "print(f\"\\nModel info: {wrapped_model}\")\n",
    "print(f\"\\nTo use on Kaggle:\")\n",
    "print(f\"  1. Upload to Kaggle dataset:\")\n",
    "print(f\"     - {wrapped_model_path.name}\")\n",
    "print(f\"     - {MODELS_BASE_DIR / 'ensemble_classifier.py'}\")\n",
    "print(f\"  2. In inference notebook:\")\n",
    "print(f\"     from ensemble_classifier import EnsembleClassifier\")\n",
    "print(f\"     model = joblib.load('ensemble_model.joblib')\")\n",
    "print(f\"     predictions = model.predict(test_df)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cf7f59f",
   "metadata": {},
   "source": [
    "## Create wrapped ensemble model for Kaggle\n",
    "\n",
    "Create a sklearn-compatible wrapper that bundles the entire ensemble into a single classifier.\n",
    "This makes inference identical to the logistic regression workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab336e9",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3b0fe78",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'=' * 80}\")\n",
    "print(\"ENSEMBLE TRAINING SUMMARY\")\n",
    "print(f\"{'=' * 80}\")\n",
    "print(f\"\\nFinal Statistics:\")\n",
    "print(f\"  Ensemble size: {len(ensemble_models)}\")\n",
    "print(f\"  Best validation AUC: {best_ensemble_score:.6f}\")\n",
    "print(f\"  Total iterations: {iteration}\")\n",
    "print(f\"  Acceptance rate: {acceptance_rate:.1%}\")\n",
    "print(f\"  Parallel configuration: {BATCH_SIZE} candidates, {N_WORKERS} workers\")\n",
    "print(f\"\\nFiles created:\")\n",
    "print(f\"  Database: {ensemble_database.DB_PATH}\")\n",
    "print(f\"  Models: {ENSEMBLE_DIR}\")\n",
    "print(f\"  Checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"  Metadata: {metadata_path}\")\n",
    "print(f\"  Bundle: {ensemble_bundle_path}\")\n",
    "print(f\"\\n{'=' * 80}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
