{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45695246",
   "metadata": {},
   "source": [
    "# Gradient boosting model\n",
    "\n",
    "Current leaderboard score: 0.69361\n",
    "\n",
    "One, two, you know what to do...\n",
    "\n",
    "## Notebook set up\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb5c3844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard library imports\n",
    "import copy\n",
    "import os\n",
    "import sys\n",
    "from itertools import combinations, permutations\n",
    "from pathlib import Path\n",
    "\n",
    "# Set environment variable to limit thread usage by underlying linear algebra libraries.\n",
    "# Note: this must be set before importing numpy, pandas, or sklearn.\n",
    "os.environ['OMP_NUM_THREADS'] = '2'\n",
    "\n",
    "# Third party imports\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from scipy.stats import uniform, randint, loguniform\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.feature_selection import SelectPercentile\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_val_predict, RandomizedSearchCV\n",
    "from sklearn.metrics import make_scorer, roc_auc_score, ConfusionMatrixDisplay\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from sklearn.preprocessing import OrdinalEncoder, OneHotEncoder, MinMaxScaler, KBinsDiscretizer, PolynomialFeatures\n",
    "\n",
    "# Add the models directory to path so we can import custom transformers\n",
    "sys.path.insert(0, '../models')\n",
    "\n",
    "from gradient_boosting_transformers import (\n",
    "    IDColumnDropper, IQRClipper, DifferenceFeatures, SumFeatures,\n",
    "    RatioFeatures, ReciprocalFeatures, LogFeatures, SquareRootFeatures,\n",
    "    KMeansClusterFeatures\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4e1f00",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17412612",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_FEATURE_ENGINEERING = False\n",
    "RUN_HYPERPARAMETER_OPTIMIZATION = True\n",
    "\n",
    "# Sample sizes, use 1 for full dataset\n",
    "OPTIMIZATION_SAMPLE = 0.50\n",
    "EVALUATION_SAMPLE = 0.50\n",
    "FINAL_TRAINING_SAMPLE = 1.0\n",
    "\n",
    "# Hyperparameter search and evaluation parameters\n",
    "CV_FOLDS = 3\n",
    "N_JOBS = 3\n",
    "HYPERPARAMETER_SEARCH_ITERATIONS = 500"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0bf85c9",
   "metadata": {},
   "source": [
    "### Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed701ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df_path = 'https://gperdrizet.github.io/FSA_devops/assets/data/unit3/diabetes_prediction_train.csv'\n",
    "\n",
    "# Load the training dataset\n",
    "df = pd.read_csv(train_df_path)\n",
    "\n",
    "# Split test set for internal evaluation\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=315)\n",
    "train_df.reset_index(inplace=True, drop=True)\n",
    "test_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "# Display first few rows of training data\n",
    "train_df.head().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff30c8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display dataset information (columns, dtypes, non-null counts)\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa085fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21867ae9",
   "metadata": {},
   "source": [
    "### Column definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a379457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the label\n",
    "label = 'diagnosed_diabetes'\n",
    "\n",
    "# Define numerical features to apply IQR clipping\n",
    "numerical_features = [\n",
    "    'age', 'physical_activity_minutes_per_week', 'diet_score',\n",
    "    'sleep_hours_per_day', 'screen_time_hours_per_day', 'bmi',\n",
    "    'waist_to_hip_ratio', 'systolic_bp', 'diastolic_bp', 'heart_rate',\n",
    "    'cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides',\n",
    "]\n",
    "\n",
    "# Define features for one-hot encoding\n",
    "nominal_features = [\n",
    "    'gender', 'ethnicity', 'smoking_status', 'employment_status'\n",
    "]\n",
    "\n",
    "# Define ordinal features to encode\n",
    "ordinal_features = [\n",
    "    'education_level', 'income_level', 'alcohol_consumption_per_week',\n",
    "    'family_history_diabetes', 'hypertension_history', 'cardiovascular_history'\n",
    "]\n",
    "\n",
    "# Define ordinal categories in order\n",
    "education_categories = ['No formal', 'Highschool', 'Graduate', 'Postgraduate']\n",
    "income_categories = ['Low', 'Lower-Middle', 'Middle', 'Upper-Middle', 'High']\n",
    "alcohol_categories = [1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "diabetes_history_categories = [0, 1]\n",
    "hypertension_history_categories = [0, 1]\n",
    "cardiovascular_history_categories = [0, 1]\n",
    "\n",
    "ordinal_categories = [\n",
    "    education_categories,\n",
    "    income_categories,\n",
    "    alcohol_categories,\n",
    "    diabetes_history_categories,\n",
    "    hypertension_history_categories,\n",
    "    cardiovascular_history_categories\n",
    "]\n",
    "\n",
    "# Define features for KMeans clustering\n",
    "heart_features = ['systolic_bp', 'diastolic_bp', 'heart_rate', 'hypertension_history', 'cardiovascular_history']\n",
    "cholesterol_features = ['cholesterol_total', 'hdl_cholesterol', 'ldl_cholesterol', 'triglycerides']\n",
    "lifestyle_features = [\n",
    "    'physical_activity_minutes_per_week', 'diet_score', 'sleep_hours_per_day',\n",
    "    'screen_time_hours_per_day', 'alcohol_consumption_per_week'\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3963201",
   "metadata": {},
   "source": [
    "## 1. Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b00d402",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    # Drop ID column\n",
    "    train_df.drop(columns=['id'], inplace=True)\n",
    "    test_df.drop(columns=['id'], inplace=True)\n",
    "\n",
    "    print(f'ID column removed')\n",
    "    print(f'Remaining columns: {list(train_df.columns)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56138fb0",
   "metadata": {},
   "source": [
    "### 2.1. Ordinal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5df527d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    # Create ordinal encoder with categories\n",
    "    ordinal_encoder = OrdinalEncoder(\n",
    "        categories=ordinal_categories,\n",
    "        handle_unknown='use_encoded_value',\n",
    "        unknown_value=-1\n",
    "    )\n",
    "\n",
    "    # Fit and transform ordinal features\n",
    "    ordinal_encoded = ordinal_encoder.fit_transform(train_df[ordinal_features])\n",
    "    train_df.drop(columns=ordinal_features, inplace=True)\n",
    "    train_df[ordinal_features] = ordinal_encoded\n",
    "\n",
    "    # And the test data\n",
    "    ordinal_encoded = ordinal_encoder.transform(test_df[ordinal_features])\n",
    "    test_df.drop(columns=ordinal_features, inplace=True)\n",
    "    test_df[ordinal_features] = ordinal_encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "105c16c9",
   "metadata": {},
   "source": [
    "### 2.2. Nominal encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae985fcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    # Create one-hot encoder\n",
    "    onehot_encoder = OneHotEncoder(\n",
    "        drop='first',\n",
    "        sparse_output=False,\n",
    "        handle_unknown='ignore'\n",
    "    )\n",
    "\n",
    "    # Convert encoded features to DataFrame\n",
    "    encoded_features_df = pd.DataFrame(\n",
    "        onehot_encoder.fit_transform(train_df[nominal_features]),\n",
    "        columns=onehot_encoder.get_feature_names_out(nominal_features)\n",
    "    )\n",
    "\n",
    "    # Remove original nominal features and add encoded versions\n",
    "    train_df = pd.concat([train_df.drop(columns=nominal_features), encoded_features_df], axis=1)\n",
    "\n",
    "    # And the test data\n",
    "    encoded_features_df = pd.DataFrame(\n",
    "        onehot_encoder.transform(test_df[nominal_features]),\n",
    "        columns=onehot_encoder.get_feature_names_out(nominal_features)\n",
    "    )\n",
    "\n",
    "    test_df = pd.concat([test_df.drop(columns=nominal_features), encoded_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0572141e",
   "metadata": {},
   "source": [
    "## 3. Feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da12a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    features = train_df.drop(columns=[label]).columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7993c51b",
   "metadata": {},
   "source": [
    "### 3.1. Feature discretization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5953e243",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    # Create a KbinsDiscretizer\n",
    "    binning_transformer = KBinsDiscretizer(n_bins=5, encode='ordinal')\n",
    "\n",
    "    # Bin the numerical features\n",
    "    binned_features = binning_transformer.fit_transform(train_df[numerical_features])\n",
    "\n",
    "    # Add new binned features to the training DataFrame\n",
    "    binned_features_df = pd.DataFrame(binned_features, columns=[f'binned_{feature}' for feature in numerical_features])\n",
    "    train_df = pd.concat([train_df, binned_features_df], axis=1)\n",
    "\n",
    "    # And the test data\n",
    "    binned_features = binning_transformer.transform(test_df[numerical_features])\n",
    "    binned_features_df = pd.DataFrame(binned_features, columns=[f'binned_{feature}' for feature in numerical_features])\n",
    "    test_df = pd.concat([test_df, binned_features_df], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6743d1f9",
   "metadata": {},
   "source": [
    "### 3.2. Clustering\n",
    "\n",
    "#### 3.2.1. Heart health clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd916f3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    # Fit KMean clustering model on the training data\n",
    "    kmeans_model = KMeans(n_clusters=4, random_state=315)\n",
    "    kmeans_model.fit(train_df[heart_features], train_df[label])\n",
    "\n",
    "    # Add cluster membership as a new feature\n",
    "    train_df['heart_cluster'] = kmeans_model.predict(train_df[heart_features])\n",
    "    test_df['heart_cluster'] = kmeans_model.predict(test_df[heart_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b440ec7",
   "metadata": {},
   "source": [
    "#### 3.2.2. Cholesterol clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ff0975",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    # Fit KMean clustering model on the training data\n",
    "    kmeans_model = KMeans(n_clusters=4, random_state=315)\n",
    "    kmeans_model.fit(train_df[cholesterol_features], train_df[label])\n",
    "\n",
    "    # Add cluster membership as a new feature\n",
    "    train_df['cholesterol_cluster'] = kmeans_model.predict(train_df[cholesterol_features])\n",
    "    test_df['cholesterol_cluster'] = kmeans_model.predict(test_df[cholesterol_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f44f40",
   "metadata": {},
   "source": [
    "#### 3.2.3. Lifestyle clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fd9033",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    # Fit KMean clustering model on the training data\n",
    "    kmeans_model = KMeans(n_clusters=4, random_state=315)\n",
    "    kmeans_model.fit(train_df[lifestyle_features], train_df[label])\n",
    "\n",
    "    # Add cluster membership as a new feature\n",
    "    train_df['lifestyle_cluster'] = kmeans_model.predict(train_df[lifestyle_features])\n",
    "    test_df['lifestyle_cluster'] = kmeans_model.predict(test_df[lifestyle_features])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d4d7b61",
   "metadata": {},
   "source": [
    "### 3.3. Polynomial features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b1fd35",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    features = train_df.drop(columns=[label]).columns.tolist()\n",
    "\n",
    "    poly_transformer = PolynomialFeatures(degree=2, include_bias=False)\n",
    "\n",
    "    # Fit and transform the training data\n",
    "    train_labels = train_df[label].values\n",
    "    poly_features_train = poly_transformer.fit_transform(train_df[features])\n",
    "    train_df = pd.DataFrame(poly_features_train, columns=poly_transformer.get_feature_names_out(features))\n",
    "    train_df[label] = train_labels\n",
    "\n",
    "    # And the test data\n",
    "    test_labels = test_df[label].values\n",
    "    poly_features_test = poly_transformer.transform(test_df[features])\n",
    "    test_df = pd.DataFrame(poly_features_test, columns=poly_transformer.get_feature_names_out(features))\n",
    "    test_df[label] = test_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20aa842a",
   "metadata": {},
   "source": [
    "### 3.4. Other synthetic features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a477351",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    # Dictionary to collect new features and add at the end\n",
    "    new_train_features = {}\n",
    "    new_test_features = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f335b8b3",
   "metadata": {},
   "source": [
    "#### 3.4.1. Difference features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4653da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    for feature_a, feature_b in permutations(features, 2):\n",
    "        feature_name = f'{feature_a}-{feature_b}'\n",
    "        new_train_features[feature_name] = train_df[feature_a] - train_df[feature_b]\n",
    "        new_test_features[feature_name] = test_df[feature_a] - test_df[feature_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11977a5d",
   "metadata": {},
   "source": [
    "#### 3.4.2. Sum features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1739c859",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    for feature_a, feature_b in combinations(features, 2):\n",
    "        feature_name = f'{feature_a}+{feature_b}'\n",
    "        new_train_features[feature_name] = train_df[feature_a] + train_df[feature_b]\n",
    "        new_test_features[feature_name] = test_df[feature_a] + test_df[feature_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536a7b51",
   "metadata": {},
   "source": [
    "#### 3.4.3. Ratio features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b558a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    for feature_a, feature_b in permutations(features, 2):\n",
    "        feature_name = f'{feature_a}/{feature_b}'\n",
    "        new_train_features[feature_name] = train_df[feature_a] / (train_df[feature_b] + train_df[feature_b].min() + 1)\n",
    "        new_test_features[feature_name] = test_df[feature_a] / (test_df[feature_b] + test_df[feature_b].min() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28c7a56c",
   "metadata": {},
   "source": [
    "#### 3.4.4. Reciprocal features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef30504",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    for feature in features:\n",
    "        feature_name = f'1/{feature}'\n",
    "        new_train_features[feature_name] = 1 / (train_df[feature] + train_df[feature].min() + 1)\n",
    "        new_test_features[feature_name] = 1 / (test_df[feature] + test_df[feature].min() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0269b7",
   "metadata": {},
   "source": [
    "#### 3.4.5. Log features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df47201f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    for feature in features:\n",
    "        feature_name = f'log{feature}'\n",
    "        new_train_features[feature_name] = np.log(train_df[feature] + train_df[feature].min() + 1)\n",
    "        new_test_features[feature_name] = np.log(test_df[feature] + test_df[feature].min() + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57630d0d",
   "metadata": {},
   "source": [
    "#### 3.4.6. Square root features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2de4f61",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    for feature in features:\n",
    "        feature_name = f'root{feature}'\n",
    "        new_train_features[feature_name] = (train_df[feature] + train_df[feature].min() + 1) ** (1/2)\n",
    "        new_test_features[feature_name] = (test_df[feature] + test_df[feature].min() + 1) ** (1/2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e630e7",
   "metadata": {},
   "source": [
    "#### 3.4.8. Add new features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5aa2bf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    train_df = pd.concat([train_df, pd.DataFrame(new_train_features)], axis=1)\n",
    "    test_df = pd.concat([test_df, pd.DataFrame(new_test_features)], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04098f20",
   "metadata": {},
   "source": [
    "### 3.5. Remove constant features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e3d822",
   "metadata": {},
   "outputs": [],
   "source": [
    "if TEST_FEATURE_ENGINEERING:\n",
    "\n",
    "    train_df = train_df.loc[:, train_df.nunique() > 1]\n",
    "    test_df = test_df.loc[:, test_df.columns.isin(train_df.columns)]\n",
    "    train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a989583",
   "metadata": {},
   "source": [
    "## 4. Model training and optimization\n",
    "\n",
    "### 4.1. Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4219a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Draw the first sample without replacement\n",
    "train_optimization_df = train_df.sample(\n",
    "    n=int(len(train_df) * OPTIMIZATION_SAMPLE),\n",
    "    replace=False,\n",
    "    random_state=315\n",
    ")\n",
    "\n",
    "# Get the indices of the remaining rows not in sample1\n",
    "remaining_indices = train_df.index.difference(train_optimization_df.index)\n",
    "\n",
    "# Draw the second sample from the *remaining* rows\n",
    "# We use .loc to select the specific rows from the original df using the remaining indices\n",
    "train_eval_df = train_df.loc[remaining_indices].sample(\n",
    "    n=int(len(train_df) * EVALUATION_SAMPLE),\n",
    "    replace=False,\n",
    "    random_state=315\n",
    ")\n",
    "\n",
    "print(f'Optimization sample shape: {train_optimization_df.shape}')\n",
    "print(f'Evaluation sample shape: {train_eval_df.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20660031",
   "metadata": {},
   "source": [
    "### 4.2. Build Complete Pipeline\n",
    "\n",
    "Create a scikit-learn pipeline that includes all preprocessing and feature engineering steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d203cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import VarianceThreshold, SelectPercentile, f_classif\n",
    "\n",
    "# Build the complete pipeline\n",
    "pipeline = Pipeline([\n",
    "\n",
    "    # Step 1: Drop ID column\n",
    "    ('drop_id', IDColumnDropper(id_column='id')),\n",
    "    \n",
    "    # Step 2: Preprocessing transformations\n",
    "    ('preprocessing', ColumnTransformer(\n",
    "        transformers=[\n",
    "\n",
    "            # Ordinal encoding\n",
    "            ('ordinal_encoder', OrdinalEncoder(\n",
    "                categories=ordinal_categories,\n",
    "                handle_unknown='use_encoded_value',\n",
    "                unknown_value=-1\n",
    "            ), ordinal_features),\n",
    "            \n",
    "            # One-hot encoding for nominal features\n",
    "            ('onehot_encoder', OneHotEncoder(\n",
    "                drop='first',\n",
    "                sparse_output=False,\n",
    "                handle_unknown='ignore'\n",
    "            ), nominal_features),\n",
    "            \n",
    "            # Pass through numerical features (so they're not consumed by clustering/binning)\n",
    "            ('numerical_passthrough', 'passthrough', numerical_features),\n",
    "            \n",
    "            # Binning (discretization) for numerical features\n",
    "            ('binning', KBinsDiscretizer(\n",
    "                n_bins=5,\n",
    "                encode='ordinal',\n",
    "                strategy='quantile'\n",
    "            ), numerical_features),\n",
    "            \n",
    "            # KMeans clustering for heart features\n",
    "            ('heart_cluster', KMeansClusterFeatures(\n",
    "                n_clusters=4,\n",
    "                random_state=315\n",
    "            ), heart_features),\n",
    "            \n",
    "            # KMeans clustering for cholesterol features\n",
    "            ('cholesterol_cluster', KMeansClusterFeatures(\n",
    "                n_clusters=4,\n",
    "                random_state=315\n",
    "            ), cholesterol_features),\n",
    "            \n",
    "            # KMeans clustering for lifestyle features\n",
    "            ('lifestyle_cluster', KMeansClusterFeatures(\n",
    "                n_clusters=4,\n",
    "                random_state=315\n",
    "            ), lifestyle_features),\n",
    "        ],\n",
    "        remainder='drop'  # Drop any remaining columns\n",
    "    )),\n",
    "    \n",
    "    # Step 3: Feature engineering - create synthetic features in parallel\n",
    "    ('feature_engineering', FeatureUnion([\n",
    "\n",
    "        # Keep original features\n",
    "        ('passthrough', 'passthrough'),\n",
    "        \n",
    "        # Polynomial features (degree 2)\n",
    "        ('polynomial', PolynomialFeatures(degree=2, include_bias=False)),\n",
    "        \n",
    "        # Difference features\n",
    "        ('difference', DifferenceFeatures()),\n",
    "        \n",
    "        # Sum features\n",
    "        ('sum', SumFeatures()),\n",
    "        \n",
    "        # Ratio features\n",
    "        ('ratio', RatioFeatures()),\n",
    "        \n",
    "        # Reciprocal features\n",
    "        ('reciprocal', ReciprocalFeatures()),\n",
    "        \n",
    "        # Log features\n",
    "        ('log', LogFeatures()),\n",
    "        \n",
    "        # Square root features\n",
    "        ('sqrt', SquareRootFeatures()),\n",
    "    ])),\n",
    "    \n",
    "    # Step 4: Feature selection - remove low variance and select top features\n",
    "    ('variance_threshold', VarianceThreshold(threshold=0.01)),\n",
    "    \n",
    "    ('select_percentile', SelectPercentile(\n",
    "        score_func=f_classif,\n",
    "        percentile=50\n",
    "    )),\n",
    "    \n",
    "    # Step 5: Classifier\n",
    "    ('classifier', HistGradientBoostingClassifier(random_state=315))\n",
    "])\n",
    "\n",
    "pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "093f8015",
   "metadata": {},
   "source": [
    "### 4.3. Baseline model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e856c13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create baseline model from pipeline\n",
    "baseline_model = copy.deepcopy(pipeline)\n",
    "\n",
    "# Estimate AUC with cross-validation\n",
    "baseline_scores = cross_val_score(\n",
    "    baseline_model,\n",
    "    train_eval_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_eval_df['diagnosed_diabetes'],\n",
    "    cv=CV_FOLDS,\n",
    "    scoring=make_scorer(roc_auc_score),\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "print(f'Baseline model mean cross-validation score (ROC-AUC): {np.mean(baseline_scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9a38db3",
   "metadata": {},
   "source": [
    "### 4.4. Hyperparameter optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d048c0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# Best mean cross-validation score (ROC-AUC): 0.7048\n",
    "#\n",
    "# N_JOBS = -1\n",
    "# OPTIMIZATION_SAMPLE = 0.10\n",
    "# CV_FOLDS = 3\n",
    "#\n",
    "# Best parameters:\n",
    "#   classifier__class_weight: None\n",
    "#   classifier__early_stopping: True\n",
    "#   classifier__l2_regularization: 0.16700213384089524\n",
    "#   classifier__learning_rate: 0.012671253422340085\n",
    "#   classifier__max_bins: 255\n",
    "#   classifier__max_depth: 19\n",
    "#   classifier__max_iter: 401\n",
    "#   classifier__min_samples_leaf: 28\n",
    "#   classifier__n_iter_no_change: 50\n",
    "#   classifier__validation_fraction: 0.1\n",
    "#   select_percentile__percentile: 61\n",
    "#   variance_threshold__threshold: 0.00013743694392305273\n",
    "#\n",
    "# CPU times: user 21min 50s, sys: 50.5 s, total: 22min 41s\n",
    "# Wall time: 6h 44s\n",
    "\n",
    "if RUN_HYPERPARAMETER_OPTIMIZATION:\n",
    "\n",
    "    # Define parameter distributions for randomized search\n",
    "    param_distributions = {\n",
    "\n",
    "        # Feature selection parameters\n",
    "        'variance_threshold__threshold': uniform(0, 0.1),\n",
    "        'select_percentile__percentile': randint(25, 76),\n",
    "        \n",
    "        # Classifier parameters\n",
    "        'classifier__learning_rate': loguniform(0.001, 0.3),\n",
    "        'classifier__max_iter': randint(100, 1001),\n",
    "        'classifier__max_depth': randint(10, 101),\n",
    "        'classifier__min_samples_leaf': randint(5, 51),\n",
    "        'classifier__l2_regularization': loguniform(1e-4, 100.0),\n",
    "        'classifier__max_bins': [255],\n",
    "        'classifier__early_stopping': [True],\n",
    "        'classifier__validation_fraction': [0.1],\n",
    "        'classifier__n_iter_no_change': [50],\n",
    "        'classifier__class_weight': ['balanced']\n",
    "    }\n",
    "\n",
    "    # Create RandomizedSearchCV\n",
    "    random_search = RandomizedSearchCV(\n",
    "        copy.deepcopy(pipeline),\n",
    "        param_distributions=param_distributions,\n",
    "        n_iter=HYPERPARAMETER_SEARCH_ITERATIONS,\n",
    "        cv=CV_FOLDS,\n",
    "        scoring='roc_auc',\n",
    "        n_jobs=N_JOBS,\n",
    "        random_state=315,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    # Fit the randomized search\n",
    "    random_search.fit(\n",
    "        train_optimization_df.drop(columns=['diagnosed_diabetes']),\n",
    "        train_optimization_df['diagnosed_diabetes']\n",
    "    )\n",
    "\n",
    "    # Get the best model from random search\n",
    "    optimized_model = random_search.best_estimator_\n",
    "\n",
    "    print(f'\\nBest mean cross-validation score (ROC-AUC): {random_search.best_score_:.4f}')\n",
    "    print(f'\\nBest parameters:')\n",
    "\n",
    "    for param, value in random_search.best_params_.items():\n",
    "        print(f'  {param}: {value}')\n",
    "\n",
    "    print()\n",
    "\n",
    "    # Define model directory and ensure it exists\n",
    "    model_dir = Path('../models')\n",
    "    model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Create model filename\n",
    "    model_name = 'gradient_boosting'\n",
    "    model_path = model_dir / f'{model_name}.joblib'\n",
    "\n",
    "    # Save the final model\n",
    "    joblib.dump(optimized_model, model_path)\n",
    "    print('Model saved to:', model_path)\n",
    "    print(f'File size: {model_path.stat().st_size / (1024**2):.2f} MB')\n",
    "    print()\n",
    "\n",
    "else:\n",
    "\n",
    "    # Load the optimized model from file\n",
    "    model_dir = Path('../models')\n",
    "    model_name = 'gradient_boosting'\n",
    "    model_path = model_dir / f'{model_name}.joblib'\n",
    "    optimized_model = joblib.load(model_path)\n",
    "    print('Optimized model loaded from:', model_path)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830e97e4",
   "metadata": {},
   "source": [
    "### 4.5. Evaluate optimized model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42fd8353",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate with cross-validation to verify performance\n",
    "scores = cross_val_score(\n",
    "    optimized_model,\n",
    "    train_eval_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_eval_df['diagnosed_diabetes'],\n",
    "    cv=CV_FOLDS,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "print(f'Optimized model mean cross-validation score (ROC-AUC): {np.mean(scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63a2d6f2",
   "metadata": {},
   "source": [
    "### 4.6. Compare baseline vs optimized performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc555b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for boxplot\n",
    "comparison_data = [baseline_scores, scores]\n",
    "labels = ['Baseline\\n(Unoptimized)', 'Optimized']\n",
    "\n",
    "# Create boxplot\n",
    "plt.title('Cross-Validation Performance')\n",
    "plt.boxplot(comparison_data, tick_labels=labels, patch_artist=True, widths=0.6)\n",
    "plt.ylabel('ROC-AUC Score')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(f'Baseline  - mean score (ROC-AUC): {np.mean(baseline_scores):.4f}, Std: {np.std(baseline_scores):.4f}')\n",
    "print(f'Optimized - mean score (ROC-AUC): {np.mean(scores):.4f}, Std: {np.std(scores):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5235f51",
   "metadata": {},
   "source": [
    "### 4.7. Confusion matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f00e081",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions for both models using cross-validation\n",
    "baseline_predictions = cross_val_predict(\n",
    "    baseline_model,\n",
    "    train_eval_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_eval_df['diagnosed_diabetes'],\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "optimized_predictions = cross_val_predict(\n",
    "    optimized_model,\n",
    "    train_eval_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_eval_df['diagnosed_diabetes'],\n",
    "    cv=CV_FOLDS,\n",
    "    n_jobs=N_JOBS\n",
    ")\n",
    "\n",
    "# Plot confusion matrices side by side\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "# Baseline confusion matrix\n",
    "axes[0].set_title('Unoptimized model')\n",
    "\n",
    "disp1 = ConfusionMatrixDisplay.from_predictions(\n",
    "    train_eval_df['diagnosed_diabetes'],\n",
    "    baseline_predictions,\n",
    "    normalize='true',\n",
    "    ax=axes[0],\n",
    "    colorbar=False\n",
    ")\n",
    "\n",
    "# Optimized confusion matrix\n",
    "axes[1].set_title('Optimized model')\n",
    "\n",
    "disp3 = ConfusionMatrixDisplay.from_predictions(\n",
    "    train_eval_df['diagnosed_diabetes'],\n",
    "    optimized_predictions,\n",
    "    normalize='true',\n",
    "    ax=axes[1],\n",
    "    colorbar=False\n",
    ")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2b0869",
   "metadata": {},
   "source": [
    "## 5. Final model preparation\n",
    "\n",
    "### 5.1. Train and evaluate using internal test-train split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe84101",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the optimized model on the training data\n",
    "optimized_model.fit(\n",
    "    train_df.drop(columns=['diagnosed_diabetes']),\n",
    "    train_df['diagnosed_diabetes']\n",
    ")\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_probabilities = optimized_model.predict_proba(\n",
    "    test_df.drop(columns=['diagnosed_diabetes'])\n",
    ")\n",
    "\n",
    "# Calculate ROC-AUC on the test data\n",
    "test_roc_auc = roc_auc_score(\n",
    "    test_df['diagnosed_diabetes'],\n",
    "    test_probabilities[:, 1]\n",
    ")\n",
    "\n",
    "# Make predictions on the test data\n",
    "test_predictions = optimized_model.predict(\n",
    "    test_df.drop(columns=['diagnosed_diabetes'])\n",
    ")\n",
    "\n",
    "# Show confusion matrix on test data\n",
    "disp = ConfusionMatrixDisplay.from_predictions(\n",
    "    test_df['diagnosed_diabetes'],\n",
    "    test_predictions,\n",
    "    normalize='true',\n",
    "    colorbar=False\n",
    ")\n",
    "\n",
    "plt.title(f'Test set confusion matrix\\n(ROC-AUC: {test_roc_auc:.4f})')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86487da2",
   "metadata": {},
   "source": [
    "### 5.2. Train on complete dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a44cbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimized_model.fit(df.drop(columns=['diagnosed_diabetes']), df['diagnosed_diabetes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f5b9d34",
   "metadata": {},
   "source": [
    "## 6. Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d27cc18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model directory and ensure it exists\n",
    "model_dir = Path('../models')\n",
    "model_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Create model filename\n",
    "model_name = 'gradient_boosting'\n",
    "model_path = model_dir / f'{model_name}.joblib'\n",
    "\n",
    "# Save the final model\n",
    "joblib.dump(optimized_model, model_path)\n",
    "print('Model saved to:', model_path)\n",
    "print(f'File size: {model_path.stat().st_size / (1024**2):.2f} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cf2be",
   "metadata": {},
   "source": [
    "## 7. Save model description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21e6ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "# Create model description filename\n",
    "markdown_path = '../models/gradient_boosting.md'\n",
    "\n",
    "# Create markdown description\n",
    "markdown_content = \"\"\"# Gradient boosting model for diabetes prediction\n",
    "\n",
    "## Model overview\n",
    "\n",
    "This dataset contains a scikit-learn Pipeline object that chains together multiple preprocessing, feature engineering, and modeling steps into a single estimator. Pipelines ensure that all transformations are applied consistently during both training and inference, preventing data leakage and simplifying deployment. The pipeline was optimized using RandomizedSearchCV with extensive hyperparameter tuning.\n",
    "\n",
    "For details on model optimization and training, see the [gradient boosting optimization and training notebook](https://github.com/gperdrizet/diabetes-prediction/blob/main/notebooks/02.1-gradient_boosting_model.ipynb) on GitHub.\n",
    "\n",
    "## Files\n",
    "\n",
    "- **Model file**: `gradient_boosting.joblib` (scikit-learn Pipeline object serialized with joblib)\n",
    "- **Custom transformers**: `gradient_boosting_transformers.py` (required for model deserialization)\n",
    "- **Documentation**: `gradient_boosting.md`\n",
    "\n",
    "Key features:\n",
    "- **End-to-end processing**: Automatically handles all preprocessing from raw data to predictions\n",
    "- **Reproducible transformations**: All fitted parameters (encoders, feature engineering, clustering) are preserved\n",
    "- **Hyperparameter optimization**: Parameters across all pipeline steps were jointly optimized\n",
    "\n",
    "## Training information\n",
    "\n",
    "- **Training date**: \"\"\" + datetime.now().strftime('%Y-%m-%d %H:%M:%S') + \"\"\"\n",
    "- **Training samples**: 700,000\n",
    "- **Cross-validation**: 3-fold CV with ROC-AUC scoring\n",
    "- **Optimization method**: RandomizedSearchCV with 200 iterations\n",
    "- **Optimization samples**: ~70,000 samples (10%) for hyperparameter tuning\n",
    "\n",
    "## Pipeline components\n",
    "\n",
    "### 1. Preprocessing\n",
    "\n",
    "#### ID column removal\n",
    "- **ID column dropper**: Automatically removes the 'id' column from input data (custom transformer)\n",
    "\n",
    "#### Numerical features (14 features)\n",
    "- **Features**: age, physical_activity_minutes_per_week, diet_score, sleep_hours_per_day, screen_time_hours_per_day, bmi, waist_to_hip_ratio, systolic_bp, diastolic_bp, heart_rate, cholesterol_total, hdl_cholesterol, ldl_cholesterol, triglycerides\n",
    "- **Transformation**: Passed through for feature engineering\n",
    "\n",
    "#### Ordinal features (6 features)\n",
    "- **Ordinal encoding**: education_level, income_level, alcohol_consumption_per_week, family_history_diabetes, hypertension_history, cardiovascular_history\n",
    "- **Method**: OrdinalEncoder with predefined category orders\n",
    "\n",
    "#### Nominal features (4 features)\n",
    "- **One-hot encoding**: gender, ethnicity, smoking_status, employment_status\n",
    "- **Method**: OneHotEncoder (drop first category to avoid multicollinearity)\n",
    "\n",
    "### 2. Feature engineering\n",
    "\n",
    "This pipeline creates extensive synthetic features to capture complex relationships:\n",
    "\n",
    "- **Binning (discretization)**:\n",
    "  - Bins: 5 quantile-based bins\n",
    "  - Applied to all 14 numerical features\n",
    "  - Creates categorical representations of continuous features\n",
    "\n",
    "- **KMeans clustering** (3 cluster feature sets):\n",
    "  - **Heart health cluster** (4 clusters): systolic_bp, diastolic_bp, heart_rate, hypertension_history, cardiovascular_history\n",
    "  - **Cholesterol cluster** (4 clusters): cholesterol_total, hdl_cholesterol, ldl_cholesterol, triglycerides\n",
    "  - **Lifestyle cluster** (4 clusters): physical_activity_minutes_per_week, diet_score, sleep_hours_per_day, screen_time_hours_per_day, alcohol_consumption_per_week\n",
    "\n",
    "- **Polynomial features**:\n",
    "  - Degree: 2\n",
    "  - Include bias: False\n",
    "  - Creates interaction terms and squared features\n",
    "\n",
    "- **Synthetic features** (custom transformers):\n",
    "  - **Difference features**: All pairwise feature differences (A - B)\n",
    "  - **Sum features**: All pairwise feature sums (A + B)\n",
    "  - **Ratio features**: All pairwise feature ratios (A / B)\n",
    "  - **Reciprocal features**: 1/feature for all features\n",
    "  - **Log features**: Natural log transform of all features\n",
    "  - **Square root features**: Square root of all features\n",
    "\n",
    "### 3. Feature selection\n",
    "\n",
    "After feature engineering, the pipeline reduces dimensionality:\n",
    "\n",
    "- **Variance threshold**: Removes low-variance features (optimized threshold)\n",
    "- **Select percentile**: Keeps top N% of features based on ANOVA F-value (optimized percentage)\n",
    "\n",
    "### 4. Classifier\n",
    "\n",
    "- **Algorithm**: HistGradientBoostingClassifier\n",
    "- **Optimized hyperparameters**:\n",
    "  - Learning rate: Optimized via log-uniform distribution (0.001 - 0.3)\n",
    "  - Max iterations: Optimized (100 - 1000 range)\n",
    "  - Max depth: Optimized (10 - 100 range)\n",
    "  - Min samples leaf: Optimized (5 - 50 range)\n",
    "  - L2 regularization: Optimized via log-uniform distribution\n",
    "  - Class weight: Optimized (None or balanced)\n",
    "- **Fixed parameters**:\n",
    "  - Max bins: 255\n",
    "  - Early stopping: Enabled\n",
    "  - Validation fraction: 0.1\n",
    "  - N iter no change: 50\n",
    "\n",
    "## Custom transformers\n",
    "\n",
    "The model uses multiple custom scikit-learn transformers defined in `gradient_boosting_transformers.py`:\n",
    "\n",
    "### IDColumnDropper\n",
    "Automatically removes the 'id' column from input DataFrames before processing.\n",
    "\n",
    "### IQRClipper\n",
    "Clips outliers using the interquartile range (IQR) method (not used in final pipeline but available).\n",
    "\n",
    "### Feature engineering transformers\n",
    "- **DifferenceFeatures**: Creates difference features between all pairs of input features\n",
    "- **SumFeatures**: Creates sum features from all combinations of input features\n",
    "- **RatioFeatures**: Creates ratio features between all pairs of input features (handles division by zero)\n",
    "- **ReciprocalFeatures**: Creates reciprocal (1/x) features for all input features (handles division by zero)\n",
    "- **LogFeatures**: Creates log-transformed features for all input features (handles negative values)\n",
    "- **SquareRootFeatures**: Creates square root features for all input features (handles negative values)\n",
    "- **KMeansClusterFeatures**: Creates cluster membership features using KMeans clustering\n",
    "\n",
    "**Important**: The `gradient_boosting_transformers.py` file must be available in the Python path when loading the model, as joblib stores references to these classes and needs to import them during deserialization.\n",
    "\n",
    "## Usage\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "# Add the models directory to the path (adjust as needed)\n",
    "sys.path.insert(0, str(Path('models').resolve()))\n",
    "\n",
    "# Load the model (this will import the custom transformers)\n",
    "model = joblib.load('models/gradient_boosting.joblib')\n",
    "\n",
    "# Prepare test data (pipeline will automatically handle 'id' column)\n",
    "X_test = pd.read_csv('test.csv')\n",
    "\n",
    "# Make predictions\n",
    "predictions = model.predict(X_test)\n",
    "probabilities = model.predict_proba(X_test)\n",
    "```\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Input data can include the 'id' column - it will be automatically removed by the pipeline\n",
    "- The pipeline handles all preprocessing and feature engineering automatically\n",
    "- The `gradient_boosting_transformers.py` file must be in the Python path when loading the model\n",
    "- This model uses extensive feature engineering including polynomial features, clustering, and synthetic features\n",
    "- Feature engineering creates thousands of features which are then reduced via feature selection\n",
    "\"\"\"\n",
    "\n",
    "with open(markdown_path, 'w') as f:\n",
    "    f.write(markdown_content)\n",
    "\n",
    "print('Model description saved to:', markdown_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
